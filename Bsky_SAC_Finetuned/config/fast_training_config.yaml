# Fast Training Configuration - Optimized for Speed

# Model Configuration
model:
  name: "meta-llama/Llama-3.1-8B-Instruct"
  output_name: "llama3.1-bsky-sac-fast"
  
# Training Configuration - Speed Optimized
training:
  learning_rate: 0.0005  # Higher for faster convergence
  batch_size: 64  # Very large batch size for maximum GPU utilization
  epochs: 1  # Single epoch for quick testing
  gradient_accumulation_steps: 1
  warmup_steps: 25  # Minimal warmup
  save_steps: 100  # Frequent saves
  eval_steps: 50   # Frequent evaluation
  logging_steps: 10  # Frequent logging

# Data Configuration - Speed Optimized
data:
  train_split: 0.8
  validation_split: 0.15
  test_split: 0.05
  max_sequence_length: 256  # Very short sequences for speed
  deduplication:
    enabled: true

# LoRA Configuration - Balanced
lora:
  rank: 16  # Reduced for speed
  alpha: 32
  dropout: 0.1
  target_modules:
    - "q_proj"
    - "v_proj" 
    - "k_proj"
    - "o_proj"

# Hardware Configuration
hardware:
  device: "cuda"
  bf16: true
  dataloader_num_workers: 0
  pin_memory: true
