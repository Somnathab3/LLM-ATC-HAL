#!/usr/bin/env python3
"""
LLM Prompt Engine Integration Example
===================================
Demonstrates how to integrate the LLM prompt engine with existing
conflict resolution components.
"""

import sys
import os
import time
from typing import Dict, List, Any, Optional

# Add project root to path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
sys.path.insert(0, project_root)

from llm_atc.tools.llm_prompt_engine import LLMPromptEngine, ResolutionResponse


class EnhancedConflictResolver:
    """
    Enhanced conflict resolver that integrates LLM prompt engine
    with traditional conflict resolution methods.
    """
    
    def __init__(self, use_llm: bool = True, fallback_enabled: bool = True):
        """
        Initialize enhanced resolver.
        
        Args:
            use_llm: Whether to use LLM for resolution
            fallback_enabled: Whether to fall back to traditional methods
        """
        self.use_llm = use_llm
        self.fallback_enabled = fallback_enabled
        
        if self.use_llm:
            self.llm_engine = LLMPromptEngine(enable_function_calls=True)
        
        self.resolution_history = []
    
    def resolve_conflict(self, conflict_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Resolve conflict using LLM engine with fallback.
        
        Args:
            conflict_info: Conflict scenario information
            
        Returns:
            Resolution result with command and metadata
        """
        resolution_result = {
            'success': False,
            'command': None,
            'method': 'none',
            'confidence': 0.0,
            'rationale': '',
            'safety_assessment': {},
            'timestamp': time.time()
        }
        
        # Try LLM-based resolution first
        if self.use_llm:
            try:
                llm_result = self._resolve_via_llm(conflict_info)
                if llm_result['success']:
                    resolution_result.update(llm_result)
                    resolution_result['method'] = 'llm'
                    
                    # Perform safety assessment
                    safety_check = self._assess_safety(
                        llm_result['command'], 
                        conflict_info
                    )
                    resolution_result['safety_assessment'] = safety_check
                    
                    # Store in history
                    self.resolution_history.append(resolution_result.copy())
                    return resolution_result
                    
            except Exception as e:
                print(f"LLM resolution failed: {e}")
        
        # Fallback to traditional methods if enabled
        if self.fallback_enabled:
            fallback_result = self._resolve_via_fallback(conflict_info)
            if fallback_result['success']:
                resolution_result.update(fallback_result)
                resolution_result['method'] = 'fallback'
                
                # Store in history
                self.resolution_history.append(resolution_result.copy())
        
        return resolution_result
    
    def _resolve_via_llm(self, conflict_info: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve conflict using LLM prompt engine"""
        try:
            # Get resolution from LLM
            command = self.llm_engine.get_conflict_resolution(conflict_info)
            
            if command:
                return {
                    'success': True,
                    'command': command,
                    'confidence': 0.8,  # Default, could be extracted from response
                    'rationale': 'Generated by LLM'
                }
            else:
                return {'success': False, 'error': 'No valid command generated'}
                
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _resolve_via_fallback(self, conflict_info: Dict[str, Any]) -> Dict[str, Any]:
        """Fallback resolution using traditional methods"""
        try:
            # Simple rule-based resolution
            ac1_id = conflict_info.get('aircraft_1_id', 'AC001')
            ac2_id = conflict_info.get('aircraft_2_id', 'AC002')
            
            # Default to heading change for first aircraft
            command = f"HDG {ac1_id} 270"
            
            return {
                'success': True,
                'command': command,
                'confidence': 0.6,
                'rationale': 'Rule-based fallback resolution'
            }
            
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    def _assess_safety(self, command: str, conflict_info: Dict[str, Any]) -> Dict[str, Any]:
        """Assess safety of proposed resolution"""
        try:
            if hasattr(self, 'llm_engine'):
                return self.llm_engine.assess_resolution_safety(command, conflict_info)
            else:
                return {
                    'safety_rating': 'UNKNOWN',
                    'note': 'LLM not available for safety assessment'
                }
        except Exception as e:
            return {'safety_rating': 'ERROR', 'error': str(e)}
    
    def detect_conflicts_via_llm(self, aircraft_states: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Use LLM for conflict detection"""
        if self.use_llm and hasattr(self, 'llm_engine'):
            try:
                return self.llm_engine.detect_conflict_via_llm(aircraft_states)
            except Exception as e:
                return {'conflict_detected': False, 'error': str(e)}
        
        return {'conflict_detected': False, 'error': 'LLM not available'}
    
    def get_resolution_statistics(self) -> Dict[str, Any]:
        """Get statistics about resolution performance"""
        if not self.resolution_history:
            return {'total_resolutions': 0}
        
        total = len(self.resolution_history)
        successful = sum(1 for r in self.resolution_history if r['success'])
        llm_resolutions = sum(1 for r in self.resolution_history if r['method'] == 'llm')
        fallback_resolutions = sum(1 for r in self.resolution_history if r['method'] == 'fallback')
        
        avg_confidence = sum(r['confidence'] for r in self.resolution_history) / total
        
        return {
            'total_resolutions': total,
            'successful_resolutions': successful,
            'success_rate': successful / total if total > 0 else 0,
            'llm_resolutions': llm_resolutions,
            'fallback_resolutions': fallback_resolutions,
            'average_confidence': avg_confidence,
            'llm_usage_rate': llm_resolutions / total if total > 0 else 0
        }


def demonstration_scenario():
    """Demonstrate the enhanced conflict resolver"""
    print("Enhanced Conflict Resolver Demonstration")
    print("=" * 50)
    
    # Initialize resolver
    resolver = EnhancedConflictResolver(use_llm=True, fallback_enabled=True)
    
    # Sample conflict scenarios
    scenarios = [
        {
            'aircraft_1_id': 'AAL123',
            'aircraft_2_id': 'UAL456',
            'time_to_conflict': 95.5,
            'closest_approach_distance': 3.2,
            'conflict_type': 'convergent',
            'urgency_level': 'high',
            'aircraft_1': {
                'lat': 52.3676, 'lon': 4.9041, 'alt': 35000,
                'hdg': 90, 'spd': 450, 'type': 'B738'
            },
            'aircraft_2': {
                'lat': 52.3700, 'lon': 4.9100, 'alt': 35000,
                'hdg': 270, 'spd': 460, 'type': 'A320'
            },
            'environmental_conditions': {
                'wind_direction_deg': 270, 'wind_speed_kts': 15
            }
        },
        {
            'aircraft_1_id': 'DAL789',
            'aircraft_2_id': 'SWA321',
            'time_to_conflict': 120.0,
            'closest_approach_distance': 4.1,
            'conflict_type': 'horizontal',
            'urgency_level': 'medium',
            'aircraft_1': {
                'lat': 40.7128, 'lon': -74.0060, 'alt': 37000,
                'hdg': 180, 'spd': 480, 'type': 'B737'
            },
            'aircraft_2': {
                'lat': 40.7200, 'lon': -74.0100, 'alt': 37000,
                'hdg': 360, 'spd': 470, 'type': 'A319'
            },
            'environmental_conditions': {
                'wind_direction_deg': 90, 'wind_speed_kts': 25
            }
        }
    ]
    
    # Process each scenario
    for i, scenario in enumerate(scenarios, 1):
        print(f"\nScenario {i}: {scenario['aircraft_1_id']} vs {scenario['aircraft_2_id']}")
        print("-" * 40)
        
        # Resolve conflict
        result = resolver.resolve_conflict(scenario)
        
        if result['success']:
            print(f"✅ Resolution: {result['command']}")
            print(f"   Method: {result['method']}")
            print(f"   Confidence: {result['confidence']:.2f}")
            print(f"   Rationale: {result['rationale']}")
            
            if result['safety_assessment']:
                safety = result['safety_assessment']
                print(f"   Safety: {safety.get('safety_rating', 'Unknown')}")
        else:
            print(f"❌ Resolution failed: {result.get('error', 'Unknown error')}")
    
    # Show statistics
    print(f"\nResolution Statistics:")
    print("-" * 40)
    stats = resolver.get_resolution_statistics()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"{key}: {value:.2f}")
        else:
            print(f"{key}: {value}")


def conflict_detection_demo():
    """Demonstrate LLM-based conflict detection"""
    print("\n" + "=" * 50)
    print("LLM-Based Conflict Detection Demo")
    print("=" * 50)
    
    resolver = EnhancedConflictResolver(use_llm=True)
    
    # Sample aircraft states
    aircraft_states = [
        {
            'id': 'AAL100', 'lat': 52.3676, 'lon': 4.9041, 'alt': 35000,
            'hdg': 90, 'spd': 450, 'vs': 0
        },
        {
            'id': 'UAL200', 'lat': 52.3680, 'lon': 4.9050, 'alt': 35000,
            'hdg': 270, 'spd': 460, 'vs': 0
        },
        {
            'id': 'DAL300', 'lat': 52.4000, 'lon': 4.9500, 'alt': 37000,
            'hdg': 180, 'spd': 440, 'vs': 0
        }
    ]
    
    print("Aircraft states provided for analysis:")
    for aircraft in aircraft_states:
        print(f"  {aircraft['id']}: {aircraft['lat']:.4f}°N, {aircraft['lon']:.4f}°E, "
              f"{aircraft['alt']}ft, HDG{aircraft['hdg']}°")
    
    # Detect conflicts
    detection_result = resolver.detect_conflicts_via_llm(aircraft_states)
    
    print(f"\nDetection Result:")
    print(f"Conflict Detected: {detection_result.get('conflict_detected', 'Unknown')}")
    if detection_result.get('aircraft_pairs'):
        print(f"Aircraft Pairs at Risk: {detection_result['aircraft_pairs']}")
    if detection_result.get('confidence'):
        print(f"Confidence: {detection_result['confidence']:.2f}")
    
    print("\n⚠️  Note: Actual LLM integration requires Ollama service running")


def main():
    """Run the integration demonstration"""
    print("LLM Prompt Engine Integration Demo")
    print("=" * 50)
    
    try:
        demonstration_scenario()
        conflict_detection_demo()
        
        print("\n" + "=" * 50)
        print("✅ Integration demonstration completed!")
        print("\nKey Features Demonstrated:")
        print("• LLM-based conflict resolution with fallback")
        print("• Safety assessment of proposed maneuvers")
        print("• Conflict detection via LLM analysis")
        print("• Resolution performance statistics")
        print("• Hybrid approach (LLM + traditional methods)")
        
    except Exception as e:
        print(f"\n❌ Demo failed: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
