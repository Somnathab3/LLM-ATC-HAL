{
  "current_state": {
    "functions": {
      "llm_atc\\agents\\executor.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 46,
          "line_end": 50,
          "args": [
            "self",
            "command_sender"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult"
          ],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "send_plan",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 52,
          "line_end": 164,
          "args": [
            "self",
            "plan"
          ],
          "decorators": [],
          "docstring": "\n        Execute an action plan by sending commands to BlueSky\n\n        Args:\n            plan: ActionPlan to execute\n\n        Returns:\n            ExecutionResult with execution status and details\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult",
            "ActionPlan",
            "ExecutionStatus"
          ],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "_send_command",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 166,
          "line_end": 204,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "\n        Send a single command to BlueSky simulator\n\n        Args:\n            command: BlueSky command string\n\n        Returns:\n            Response dictionary with success status and details\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "_simulate_command_execution",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 206,
          "line_end": 242,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "\n        Simulate command execution for testing purposes\n\n        Args:\n            command: BlueSky command string\n\n        Returns:\n            Simulated response dictionary\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "cancel_execution",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 244,
          "line_end": 259,
          "args": [
            "self",
            "execution_id"
          ],
          "decorators": [],
          "docstring": "\n        Cancel an active execution\n\n        Args:\n            execution_id: ID of execution to cancel\n\n        Returns:\n            True if cancelled successfully, False otherwise\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionStatus"
          ],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "get_execution_status",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 261,
          "line_end": 279,
          "args": [
            "self",
            "execution_id"
          ],
          "decorators": [],
          "docstring": "\n        Get current status of an execution\n\n        Args:\n            execution_id: ID of execution to check\n\n        Returns:\n            ExecutionStatus or None if not found\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionStatus"
          ],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "get_active_executions",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 281,
          "line_end": 283,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get all currently active executions",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult"
          ],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "get_execution_history",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 285,
          "line_end": 287,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get history of all executions",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult"
          ],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "get_execution_metrics",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 289,
          "line_end": 315,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get overall execution performance metrics",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionStatus"
          ],
          "is_method": true,
          "class_name": "Executor"
        },
        {
          "name": "set_command_sender",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 317,
          "line_end": 325,
          "args": [
            "self",
            "command_sender"
          ],
          "decorators": [],
          "docstring": "\n        Set the command sender function for actual BlueSky integration\n\n        Args:\n            command_sender: Function that takes a command string and returns response\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Executor"
        }
      ],
      "llm_atc\\agents\\planner.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 75,
          "line_end": 79,
          "args": [
            "self",
            "llm_client"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "ActionPlan",
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "assess_conflict",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 81,
          "line_end": 126,
          "args": [
            "self",
            "aircraft_info"
          ],
          "decorators": [],
          "docstring": "\n        Assess current aircraft situation for potential conflicts\n\n        Args:\n            aircraft_info: Dictionary containing all aircraft information\n\n        Returns:\n            ConflictAssessment or None if no conflicts detected\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "generate_action_plan",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 128,
          "line_end": 176,
          "args": [
            "self",
            "assessment"
          ],
          "decorators": [],
          "docstring": "\n        Generate detailed action plan based on conflict assessment\n\n        Args:\n            assessment: ConflictAssessment from assess_conflict\n\n        Returns:\n            ActionPlan with specific commands and expected outcomes\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ActionPlan",
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_detect_proximity_conflicts",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 178,
          "line_end": 213,
          "args": [
            "self",
            "aircraft_data"
          ],
          "decorators": [],
          "docstring": "Detect proximity-based conflicts between aircraft",
          "imports_used": [],
          "intra_repo_calls": [
            "aircraft_list"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_calculate_separation",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 215,
          "line_end": 240,
          "args": [
            "self",
            "ac1_data",
            "ac2_data"
          ],
          "decorators": [],
          "docstring": "Calculate horizontal and vertical separation between aircraft",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_assess_severity",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 242,
          "line_end": 259,
          "args": [
            "self",
            "separation"
          ],
          "decorators": [],
          "docstring": "Assess conflict severity based on separation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_estimate_time_to_conflict",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 261,
          "line_end": 265,
          "args": [
            "self",
            "_ac1_data",
            "_ac2_data"
          ],
          "decorators": [],
          "docstring": "Estimate time to conflict in seconds",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_prioritize_conflicts",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 267,
          "line_end": 281,
          "args": [
            "self",
            "conflicts"
          ],
          "decorators": [],
          "docstring": "Select the most critical conflict to address first",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_generate_assessment",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 283,
          "line_end": 307,
          "args": [
            "self",
            "conflict",
            "aircraft_data"
          ],
          "decorators": [],
          "docstring": "Generate comprehensive conflict assessment",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_determine_recommended_action",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 309,
          "line_end": 321,
          "args": [
            "self",
            "conflict",
            "_aircraft_data"
          ],
          "decorators": [],
          "docstring": "Determine the most appropriate action type for conflict resolution",
          "imports_used": [],
          "intra_repo_calls": [
            "PlanType"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_generate_reasoning",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 323,
          "line_end": 333,
          "args": [
            "self",
            "conflict",
            "action"
          ],
          "decorators": [],
          "docstring": "Generate human-readable reasoning for the recommended action",
          "imports_used": [],
          "intra_repo_calls": [
            "PlanType"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_generate_commands",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 335,
          "line_end": 364,
          "args": [
            "self",
            "assessment"
          ],
          "decorators": [],
          "docstring": "Generate specific BlueSky commands for conflict resolution",
          "imports_used": [],
          "intra_repo_calls": [
            "PlanType",
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_calculate_expected_outcome",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 366,
          "line_end": 377,
          "args": [
            "self",
            "_assessment",
            "_commands"
          ],
          "decorators": [],
          "docstring": "Calculate expected outcome of executing the commands",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "_calculate_priority",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 379,
          "line_end": 396,
          "args": [
            "self",
            "assessment"
          ],
          "decorators": [],
          "docstring": "Calculate plan priority (1-10, higher is more urgent)",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "get_assessment_history",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 398,
          "line_end": 400,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get history of conflict assessments",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Planner"
        },
        {
          "name": "get_plan_history",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 402,
          "line_end": 404,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get history of generated plans",
          "imports_used": [],
          "intra_repo_calls": [
            "ActionPlan"
          ],
          "is_method": true,
          "class_name": "Planner"
        }
      ],
      "llm_atc\\agents\\scratchpad.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 65,
          "line_end": 78,
          "args": [
            "self",
            "session_id"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "SessionSummary",
            "ReasoningStep"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "log_step",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 80,
          "line_end": 125,
          "args": [
            "self",
            "step_data"
          ],
          "decorators": [],
          "docstring": "\n        Log a reasoning step in the current session\n\n        Args:\n            step_data: Dictionary containing step information\n\n        Returns:\n            step_id of the logged step\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "StepType",
            "ReasoningStep"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "log_assessment_step",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 127,
          "line_end": 146,
          "args": [
            "self",
            "assessment"
          ],
          "decorators": [],
          "docstring": "Log a conflict assessment step",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictAssessment"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "log_planning_step",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 148,
          "line_end": 170,
          "args": [
            "self",
            "plan"
          ],
          "decorators": [],
          "docstring": "Log an action planning step",
          "imports_used": [],
          "intra_repo_calls": [
            "ActionPlan"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "log_execution_step",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 172,
          "line_end": 195,
          "args": [
            "self",
            "execution"
          ],
          "decorators": [],
          "docstring": "Log a plan execution step",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "log_verification_step",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 197,
          "line_end": 221,
          "args": [
            "self",
            "verification"
          ],
          "decorators": [],
          "docstring": "Log a verification step",
          "imports_used": [],
          "intra_repo_calls": [
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "log_error_step",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 223,
          "line_end": 236,
          "args": [
            "self",
            "error_msg",
            "error_data"
          ],
          "decorators": [],
          "docstring": "Log an error step",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "log_monitoring_step",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 238,
          "line_end": 249,
          "args": [
            "self",
            "monitoring_data"
          ],
          "decorators": [],
          "docstring": "Log a monitoring step",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "get_history",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 251,
          "line_end": 267,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "\n        Get complete history of the current session\n\n        Returns:\n            Dictionary containing session history and steps\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "get_step_by_id",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 269,
          "line_end": 274,
          "args": [
            "self",
            "step_id"
          ],
          "decorators": [],
          "docstring": "Get a specific step by its ID",
          "imports_used": [],
          "intra_repo_calls": [
            "ReasoningStep"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "get_steps_by_type",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 276,
          "line_end": 278,
          "args": [
            "self",
            "step_type"
          ],
          "decorators": [],
          "docstring": "Get all steps of a specific type",
          "imports_used": [],
          "intra_repo_calls": [
            "StepType",
            "ReasoningStep"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "get_recent_steps",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 280,
          "line_end": 286,
          "args": [
            "self",
            "count"
          ],
          "decorators": [],
          "docstring": "Get the most recent steps",
          "imports_used": [],
          "intra_repo_calls": [
            "ReasoningStep"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "complete_session",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 288,
          "line_end": 344,
          "args": [
            "self",
            "success",
            "final_status"
          ],
          "decorators": [],
          "docstring": "\n        Complete the current session and generate summary\n\n        Args:\n            success: Whether the session completed successfully\n            final_status: Final status description\n\n        Returns:\n            SessionSummary of the completed session\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "SessionSummary",
            "StepType"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "start_new_session",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 346,
          "line_end": 367,
          "args": [
            "self",
            "session_id"
          ],
          "decorators": [],
          "docstring": "\n        Start a new reasoning session\n\n        Args:\n            session_id: Optional custom session ID\n\n        Returns:\n            New session ID\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "_generate_session_summary",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 369,
          "line_end": 390,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Generate a summary of the current session",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "_calculate_average_confidence",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 392,
          "line_end": 398,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Calculate average confidence across all steps",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "_extract_key_decisions",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 400,
          "line_end": 412,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Extract key decisions from the session",
          "imports_used": [],
          "intra_repo_calls": [
            "StepType"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "_extract_lessons_learned",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 414,
          "line_end": 432,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Extract lessons learned from errors and low-confidence steps",
          "imports_used": [],
          "intra_repo_calls": [
            "StepType"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "export_session_data",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 434,
          "line_end": 448,
          "args": [
            "self",
            "format"
          ],
          "decorators": [],
          "docstring": "\n        Export session data in specified format\n\n        Args:\n            format: Export format ('json', 'dict')\n\n        Returns:\n            Session data in requested format\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "set_session_metadata",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 450,
          "line_end": 452,
          "args": [
            "self",
            "metadata"
          ],
          "decorators": [],
          "docstring": "Set metadata for the current session",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scratchpad"
        },
        {
          "name": "get_session_metrics",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 454,
          "line_end": 473,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get performance metrics for the current session",
          "imports_used": [],
          "intra_repo_calls": [
            "StepType"
          ],
          "is_method": true,
          "class_name": "Scratchpad"
        }
      ],
      "llm_atc\\agents\\verifier.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 44,
          "line_end": 56,
          "args": [
            "self",
            "safety_thresholds"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "check",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 58,
          "line_end": 133,
          "args": [
            "self",
            "execution_result",
            "timeout_seconds"
          ],
          "decorators": [],
          "docstring": "\n        Perform verification check on execution result\n\n        Args:\n            execution_result: ExecutionResult to verify\n            timeout_seconds: Maximum time to wait for verification\n\n        Returns:\n            True if verification passes, False otherwise\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult",
            "VerificationResult",
            "VerificationStatus"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_check_execution_status",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 135,
          "line_end": 153,
          "args": [
            "self",
            "execution",
            "verification"
          ],
          "decorators": [],
          "docstring": "Check if execution completed successfully",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult",
            "VerificationResult",
            "ExecutionStatus"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_check_execution_timing",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 155,
          "line_end": 175,
          "args": [
            "self",
            "execution",
            "verification"
          ],
          "decorators": [],
          "docstring": "Check execution timing constraints",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult",
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_check_command_success_rate",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 177,
          "line_end": 199,
          "args": [
            "self",
            "execution",
            "verification"
          ],
          "decorators": [],
          "docstring": "Check command success rate",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult",
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_check_safety_compliance",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 201,
          "line_end": 222,
          "args": [
            "self",
            "execution",
            "verification"
          ],
          "decorators": [],
          "docstring": "Check safety compliance of executed commands",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult",
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_check_response_validity",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 224,
          "line_end": 248,
          "args": [
            "self",
            "execution",
            "verification"
          ],
          "decorators": [],
          "docstring": "Check validity of command responses",
          "imports_used": [],
          "intra_repo_calls": [
            "ExecutionResult",
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_is_unsafe_command",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 250,
          "line_end": 286,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Check if a command is potentially unsafe",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_is_valid_response",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 288,
          "line_end": 295,
          "args": [
            "self",
            "response"
          ],
          "decorators": [],
          "docstring": "Check if a command response is valid",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_calculate_safety_score",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 297,
          "line_end": 315,
          "args": [
            "self",
            "verification"
          ],
          "decorators": [],
          "docstring": "Calculate overall safety score based on verification results",
          "imports_used": [],
          "intra_repo_calls": [
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "_calculate_confidence",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 317,
          "line_end": 334,
          "args": [
            "self",
            "verification"
          ],
          "decorators": [],
          "docstring": "Calculate confidence in verification results",
          "imports_used": [],
          "intra_repo_calls": [
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "get_verification_history",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 336,
          "line_end": 338,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get history of all verification results",
          "imports_used": [],
          "intra_repo_calls": [
            "VerificationResult"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "get_verification_metrics",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 340,
          "line_end": 370,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get overall verification performance metrics",
          "imports_used": [],
          "intra_repo_calls": [
            "VerificationStatus"
          ],
          "is_method": true,
          "class_name": "Verifier"
        },
        {
          "name": "update_safety_thresholds",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 372,
          "line_end": 375,
          "args": [
            "self",
            "new_thresholds"
          ],
          "decorators": [],
          "docstring": "Update safety thresholds for verification",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Verifier"
        }
      ],
      "llm_atc\\memory\\experience_integrator.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 23,
          "line_end": 33,
          "args": [
            "self",
            "replay_store"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "VectorReplayStore",
            "EnhancedHallucinationDetector",
            "SafetyMarginQuantifier"
          ],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "process_conflict_resolution",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 35,
          "line_end": 84,
          "args": [
            "self",
            "scenario_context",
            "conflict_geometry",
            "environmental_conditions",
            "llm_decision",
            "baseline_decision"
          ],
          "decorators": [],
          "docstring": "\n        Process a conflict resolution with experience replay integration\n\n        Returns:\n            Tuple of (enhanced_decision, lessons_learned)\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "_find_relevant_experiences",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 86,
          "line_end": 121,
          "args": [
            "self",
            "scenario_context",
            "conflict_geometry",
            "environmental_conditions"
          ],
          "decorators": [],
          "docstring": "Find experiences relevant to current scenario",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictExperience",
            "SimilarityResult"
          ],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "_extract_lessons",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 123,
          "line_end": 183,
          "args": [
            "self",
            "similar_experiences"
          ],
          "decorators": [],
          "docstring": "Extract actionable lessons from similar experiences",
          "imports_used": [],
          "intra_repo_calls": [
            "SimilarityResult"
          ],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "_check_hallucination_patterns",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 185,
          "line_end": 259,
          "args": [
            "self",
            "scenario_context",
            "environmental_conditions",
            "similar_experiences"
          ],
          "decorators": [],
          "docstring": "Check for hallucination risk patterns",
          "imports_used": [],
          "intra_repo_calls": [
            "SimilarityResult"
          ],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "_enhance_decision_with_experience",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 261,
          "line_end": 363,
          "args": [
            "self",
            "llm_decision",
            "baseline_decision",
            "similar_experiences"
          ],
          "decorators": [],
          "docstring": "Enhance current decision using historical experience",
          "imports_used": [],
          "intra_repo_calls": [
            "SimilarityResult"
          ],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "record_resolution_outcome",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 365,
          "line_end": 409,
          "args": [
            "self",
            "scenario_context",
            "conflict_geometry",
            "environmental_conditions",
            "llm_decision",
            "baseline_decision",
            "actual_outcome",
            "safety_metrics",
            "hallucination_result",
            "controller_override",
            "lessons_learned"
          ],
          "decorators": [],
          "docstring": "Record the outcome of a conflict resolution for future learning",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictExperience"
          ],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "get_experience_summary",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 411,
          "line_end": 426,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get summary of stored experiences",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "_generate_learning_insights",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 428,
          "line_end": 482,
          "args": [
            "self",
            "stats",
            "patterns"
          ],
          "decorators": [],
          "docstring": "Generate insights from experience data",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        },
        {
          "name": "store_experience",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 484,
          "line_end": 518,
          "args": [
            "self",
            "experience_data"
          ],
          "decorators": [],
          "docstring": "Simple interface to store experience data directly",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictExperience"
          ],
          "is_method": true,
          "class_name": "ExperienceIntegrator"
        }
      ],
      "llm_atc\\memory\\replay_store.py": [
        {
          "name": "__post_init__",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 58,
          "line_end": 78,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ConflictExperience"
        },
        {
          "name": "__init__",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 106,
          "line_end": 168,
          "args": [
            "self",
            "storage_dir"
          ],
          "decorators": [],
          "docstring": "\n        Initialize the replay store\n\n        Args:\n            storage_dir: Directory for Chroma persistence\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "VectorReplayStore"
        },
        {
          "name": "store_experience",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 170,
          "line_end": 235,
          "args": [
            "self",
            "experience"
          ],
          "decorators": [],
          "docstring": "\n        Store a conflict experience in the vector store\n\n        Args:\n            experience: ConflictExperience object to store\n\n        Returns:\n            str: Experience ID if successful, empty string if failed\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictExperience"
          ],
          "is_method": true,
          "class_name": "VectorReplayStore"
        },
        {
          "name": "retrieve_experience",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 237,
          "line_end": 360,
          "args": [
            "self",
            "conflict_desc",
            "conflict_type",
            "num_ac",
            "k"
          ],
          "decorators": [],
          "docstring": "\n        Retrieve similar experiences using metadata filtering + vector search\n\n        Args:\n            conflict_desc: Description of the conflict to search for\n            conflict_type: Type of conflict to filter by\n            num_ac: Number of aircraft to filter by\n            k: Number of results to return\n\n        Returns:\n            List of experience documents in score-ascending order\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "VectorReplayStore"
        },
        {
          "name": "get_all_experiences",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 362,
          "line_end": 432,
          "args": [
            "self",
            "conflict_type",
            "num_ac",
            "limit"
          ],
          "decorators": [],
          "docstring": "\n        Get all experiences, optionally filtered by metadata\n\n        Args:\n            conflict_type: Optional conflict type filter\n            num_ac: Optional number of aircraft filter\n            limit: Optional limit on number of results\n\n        Returns:\n            List of experience documents\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "VectorReplayStore"
        },
        {
          "name": "get_stats",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 434,
          "line_end": 458,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get statistics about stored experiences",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "VectorReplayStore"
        },
        {
          "name": "delete_experience",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 460,
          "line_end": 468,
          "args": [
            "self",
            "experience_id"
          ],
          "decorators": [],
          "docstring": "Delete an experience by ID",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "VectorReplayStore"
        },
        {
          "name": "clear_all",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 470,
          "line_end": 487,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Clear all experiences from the store",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "VectorReplayStore"
        }
      ],
      "llm_atc\\metrics\\monte_carlo_analysis.py": [
        {
          "name": "calc_separation_margin",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 46,
          "line_end": 47,
          "args": [
            "trajectories"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "calc_efficiency_penalty",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 49,
          "line_end": 50,
          "args": [
            "planned",
            "executed"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "__init__",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 58,
          "line_end": 60,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize the analyzer.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "read_results_file",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 62,
          "line_end": 87,
          "args": [
            "self",
            "file_path"
          ],
          "decorators": [],
          "docstring": "\n        Read Monte Carlo results from JSON or CSV file.\n\n        Args:\n            file_path: Path to results file (.json or .csv)\n\n        Returns:\n            DataFrame with simulation results\n\n        Raises:\n            FileNotFoundError: If file doesn't exist\n            ValueError: If file format not supported\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_read_json_results",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 89,
          "line_end": 113,
          "args": [
            "self",
            "file_path"
          ],
          "decorators": [],
          "docstring": "Read results from JSON file format.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "compute_false_positive_negative_rates",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 115,
          "line_end": 164,
          "args": [
            "self",
            "results_df"
          ],
          "decorators": [],
          "docstring": "\n        Compute false positive and false negative rates from results.\n\n        Args:\n            results_df: DataFrame with columns 'predicted_conflicts', 'actual_conflicts'\n\n        Returns:\n            Dict with 'false_positive_rate' and 'false_negative_rate'\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_conflicts_to_set",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 166,
          "line_end": 181,
          "args": [
            "self",
            "conflicts"
          ],
          "decorators": [],
          "docstring": "Convert conflict list to set of aircraft pairs.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "compute_success_rates_by_scenario",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 183,
          "line_end": 231,
          "args": [
            "self",
            "results_df"
          ],
          "decorators": [],
          "docstring": "\n        Compute success rates grouped by scenario type.\n\n        Args:\n            results_df: DataFrame with columns 'scenario_type', 'success'\n\n        Returns:\n            Dict mapping scenario types to success metrics\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "compute_success_rates_by_group",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 233,
          "line_end": 313,
          "args": [
            "self",
            "results_df",
            "group_cols"
          ],
          "decorators": [],
          "docstring": "\n        Compute success rates grouped by specified columns.\n\n        Args:\n            results_df: DataFrame with columns including 'success' and the grouping columns\n            group_cols: List of column names to group by (e.g. ['scenario_type', 'complexity_tier', 'distribution_shift'])\n\n        Returns:\n            Multi-index DataFrame of success rates grouped by specified columns\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "compute_average_separation_margins",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 315,
          "line_end": 367,
          "args": [
            "self",
            "results_df"
          ],
          "decorators": [],
          "docstring": "\n        Compute average separation margins from results.\n\n        Args:\n            results_df: DataFrame with trajectory or margin data\n\n        Returns:\n            Dict with horizontal and vertical margin averages\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "calc_separation_margin"
          ],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "compute_efficiency_penalties",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 369,
          "line_end": 412,
          "args": [
            "self",
            "results_df"
          ],
          "decorators": [],
          "docstring": "\n        Compute efficiency penalties from trajectory comparisons.\n\n        Args:\n            results_df: DataFrame with planned and executed trajectory data\n\n        Returns:\n            Dict with efficiency penalty statistics\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "calc_efficiency_penalty"
          ],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "generate_report",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 414,
          "line_end": 618,
          "args": [
            "self",
            "results_df",
            "aggregated_metrics",
            "output_file"
          ],
          "decorators": [],
          "docstring": "\n        Generate a comprehensive markdown report with all metrics and analysis.\n\n        Args:\n            results_df: DataFrame with simulation results\n            aggregated_metrics: Pre-computed metrics (if None, will compute from results_df)\n            output_file: Path to save the markdown report\n\n        Returns:\n            Path to the generated report file\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_generate_executive_summary",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 620,
          "line_end": 687,
          "args": [
            "self",
            "metrics"
          ],
          "decorators": [],
          "docstring": "Generate executive summary section.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_assess_detection_performance",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 689,
          "line_end": 722,
          "args": [
            "self",
            "detection"
          ],
          "decorators": [],
          "docstring": "Assess detection performance and provide interpretation.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_assess_safety_margins",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 724,
          "line_end": 759,
          "args": [
            "self",
            "margins"
          ],
          "decorators": [],
          "docstring": "Assess safety margin performance.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_assess_efficiency_performance",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 761,
          "line_end": 769,
          "args": [
            "self",
            "efficiency"
          ],
          "decorators": [],
          "docstring": "Assess efficiency performance.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_format_grouped_success_table",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 771,
          "line_end": 797,
          "args": [
            "self",
            "grouped_df"
          ],
          "decorators": [],
          "docstring": "Format grouped success rates as a markdown table.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_format_distribution_shift_analysis",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 799,
          "line_end": 826,
          "args": [
            "self",
            "shift_analysis"
          ],
          "decorators": [],
          "docstring": "Format distribution shift analysis as markdown.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_generate_recommendations",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 828,
          "line_end": 889,
          "args": [
            "self",
            "metrics"
          ],
          "decorators": [],
          "docstring": "Generate specific recommendations based on the analysis.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "aggregate_monte_carlo_metrics",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 891,
          "line_end": 938,
          "args": [
            "self",
            "results_df"
          ],
          "decorators": [],
          "docstring": "\n        Compute comprehensive aggregated metrics from Monte Carlo results.\n\n        Args:\n            results_df: DataFrame with simulation results\n\n        Returns:\n            Dict containing all aggregated metrics\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_analyze_distribution_shift_performance",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 940,
          "line_end": 969,
          "args": [
            "self",
            "results_df"
          ],
          "decorators": [],
          "docstring": "Analyze performance across different distribution shift levels.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "_create_empty_aggregated_metrics",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 971,
          "line_end": 992,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Create empty metrics structure for error cases.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloResultsAnalyzer"
        },
        {
          "name": "__init__",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 1000,
          "line_end": 1007,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize the visualizer.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "create_performance_summary_charts",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 1009,
          "line_end": 1057,
          "args": [
            "self",
            "aggregated_metrics",
            "output_dir"
          ],
          "decorators": [],
          "docstring": "\n        Create bar charts summarizing performance across scenario types.\n\n        Args:\n            aggregated_metrics: Output from aggregate_monte_carlo_metrics()\n            output_dir: Directory to save plots\n\n        Returns:\n            List of created plot file paths\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "create_distribution_shift_plots",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 1059,
          "line_end": 1096,
          "args": [
            "self",
            "aggregated_metrics",
            "output_dir"
          ],
          "decorators": [],
          "docstring": "\n        Create scatter plots showing performance differences under distribution shifts.\n\n        Args:\n            aggregated_metrics: Output from aggregate_monte_carlo_metrics()\n            output_dir: Directory to save plots\n\n        Returns:\n            List of created plot file paths\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_create_success_rate_chart",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 1098,
          "line_end": 1144,
          "args": [
            "self",
            "success_data",
            "save_path"
          ],
          "decorators": [],
          "docstring": "Create bar chart of success rates by scenario type.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_create_detection_performance_chart",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 1146,
          "line_end": 1189,
          "args": [
            "self",
            "detection_data",
            "save_path"
          ],
          "decorators": [],
          "docstring": "Create bar chart of false positive/negative rates.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_create_safety_margins_chart",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 1191,
          "line_end": 1235,
          "args": [
            "self",
            "margins_data",
            "save_path"
          ],
          "decorators": [],
          "docstring": "Create bar chart of safety margins.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_create_shift_performance_scatter",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 1237,
          "line_end": 1300,
          "args": [
            "self",
            "shift_data",
            "save_path"
          ],
          "decorators": [],
          "docstring": "Create scatter plot of performance vs distribution shift level.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "analyze_monte_carlo_results",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 1304,
          "line_end": 1355,
          "args": [
            "results_file",
            "output_dir"
          ],
          "decorators": [],
          "docstring": "\n    Complete Monte Carlo analysis pipeline from results file to metrics and plots.\n\n    Args:\n        results_file: Path to results.json or results.csv file\n        output_dir: Directory for analysis outputs\n\n    Returns:\n        Dict with aggregated metrics and plot paths\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "MonteCarloResultsAnalyzer",
            "MonteCarloVisualizer"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "llm_atc\\metrics\\safety_margin_quantifier.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 64,
          "line_end": 86,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "calculate_safety_margins",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 88,
          "line_end": 153,
          "args": [
            "self",
            "conflict_geometry",
            "resolution_maneuver",
            "environmental_conditions"
          ],
          "decorators": [],
          "docstring": "\n        Calculate comprehensive safety margins for a conflict resolution\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "SafetyMargin",
            "ConflictGeometry"
          ],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_apply_resolution_maneuver",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 155,
          "line_end": 222,
          "args": [
            "self",
            "geometry",
            "maneuver"
          ],
          "decorators": [],
          "docstring": "Apply resolution maneuver and predict future conflict geometry",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictGeometry"
          ],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_predict_position",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 224,
          "line_end": 259,
          "args": [
            "self",
            "position",
            "velocity",
            "time"
          ],
          "decorators": [],
          "docstring": "Predict future position based on current velocity",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_calculate_closest_approach",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 261,
          "line_end": 302,
          "args": [
            "self",
            "pos1",
            "pos2",
            "vel1",
            "vel2"
          ],
          "decorators": [],
          "docstring": "Calculate time and distance of closest approach",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_calculate_horizontal_margin",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 304,
          "line_end": 311,
          "args": [
            "self",
            "geometry"
          ],
          "decorators": [],
          "docstring": "Calculate horizontal separation margin",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictGeometry"
          ],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_calculate_vertical_margin",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 313,
          "line_end": 319,
          "args": [
            "self",
            "geometry"
          ],
          "decorators": [],
          "docstring": "Calculate vertical separation margin",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictGeometry"
          ],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_calculate_temporal_margin",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 321,
          "line_end": 327,
          "args": [
            "self",
            "geometry"
          ],
          "decorators": [],
          "docstring": "Calculate temporal margin (time available for corrective action)",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictGeometry"
          ],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_calculate_effective_margin",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 329,
          "line_end": 350,
          "args": [
            "self",
            "h_margin",
            "v_margin",
            "t_margin"
          ],
          "decorators": [],
          "docstring": "Calculate effective combined margin using weighted approach",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_calculate_total_uncertainty",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 352,
          "line_end": 371,
          "args": [
            "self",
            "environmental_conditions"
          ],
          "decorators": [],
          "docstring": "Calculate total uncertainty in the system",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_calculate_baseline_margin",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 373,
          "line_end": 395,
          "args": [
            "self",
            "geometry"
          ],
          "decorators": [],
          "docstring": "Calculate baseline margin without any resolution maneuver",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictGeometry"
          ],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_determine_safety_level",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 397,
          "line_end": 405,
          "args": [
            "self",
            "effective_margin"
          ],
          "decorators": [],
          "docstring": "Determine safety level based on effective margin",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "_create_default_safety_margin",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 407,
          "line_end": 417,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Create default safety margin for error cases",
          "imports_used": [],
          "intra_repo_calls": [
            "SafetyMargin"
          ],
          "is_method": true,
          "class_name": "SafetyMarginQuantifier"
        },
        {
          "name": "__init__",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 423,
          "line_end": 425,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "SafetyMarginQuantifier"
          ],
          "is_method": true,
          "class_name": "SafetyMetricsAggregator"
        },
        {
          "name": "add_conflict_resolution",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 427,
          "line_end": 468,
          "args": [
            "self",
            "conflict_id",
            "geometry",
            "llm_resolution",
            "baseline_resolution",
            "environmental_conditions"
          ],
          "decorators": [],
          "docstring": "Add a conflict resolution case and compute comparative metrics",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictGeometry"
          ],
          "is_method": true,
          "class_name": "SafetyMetricsAggregator"
        },
        {
          "name": "generate_safety_summary",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 470,
          "line_end": 514,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Generate comprehensive safety summary across all conflicts",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "SafetyMetricsAggregator"
        },
        {
          "name": "export_detailed_metrics",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 516,
          "line_end": 564,
          "args": [
            "self",
            "filepath"
          ],
          "decorators": [],
          "docstring": "Export detailed metrics to JSON file",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "SafetyMetricsAggregator"
        },
        {
          "name": "calc_separation_margin",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 567,
          "line_end": 622,
          "args": [
            "trajectories"
          ],
          "decorators": [],
          "docstring": "\n    Calculate horizontal and vertical separation margins from trajectories.\n\n    Args:\n        trajectories: List of aircraft trajectories with format:\n                     [{'aircraft_id': str, 'path': [{'lat': float, 'lon': float,\n                       'alt': float, 'time': float}]}]\n\n    Returns:\n        Dict with 'hz' (horizontal) and 'vt' (vertical) margins in nm and ft\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "SeparationStandard"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "calc_efficiency_penalty",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 625,
          "line_end": 668,
          "args": [
            "planned_path",
            "executed_path"
          ],
          "decorators": [],
          "docstring": "\n    Calculate efficiency penalty as extra distance traveled due to conflict resolution.\n\n    Args:\n        planned_path: Original planned trajectory points\n                     [{'lat': float, 'lon': float, 'alt': float, 'time': float}]\n        executed_path: Actual executed trajectory points (same format)\n\n    Returns:\n        Extra distance in nautical miles\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "calculate_path_distance"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "calculate_path_distance",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 641,
          "line_end": 660,
          "args": [
            "path"
          ],
          "decorators": [],
          "docstring": "Calculate total distance of a path in nautical miles",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "count_interventions",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 671,
          "line_end": 715,
          "args": [
            "commands"
          ],
          "decorators": [],
          "docstring": "\n    Count the number of ATC interventions in a command sequence.\n\n    Args:\n        commands: List of ATC commands with format:\n                 [{'type': str, 'aircraft_id': str, 'timestamp': float, ...}]\n\n    Returns:\n        Number of intervention commands\n    ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        }
      ],
      "llm_atc\\metrics\\__init__.py": [
        {
          "name": "analyze_hallucinations_in_log",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 32,
          "line_end": 34,
          "args": [
            "_log_file"
          ],
          "decorators": [],
          "docstring": "Analyze hallucinations in log file - simplified implementation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "compute_metrics",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 37,
          "line_end": 144,
          "args": [
            "log_file"
          ],
          "decorators": [],
          "docstring": "Compute hallucination and performance metrics from simulation logs.",
          "imports_used": [],
          "intra_repo_calls": [
            "create_empty_metrics",
            "analyze_hallucinations_in_log"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "create_empty_metrics",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 147,
          "line_end": 164,
          "args": [],
          "decorators": [],
          "docstring": "Create empty metrics structure when no data is available.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "print_metrics_summary",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 167,
          "line_end": 196,
          "args": [
            "metrics"
          ],
          "decorators": [],
          "docstring": "Print a formatted summary of the metrics using logging.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "calc_fp_fn",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 199,
          "line_end": 230,
          "args": [
            "pred_conflicts",
            "gt_conflicts"
          ],
          "decorators": [],
          "docstring": "Calculate false positive and false negative rates.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "calc_path_extra",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 233,
          "line_end": 268,
          "args": [
            "actual_traj",
            "original_traj"
          ],
          "decorators": [],
          "docstring": "Calculate extra distance traveled due to resolution maneuvers.",
          "imports_used": [],
          "intra_repo_calls": [
            "calc_trajectory_distance"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "calc_trajectory_distance",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 241,
          "line_end": 263,
          "args": [
            "traj"
          ],
          "decorators": [],
          "docstring": "Calculate total distance of a trajectory.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "aggregate_thesis_metrics",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 271,
          "line_end": 330,
          "args": [
            "results_dir"
          ],
          "decorators": [],
          "docstring": "Aggregate metrics from multiple test result files for thesis analysis.",
          "imports_used": [],
          "intra_repo_calls": [
            "create_empty_metrics",
            "compute_metrics"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "plot_metrics_comparison",
          "file_path": "llm_atc\\metrics\\__init__.py",
          "line_start": 334,
          "line_end": 414,
          "args": [
            "llm_metrics",
            "baseline_metrics",
            "save_path"
          ],
          "decorators": [],
          "docstring": "Create comparison plots between LLM and baseline metrics.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        }
      ],
      "llm_atc\\tools\\bluesky_tools.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 41,
          "line_end": 43,
          "args": [
            "self",
            "config_path"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyConfig"
        },
        {
          "name": "_find_config_file",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 45,
          "line_end": 63,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Find the BlueSky configuration file",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyConfig"
        },
        {
          "name": "_create_default_config",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 65,
          "line_end": 113,
          "args": [
            "self",
            "path"
          ],
          "decorators": [],
          "docstring": "Create a default configuration file",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyConfig"
        },
        {
          "name": "_load_config",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 115,
          "line_end": 126,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Load configuration from file",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyConfig"
        },
        {
          "name": "_get_default_config",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 128,
          "line_end": 143,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get default configuration",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyConfig"
        },
        {
          "name": "get",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 145,
          "line_end": 156,
          "args": [
            "self",
            "key_path",
            "default"
          ],
          "decorators": [],
          "docstring": "Get configuration value by dot-separated path",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyConfig"
        },
        {
          "name": "__init__",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 166,
          "line_end": 175,
          "args": [
            "self",
            "strict_mode"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_initialize_bluesky",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 177,
          "line_end": 215,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize BlueSky simulator",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_setup_simulation",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 217,
          "line_end": 261,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Setup simulation parameters",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_test_network_connection",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 263,
          "line_end": 281,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Test network connection to BlueSky",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "is_available",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 283,
          "line_end": 285,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Check if BlueSky is available and initialized",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "get_aircraft_data",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 287,
          "line_end": 369,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get real aircraft data from BlueSky",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "get_conflict_data",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 371,
          "line_end": 438,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get real conflict data from BlueSky",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_calculate_horizontal_separation",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 440,
          "line_end": 457,
          "args": [
            "self",
            "ac1_idx",
            "ac2_idx"
          ],
          "decorators": [],
          "docstring": "Calculate horizontal separation between two aircraft",
          "imports_used": [],
          "intra_repo_calls": [
            "haversine_distance"
          ],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_assess_conflict_severity",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 459,
          "line_end": 472,
          "args": [
            "self",
            "h_sep",
            "v_sep"
          ],
          "decorators": [],
          "docstring": "Assess conflict severity based on separation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "send_bluesky_command",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 474,
          "line_end": 519,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Send command to BlueSky simulator",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "step_simulation_real",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 521,
          "line_end": 565,
          "args": [
            "self",
            "minutes",
            "dtmult"
          ],
          "decorators": [],
          "docstring": "Step the real BlueSky simulation forward",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "reset_simulation_real",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 567,
          "line_end": 610,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Reset the real BlueSky simulation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_get_mock_aircraft_data",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 612,
          "line_end": 680,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Generate mock aircraft data when BlueSky unavailable",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_get_mock_conflict_data",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 682,
          "line_end": 705,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Generate mock conflict data when BlueSky unavailable",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_simulate_command_execution",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 707,
          "line_end": 720,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Simulate command execution when BlueSky unavailable",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_simulate_step",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 722,
          "line_end": 733,
          "args": [
            "self",
            "minutes",
            "dtmult"
          ],
          "decorators": [],
          "docstring": "Simulate stepping when BlueSky unavailable",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "_simulate_reset",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 735,
          "line_end": 745,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Simulate reset when BlueSky unavailable",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyInterface"
        },
        {
          "name": "set_strict_mode",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 752,
          "line_end": 756,
          "args": [
            "enabled"
          ],
          "decorators": [],
          "docstring": "Enable or disable strict mode for BlueSky operations",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "haversine_distance",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 791,
          "line_end": 820,
          "args": [
            "lat1",
            "lon1",
            "lat2",
            "lon2"
          ],
          "decorators": [],
          "docstring": "\n    Calculate the great circle distance between two points on Earth using the Haversine formula\n\n    Args:\n        lat1, lon1: Latitude and longitude of first point in degrees\n        lat2, lon2: Latitude and longitude of second point in degrees\n\n    Returns:\n        Distance in nautical miles\n    ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "get_all_aircraft_info",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 823,
          "line_end": 846,
          "args": [],
          "decorators": [],
          "docstring": "\n    Get information about all aircraft in the simulation\n\n    Returns:\n        Dictionary containing aircraft information\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "get_conflict_info",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 849,
          "line_end": 872,
          "args": [],
          "decorators": [],
          "docstring": "\n    Get information about current conflicts in the simulation\n\n    Returns:\n        Dictionary containing conflict information\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "continue_monitoring",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 875,
          "line_end": 900,
          "args": [],
          "decorators": [],
          "docstring": "\n    Continue monitoring aircraft without taking action\n\n    Returns:\n        Status information about monitoring continuation\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "send_command",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 903,
          "line_end": 977,
          "args": [
            "command"
          ],
          "decorators": [],
          "docstring": "\n    Send a command to the BlueSky simulator\n\n    Args:\n        command: BlueSky command string (e.g., \"ALT AAL123 FL350\")\n\n    Returns:\n        Command execution result\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "search_experience_library",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 980,
          "line_end": 1052,
          "args": [
            "scenario_type",
            "similarity_threshold"
          ],
          "decorators": [],
          "docstring": "\n    Search the experience library for similar scenarios\n\n    Args:\n        scenario_type: Type of scenario to search for\n        similarity_threshold: Minimum similarity score for matches\n\n    Returns:\n        Dictionary containing matching experiences\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "get_weather_info",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1055,
          "line_end": 1109,
          "args": [
            "lat",
            "lon"
          ],
          "decorators": [],
          "docstring": "\n    Get weather information for specified location or current area\n\n    Args:\n        lat: Latitude (optional)\n        lon: Longitude (optional)\n\n    Returns:\n        Weather information dictionary\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "get_airspace_info",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1112,
          "line_end": 1170,
          "args": [],
          "decorators": [],
          "docstring": "\n    Get information about current airspace restrictions and constraints\n\n    Returns:\n        Airspace information dictionary\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "get_distance",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1173,
          "line_end": 1239,
          "args": [
            "aircraft_id1",
            "aircraft_id2"
          ],
          "decorators": [],
          "docstring": "\n    Compute current horizontal and vertical separation between two aircraft.\n\n    Args:\n        aircraft_id1: ID of first aircraft\n        aircraft_id2: ID of second aircraft\n\n    Returns:\n        Dictionary with separation distances:\n        - horizontal_nm: Horizontal separation in nautical miles\n        - vertical_ft: Vertical separation in feet\n        - total_3d_nm: Total 3D separation in nautical miles\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "get_all_aircraft_info",
            "haversine_distance",
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "_haversine_distance",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1242,
          "line_end": 1271,
          "args": [
            "lat1",
            "lon1",
            "lat2",
            "lon2"
          ],
          "decorators": [],
          "docstring": "\n    Calculate the great circle distance between two points on Earth in nautical miles.\n\n    Args:\n        lat1, lon1: Latitude and longitude of first point in degrees\n        lat2, lon2: Latitude and longitude of second point in degrees\n\n    Returns:\n        Distance in nautical miles\n    ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "step_simulation",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1274,
          "line_end": 1311,
          "args": [
            "minutes",
            "dtmult"
          ],
          "decorators": [],
          "docstring": "\n    Advance the BlueSky simulation by a number of minutes.\n\n    Args:\n        minutes: Number of minutes to advance the simulation\n        dtmult: Time multiplier (simulation speed factor)\n\n    Returns:\n        Status dictionary with simulation step information\n    ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "reset_simulation",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1314,
          "line_end": 1351,
          "args": [],
          "decorators": [],
          "docstring": "\n    Reset the BlueSky simulation to initial state.\n\n    Returns:\n        Status dictionary with reset information\n    ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "get_minimum_separation",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1354,
          "line_end": 1369,
          "args": [],
          "decorators": [],
          "docstring": "\n    Get the current minimum separation standards.\n\n    Returns:\n        Dictionary with minimum separation requirements\n    ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "check_separation_violation",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1372,
          "line_end": 1432,
          "args": [
            "aircraft_id1",
            "aircraft_id2"
          ],
          "decorators": [],
          "docstring": "\n    Check if two aircraft are violating separation standards.\n\n    Args:\n        aircraft_id1: ID of first aircraft\n        aircraft_id2: ID of second aircraft\n\n    Returns:\n        Dictionary with violation status and details\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "get_minimum_separation",
            "get_distance"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "execute_tool",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1452,
          "line_end": 1485,
          "args": [
            "tool_name"
          ],
          "decorators": [],
          "docstring": "\n    Execute a tool by name with provided arguments\n\n    Args:\n        tool_name: Name of the tool to execute\n        **kwargs: Arguments to pass to the tool\n\n    Returns:\n        Tool execution result\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyToolsError"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "get_available_tools",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 1488,
          "line_end": 1490,
          "args": [],
          "decorators": [],
          "docstring": "Get list of available tools",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        }
      ],
      "llm_atc\\tools\\enhanced_conflict_detector.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 65,
          "line_end": 78,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictDetectionMethod"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "detect_conflicts_comprehensive",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 80,
          "line_end": 119,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "\n        Detect conflicts using all available methods and cross-validate results\n\n        Returns:\n            List of validated conflict data\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_detect_with_swarm",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 121,
          "line_end": 169,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Detect conflicts using BlueSky SWARM method",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_detect_with_statebased",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 171,
          "line_end": 221,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Detect conflicts using BlueSky STATEBASED method",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_detect_with_enhanced_analysis",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 223,
          "line_end": 238,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Detect conflicts using enhanced geometric analysis",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_analyze_aircraft_pair",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 240,
          "line_end": 319,
          "args": [
            "self",
            "ac1_idx",
            "ac2_idx",
            "method"
          ],
          "decorators": [],
          "docstring": "\n        Analyze specific aircraft pair for conflicts with CPA calculation\n        Implements the 300s time horizon check as requested\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_calculate_cpa",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 321,
          "line_end": 414,
          "args": [
            "self",
            "lat1",
            "lon1",
            "alt1",
            "hdg1",
            "spd1",
            "vs1",
            "lat2",
            "lon2",
            "alt2",
            "hdg2",
            "spd2",
            "vs2"
          ],
          "decorators": [],
          "docstring": "\n        Calculate Closest Point of Approach (CPA) for two aircraft\n\n        Returns:\n            Tuple of (time_to_cpa, distance_at_cpa, min_horizontal_sep, min_vertical_sep)\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_calculate_horizontal_distance",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 416,
          "line_end": 434,
          "args": [
            "self",
            "lat1",
            "lon1",
            "lat2",
            "lon2"
          ],
          "decorators": [],
          "docstring": "Calculate horizontal distance between two points in nautical miles",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_assess_conflict_severity",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 436,
          "line_end": 453,
          "args": [
            "self",
            "h_sep",
            "v_sep",
            "time_to_cpa",
            "violates_icao"
          ],
          "decorators": [],
          "docstring": "Assess conflict severity based on ICAO standards and time to conflict",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_calculate_confidence",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 455,
          "line_end": 475,
          "args": [
            "self",
            "method",
            "h_sep",
            "v_sep",
            "time_to_cpa"
          ],
          "decorators": [],
          "docstring": "Calculate confidence score for conflict detection",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_cross_validate_conflicts",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 477,
          "line_end": 492,
          "args": [
            "self",
            "all_conflicts"
          ],
          "decorators": [],
          "docstring": "Cross-validate conflicts detected by multiple methods",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_merge_conflict_detections",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 494,
          "line_end": 533,
          "args": [
            "self",
            "detections"
          ],
          "decorators": [],
          "docstring": "Merge multiple conflict detections for the same aircraft pair",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_get_aircraft_pair_key",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 535,
          "line_end": 537,
          "args": [
            "self",
            "ac1",
            "ac2"
          ],
          "decorators": [],
          "docstring": "Get consistent key for aircraft pair (sorted order)",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "validate_llm_conflicts",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 539,
          "line_end": 566,
          "args": [
            "self",
            "llm_conflicts"
          ],
          "decorators": [],
          "docstring": "\n        Validate LLM-detected conflicts against BlueSky ground truth\n        Returns list of (aircraft1, aircraft2, confidence) for validated conflicts\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_mock_conflict_detection",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 568,
          "line_end": 586,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Mock conflict detection when BlueSky is not available",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        }
      ],
      "llm_atc\\tools\\enhanced_conflict_detector_clean.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 65,
          "line_end": 78,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictDetectionMethod"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "detect_conflicts_comprehensive",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 80,
          "line_end": 119,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "\n        Detect conflicts using all available methods and cross-validate results\n\n        Returns:\n            List of validated conflict data\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_detect_with_swarm",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 121,
          "line_end": 169,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Detect conflicts using BlueSky SWARM method",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_detect_with_statebased",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 171,
          "line_end": 221,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Detect conflicts using BlueSky STATEBASED method",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_detect_with_enhanced_analysis",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 223,
          "line_end": 238,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Detect conflicts using enhanced geometric analysis",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_analyze_aircraft_pair",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 240,
          "line_end": 319,
          "args": [
            "self",
            "ac1_idx",
            "ac2_idx",
            "method"
          ],
          "decorators": [],
          "docstring": "\n        Analyze specific aircraft pair for conflicts with CPA calculation\n        Implements the 300s time horizon check as requested\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_calculate_cpa",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 321,
          "line_end": 414,
          "args": [
            "self",
            "lat1",
            "lon1",
            "alt1",
            "hdg1",
            "spd1",
            "vs1",
            "lat2",
            "lon2",
            "alt2",
            "hdg2",
            "spd2",
            "vs2"
          ],
          "decorators": [],
          "docstring": "\n        Calculate Closest Point of Approach (CPA) for two aircraft\n\n        Returns:\n            Tuple of (time_to_cpa, distance_at_cpa, min_horizontal_sep, min_vertical_sep)\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_calculate_horizontal_distance",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 416,
          "line_end": 434,
          "args": [
            "self",
            "lat1",
            "lon1",
            "lat2",
            "lon2"
          ],
          "decorators": [],
          "docstring": "Calculate horizontal distance between two points in nautical miles",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_assess_conflict_severity",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 436,
          "line_end": 453,
          "args": [
            "self",
            "h_sep",
            "v_sep",
            "time_to_cpa",
            "violates_icao"
          ],
          "decorators": [],
          "docstring": "Assess conflict severity based on ICAO standards and time to conflict",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_calculate_confidence",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 455,
          "line_end": 475,
          "args": [
            "self",
            "method",
            "h_sep",
            "v_sep",
            "time_to_cpa"
          ],
          "decorators": [],
          "docstring": "Calculate confidence score for conflict detection",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_cross_validate_conflicts",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 477,
          "line_end": 492,
          "args": [
            "self",
            "all_conflicts"
          ],
          "decorators": [],
          "docstring": "Cross-validate conflicts detected by multiple methods",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_merge_conflict_detections",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 494,
          "line_end": 533,
          "args": [
            "self",
            "detections"
          ],
          "decorators": [],
          "docstring": "Merge multiple conflict detections for the same aircraft pair",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_get_aircraft_pair_key",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 535,
          "line_end": 537,
          "args": [
            "self",
            "ac1",
            "ac2"
          ],
          "decorators": [],
          "docstring": "Get consistent key for aircraft pair (sorted order)",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "validate_llm_conflicts",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 539,
          "line_end": 566,
          "args": [
            "self",
            "llm_conflicts"
          ],
          "decorators": [],
          "docstring": "\n        Validate LLM-detected conflicts against BlueSky ground truth\n        Returns list of (aircraft1, aircraft2, confidence) for validated conflicts\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        },
        {
          "name": "_mock_conflict_detection",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 568,
          "line_end": 586,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Mock conflict detection when BlueSky is not available",
          "imports_used": [],
          "intra_repo_calls": [
            "ConflictData"
          ],
          "is_method": true,
          "class_name": "EnhancedConflictDetector"
        }
      ],
      "llm_atc\\tools\\llm_prompt_engine.py": [
        {
          "name": "__init__",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 56,
          "line_end": 102,
          "args": [
            "self",
            "model",
            "enable_function_calls",
            "aircraft_id_regex",
            "enable_streaming",
            "enable_caching",
            "enable_optimized_prompts"
          ],
          "decorators": [],
          "docstring": "\n        Initialize the LLM prompt engine.\n\n        Args:\n            model: LLM model to use for queries\n            enable_function_calls: Whether to enable function calling capabilities\n            aircraft_id_regex: Regular expression pattern for validating aircraft callsigns.\n                              Default pattern accepts alphanumeric characters and hyphens.\n                              Examples: r'^[A-Z]{2,4}\\d{2,4}[A-Z]?$' for traditional ICAO format,\n                                       r'^[A-Z0-9-]+$' for flexible alphanumeric with hyphens.\n            enable_streaming: Use streaming for faster responses\n            enable_caching: Cache responses for repeated scenarios\n                              enable_optimized_prompts: Use optimized, shorter prompt templates\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "LLMClient"
          ],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_init_prompt_templates",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 104,
          "line_end": 321,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize standardized prompt templates",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "format_conflict_prompt",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 323,
          "line_end": 385,
          "args": [
            "self",
            "conflict_info"
          ],
          "decorators": [],
          "docstring": "\n        Create a descriptive natural-language prompt for conflict resolution.\n\n        Args:\n            conflict_info: Dictionary containing conflict and aircraft information\n\n        Returns:\n            Formatted prompt string for LLM query\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "format_detector_prompt",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 387,
          "line_end": 488,
          "args": [
            "self",
            "aircraft_states",
            "time_horizon",
            "cpa_data"
          ],
          "decorators": [],
          "docstring": "\n        Create a prompt for LLM-based conflict detection with enhanced sector support.\n\n        Args:\n            aircraft_states: List of aircraft state dictionaries\n            time_horizon: Time horizon in minutes for conflict detection\n            cpa_data: Optional Closest Point of Approach data with additional context\n\n        Returns:\n            Formatted detection prompt string\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "aircraft_list"
          ],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "parse_resolution_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 490,
          "line_end": 597,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "\n        Extract BlueSky command from LLM response.\n\n        Args:\n            response_text: Raw LLM response text\n\n        Returns:\n            Parsed ResolutionResponse object or None if parsing fails\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ResolutionResponse"
          ],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "parse_detector_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 599,
          "line_end": 694,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "\n        Parse LLM detector response with enhanced validation for sector scenarios.\n\n        Args:\n            response_text: Raw response from LLM\n\n        Returns:\n            Parsed detection results with validation status\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_is_distilled_model_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 696,
          "line_end": 709,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Check if this is a response from the distilled BlueSky Gym model",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_parse_distilled_model_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 711,
          "line_end": 781,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Parse response from the fine-tuned BlueSky Gym distilled model",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_parse_detector_response_legacy",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 783,
          "line_end": 848,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Legacy text-based parsing for detector responses",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_validate_detector_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 850,
          "line_end": 886,
          "args": [
            "self",
            "json_data"
          ],
          "decorators": [],
          "docstring": "Validate detector response for completeness and correctness",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_validate_aircraft_pairs",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 888,
          "line_end": 903,
          "args": [
            "self",
            "pairs"
          ],
          "decorators": [],
          "docstring": "Validate and normalize aircraft pairs",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_validate_confidence",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 905,
          "line_end": 912,
          "args": [
            "self",
            "confidence"
          ],
          "decorators": [],
          "docstring": "Validate and normalize confidence score",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_validate_priority",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 914,
          "line_end": 921,
          "args": [
            "self",
            "priority"
          ],
          "decorators": [],
          "docstring": "Validate and normalize priority level",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_validate_sector_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 923,
          "line_end": 952,
          "args": [
            "self",
            "json_data"
          ],
          "decorators": [],
          "docstring": "Additional validation for sector scenarios",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_validate_calculation_details",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 954,
          "line_end": 978,
          "args": [
            "self",
            "calc_details"
          ],
          "decorators": [],
          "docstring": "Validate calculation details for mathematical accuracy",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_extract_json_from_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 980,
          "line_end": 986,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Extract JSON object from response text",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "get_conflict_resolution",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 988,
          "line_end": 1043,
          "args": [
            "self",
            "conflict_info",
            "use_function_calls"
          ],
          "decorators": [],
          "docstring": "\n        High-level API for getting conflict resolution from LLM.\n\n        Args:\n            conflict_info: Conflict scenario information\n            use_function_calls: Override function calling setting\n\n        Returns:\n            BlueSky command string or None if resolution fails\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "get_conflict_resolution_with_prompts",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1045,
          "line_end": 1107,
          "args": [
            "self",
            "conflict_info",
            "use_function_calls"
          ],
          "decorators": [],
          "docstring": "\n        Enhanced API for getting conflict resolution that returns prompt and response data.\n\n        Args:\n            conflict_info: Conflict scenario information\n            use_function_calls: Override function calling setting\n\n        Returns:\n            Dictionary with resolution data including prompt and response\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "detect_conflict_via_llm",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1109,
          "line_end": 1148,
          "args": [
            "self",
            "aircraft_states",
            "time_horizon",
            "cpa_data"
          ],
          "decorators": [],
          "docstring": "\n        High-level API for LLM-based conflict detection.\n\n        Args:\n            aircraft_states: List of aircraft state dictionaries\n            time_horizon: Time horizon in minutes\n            cpa_data: Optional Closest Point of Approach data with additional context\n                     including timing, separation distances, and severity information\n\n        Returns:\n            Dictionary with detection results\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "detect_conflict_via_llm_with_prompts",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1150,
          "line_end": 1197,
          "args": [
            "self",
            "aircraft_states",
            "time_horizon",
            "cpa_data"
          ],
          "decorators": [],
          "docstring": "\n        Enhanced API for LLM-based conflict detection that returns prompt and response data.\n\n        Args:\n            aircraft_states: List of aircraft state dictionaries\n            time_horizon: Time horizon in minutes\n            cpa_data: Optional Closest Point of Approach data with additional context\n\n        Returns:\n            Dictionary with detection results including prompt and response\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "assess_resolution_safety",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1199,
          "line_end": 1237,
          "args": [
            "self",
            "command",
            "conflict_info"
          ],
          "decorators": [],
          "docstring": "\n        Use LLM to assess the safety of a proposed resolution.\n\n        Args:\n            command: Proposed BlueSky command\n            conflict_info: Original conflict information\n\n        Returns:\n            Safety assessment dictionary\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_get_fallback_conflict_prompt",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1241,
          "line_end": 1251,
          "args": [
            "self",
            "conflict_info"
          ],
          "decorators": [],
          "docstring": "Generate a simple fallback prompt when main formatting fails",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_parse_function_call_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1253,
          "line_end": 1274,
          "args": [
            "self",
            "response_dict"
          ],
          "decorators": [],
          "docstring": "Parse function call response into ResolutionResponse",
          "imports_used": [],
          "intra_repo_calls": [
            "ResolutionResponse"
          ],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_extract_bluesky_command",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1276,
          "line_end": 1422,
          "args": [
            "self",
            "text"
          ],
          "decorators": [],
          "docstring": "\n        Extract BlueSky command using simplified two-pass approach.\n\n        First pass: Look for explicit BlueSky commands (HDG/ALT/SPD/VS)\n        Second pass: Look for natural language patterns\n        Third pass: Check for function call format\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_normalize_bluesky_command",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1424,
          "line_end": 1474,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Normalize and validate BlueSky command format using configurable aircraft ID pattern",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_extract_aircraft_id",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1476,
          "line_end": 1496,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Extract aircraft ID from BlueSky command using configurable pattern",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_determine_maneuver_type",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1498,
          "line_end": 1513,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Determine maneuver type from BlueSky command",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_parse_aircraft_pairs",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1515,
          "line_end": 1541,
          "args": [
            "self",
            "pairs_text"
          ],
          "decorators": [],
          "docstring": "Parse aircraft pairs from text using configurable aircraft ID pattern",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_parse_time_values",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1543,
          "line_end": 1553,
          "args": [
            "self",
            "time_text"
          ],
          "decorators": [],
          "docstring": "Parse time values from text",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_parse_safety_response",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1555,
          "line_end": 1672,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Parse safety assessment response with robust fallbacks for missing fields",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "format_conflict_resolution_prompt_optimized",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1676,
          "line_end": 1718,
          "args": [
            "self",
            "conflict_info"
          ],
          "decorators": [],
          "docstring": "\n        Create optimized conflict resolution prompt (system + user).\n\n        Args:\n            conflict_info: Conflict scenario data\n\n        Returns:\n            Tuple of (system_prompt, user_prompt)\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "format_conflict_detection_prompt_optimized",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1720,
          "line_end": 1750,
          "args": [
            "self",
            "aircraft_states",
            "time_horizon"
          ],
          "decorators": [],
          "docstring": "\n        Create optimized conflict detection prompt.\n\n        Args:\n            aircraft_states: List of aircraft data\n            time_horizon: Detection time horizon in minutes\n\n        Returns:\n            Tuple of (system_prompt, user_prompt)\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "get_conflict_resolution_optimized",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1752,
          "line_end": 1792,
          "args": [
            "self",
            "conflict_info",
            "priority"
          ],
          "decorators": [],
          "docstring": "\n        High-performance conflict resolution API.\n\n        Args:\n            conflict_info: Conflict scenario data\n            priority: Request priority ('low', 'normal', 'high')\n\n        Returns:\n            ResolutionResponse or None if failed\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ResolutionResponse"
          ],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "get_conflict_detection_optimized",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1794,
          "line_end": 1832,
          "args": [
            "self",
            "aircraft_states",
            "time_horizon",
            "priority"
          ],
          "decorators": [],
          "docstring": "\n        High-performance conflict detection API.\n\n        Args:\n            aircraft_states: List of aircraft data\n            time_horizon: Detection time horizon in minutes\n            priority: Request priority\n\n        Returns:\n            Detection results dictionary or None if failed\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_parse_resolution_response_fast",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1834,
          "line_end": 1877,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "\n        Fast resolution response parsing with minimal fallback.\n\n        Args:\n            response_text: LLM response content\n\n        Returns:\n            Parsed response dictionary or None\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_parse_detection_response_fast",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1879,
          "line_end": 1911,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "\n        Fast detection response parsing.\n\n        Args:\n            response_text: LLM response content (expected JSON)\n\n        Returns:\n            Parsed detection results or None\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_extract_aircraft_id_fast",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1913,
          "line_end": 1917,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Fast aircraft ID extraction",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "_determine_maneuver_type_fast",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1919,
          "line_end": 1930,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Fast maneuver type determination",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "get_performance_stats",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1933,
          "line_end": 1941,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get engine performance statistics",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "reset_performance_stats",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1943,
          "line_end": 1945,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Reset performance tracking",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMPromptEngine"
        },
        {
          "name": "quick_resolve_conflict",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1949,
          "line_end": 1981,
          "args": [
            "aircraft_1",
            "aircraft_2",
            "time_to_conflict",
            "engine"
          ],
          "decorators": [],
          "docstring": "\n    Quick conflict resolution with minimal setup.\n\n    Args:\n        aircraft_1: First aircraft data\n        aircraft_2: Second aircraft data\n        time_to_conflict: Time to conflict in seconds\n        engine: Optional engine instance\n\n    Returns:\n        ResolutionResponse or None\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "ResolutionResponse",
            "LLMPromptEngine"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "quick_detect_conflicts",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 1984,
          "line_end": 2000,
          "args": [
            "aircraft_states",
            "engine"
          ],
          "decorators": [],
          "docstring": "\n    Quick conflict detection with minimal setup.\n\n    Args:\n        aircraft_states: List of aircraft data\n        engine: Optional engine instance\n\n    Returns:\n        Detection results dictionary or None\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "LLMPromptEngine"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "BSKY_GYM_LLM\\merge_lora_and_convert.py": [
        {
          "name": "__init__",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 30,
          "line_end": 34,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "check_prerequisites",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 36,
          "line_end": 62,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Check if all required files and directories exist.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "merge_lora_adapter",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 64,
          "line_end": 126,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Merge LoRA adapter with base model.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "_save_model_metadata",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 128,
          "line_end": 161,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Save metadata about the merged model.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "convert_to_gguf",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 163,
          "line_end": 236,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Convert merged model to GGUF format.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "create_ollama_model",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 238,
          "line_end": 313,
          "args": [
            "self",
            "use_gguf"
          ],
          "decorators": [],
          "docstring": "Create Ollama model from merged weights.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "_create_enhanced_modelfile",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 315,
          "line_end": 421,
          "args": [
            "self",
            "model_source",
            "final_loss",
            "eval_loss"
          ],
          "decorators": [],
          "docstring": "Create enhanced Modelfile content.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "_verify_ollama_model",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 423,
          "line_end": 453,
          "args": [
            "self",
            "model_name"
          ],
          "decorators": [],
          "docstring": "Verify the created Ollama model.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "run_complete_pipeline",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 455,
          "line_end": 508,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Run the complete LoRA merge and conversion pipeline.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LoRAMerger"
        },
        {
          "name": "main",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 510,
          "line_end": 522,
          "args": [],
          "decorators": [],
          "docstring": "Main function to run the LoRA merger.",
          "imports_used": [],
          "intra_repo_calls": [
            "LoRAMerger"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "scenarios\\monte_carlo_framework.py": [
        {
          "name": "aircraft_list",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 62,
          "line_end": 86,
          "args": [
            "self"
          ],
          "decorators": [
            "property"
          ],
          "docstring": "Generate aircraft_list for backward compatibility",
          "imports_used": [],
          "intra_repo_calls": [
            "aircraft_list"
          ],
          "is_method": true,
          "class_name": "ScenarioConfiguration"
        },
        {
          "name": "environmental",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 89,
          "line_end": 115,
          "args": [
            "self"
          ],
          "decorators": [
            "property"
          ],
          "docstring": "Generate environmental data for backward compatibility",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioConfiguration"
        },
        {
          "name": "__init__",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 125,
          "line_end": 150,
          "args": [
            "self",
            "ranges_file",
            "distribution_shift_file",
            "ranges_dict"
          ],
          "decorators": [],
          "docstring": "Initialize generator with range configuration",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "_load_ranges",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 152,
          "line_end": 161,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Load scenario ranges from YAML configuration",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "_load_distribution_shift_config",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 163,
          "line_end": 174,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Load distribution shift configuration from YAML",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "_get_default_ranges",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 176,
          "line_end": 202,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Fallback ranges if YAML file is not available",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "sample_from_range",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 204,
          "line_end": 218,
          "args": [
            "self",
            "range_spec"
          ],
          "decorators": [],
          "docstring": "Sample a value from a range specification",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "weighted_choice",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 220,
          "line_end": 226,
          "args": [
            "self",
            "choices",
            "weights"
          ],
          "decorators": [],
          "docstring": "Make a weighted choice from options",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "apply_distribution_shift",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 228,
          "line_end": 391,
          "args": [
            "self",
            "base_ranges",
            "shift_tier"
          ],
          "decorators": [],
          "docstring": "\n        Apply distribution shift to base ranges according to specified tier.\n\n        This function warps the YAML ranges based on the shift configuration,\n        ensuring all concrete values still come from BlueSky command sampling.\n\n        Args:\n            base_ranges: Original ranges from scenario_ranges.yaml\n            shift_tier: Tier name from distribution_shift_levels.yaml\n                       ('in_distribution', 'moderate_shift', 'extreme_shift')\n\n        Returns:\n            Modified ranges with shifts applied\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "generate_scenario",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 393,
          "line_end": 547,
          "args": [
            "self",
            "complexity_tier",
            "force_conflicts",
            "airspace_region",
            "distribution_shift_tier"
          ],
          "decorators": [],
          "docstring": "\n        Generate a complete scenario using BlueSky commands.\n\n        Args:\n            complexity_tier: Scenario complexity level\n            force_conflicts: Whether to force conflict situations\n            airspace_region: Specific airspace region to use\n            distribution_shift_tier: Distribution shift tier to apply\n                                   ('in_distribution', 'moderate_shift', 'extreme_shift')\n\n        Returns:\n            ScenarioConfiguration with all parameters and BlueSky commands\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioConfiguration",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "_generate_environmental_conditions",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 549,
          "line_end": 576,
          "args": [
            "self",
            "ranges"
          ],
          "decorators": [],
          "docstring": "Generate environmental conditions from ranges (shift-aware)",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "_generate_bluesky_commands",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 578,
          "line_end": 675,
          "args": [
            "self",
            "aircraft_count",
            "aircraft_types",
            "positions",
            "speeds",
            "headings",
            "environmental_conditions",
            "force_conflicts",
            "ranges",
            "distribution_shift_tier"
          ],
          "decorators": [],
          "docstring": "Generate BlueSky commands for scenario setup (shift-aware)",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "_generate_conflict_commands",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 677,
          "line_end": 709,
          "args": [
            "self",
            "aircraft_count"
          ],
          "decorators": [],
          "docstring": "Generate commands to create conflict situations",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "_calculate_bearing",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 711,
          "line_end": 727,
          "args": [
            "self",
            "lat1",
            "lon1",
            "lat2",
            "lon2"
          ],
          "decorators": [],
          "docstring": "Calculate bearing between two lat/lon points",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "execute_scenario",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 729,
          "line_end": 786,
          "args": [
            "self",
            "scenario"
          ],
          "decorators": [],
          "docstring": "\n        Execute scenario in BlueSky and return results.\n\n        Args:\n            scenario: Generated scenario configuration\n\n        Returns:\n            Execution results with conflicts detected and command log\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioConfiguration"
          ],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "_mock_execution",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 788,
          "line_end": 815,
          "args": [
            "self",
            "scenario"
          ],
          "decorators": [],
          "docstring": "Mock execution when BlueSky is not available",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioConfiguration",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "generate_scenario_batch",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 817,
          "line_end": 886,
          "args": [
            "self",
            "count",
            "complexity_distribution",
            "distribution_shift_distribution"
          ],
          "decorators": [],
          "docstring": "\n        Generate multiple scenarios for Monte Carlo testing.\n\n        Args:\n            count: Number of scenarios to generate\n            complexity_distribution: Distribution of complexity levels\n            distribution_shift_distribution: Distribution of shift tiers\n\n        Returns:\n            List of generated scenarios\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioConfiguration",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "get_command_log",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 888,
          "line_end": 890,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get the complete command log for validation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "validate_ranges",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 892,
          "line_end": 947,
          "args": [
            "self",
            "scenario"
          ],
          "decorators": [],
          "docstring": "\n        Validate that generated scenario parameters are within specified ranges.\n\n        Args:\n            scenario: Generated scenario to validate\n\n        Returns:\n            Dictionary of validation results\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioConfiguration"
          ],
          "is_method": true,
          "class_name": "BlueSkyScenarioGenerator"
        },
        {
          "name": "generate_scenario",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 951,
          "line_end": 963,
          "args": [
            "complexity_tier",
            "force_conflicts",
            "distribution_shift_tier"
          ],
          "decorators": [],
          "docstring": "Generate a single scenario - convenience function",
          "imports_used": [],
          "intra_repo_calls": [
            "ComplexityTier",
            "ScenarioConfiguration",
            "BlueSkyScenarioGenerator"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "generate_monte_carlo_scenarios",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 966,
          "line_end": 977,
          "args": [
            "count",
            "complexity_distribution",
            "distribution_shift_distribution"
          ],
          "decorators": [],
          "docstring": "Generate multiple scenarios for Monte Carlo testing - convenience function",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioConfiguration",
            "BlueSkyScenarioGenerator"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "scenarios\\monte_carlo_runner.py": [
        {
          "name": "__post_init__",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 138,
          "line_end": 168,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Set defaults for mutable fields",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioType",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "BenchmarkConfiguration"
        },
        {
          "name": "__post_init__",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 237,
          "line_end": 244,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Set defaults for mutable fields",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioResult"
        },
        {
          "name": "__init__",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 255,
          "line_end": 303,
          "args": [
            "self",
            "config"
          ],
          "decorators": [],
          "docstring": "\n        Initialize the Monte Carlo benchmark runner.\n\n        Args:\n            config: Benchmark configuration. If None, uses defaults.\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "set_strict_mode",
            "DetectionComparison",
            "ScenarioResult",
            "ScenarioGenerator",
            "LLMPromptEngine",
            "BenchmarkConfiguration"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_setup_output_directory",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 305,
          "line_end": 319,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Create output directory structure",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_setup_logging",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 321,
          "line_end": 340,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Setup detailed logging for benchmark execution",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_setup_enhanced_logging",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 342,
          "line_end": 375,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Setup enhanced logging with separate loggers for different components",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_init_csv_file",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 377,
          "line_end": 409,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize CSV file with headers",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "run",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 411,
          "line_end": 453,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "\n        Execute the complete Monte Carlo benchmark.\n\n        Returns:\n            Summary statistics and results overview\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_calculate_total_scenarios",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 455,
          "line_end": 479,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Calculate total number of scenarios to be executed",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_run_scenario_batch",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 481,
          "line_end": 556,
          "args": [
            "self",
            "scenario_type",
            "complexity_tier",
            "shift_level"
          ],
          "decorators": [],
          "docstring": "\n        Execute a batch of scenarios for given parameters.\n\n        Args:\n            scenario_type: Type of scenario to generate\n            complexity_tier: Complexity level\n            shift_level: Distribution shift level\n\n        Returns:\n            Number of scenarios successfully executed\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioType",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_generate_scenario",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 558,
          "line_end": 588,
          "args": [
            "self",
            "scenario_type",
            "complexity_tier",
            "shift_level",
            "scenario_id"
          ],
          "decorators": [],
          "docstring": "Generate a scenario based on type and parameters",
          "imports_used": [],
          "intra_repo_calls": [
            "generate_vertical_scenario",
            "ScenarioType",
            "generate_sector_scenario",
            "generate_horizontal_scenario",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_get_aircraft_count_for_complexity",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 590,
          "line_end": 600,
          "args": [
            "self",
            "complexity_tier"
          ],
          "decorators": [],
          "docstring": "Get appropriate aircraft count for complexity level",
          "imports_used": [],
          "intra_repo_calls": [
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_run_single_scenario",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 602,
          "line_end": 679,
          "args": [
            "self",
            "scenario",
            "scenario_id"
          ],
          "decorators": [],
          "docstring": "\n        Execute a single scenario with comprehensive error handling and success tracking.\n\n        Args:\n            scenario: Generated scenario object\n            scenario_id: Unique scenario identifier\n\n        Returns:\n            ScenarioResult with success flag properly set\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioResult",
            "ScenarioType",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_execute_scenario_pipeline",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 681,
          "line_end": 835,
          "args": [
            "self",
            "scenario",
            "scenario_id"
          ],
          "decorators": [],
          "docstring": "\n        Execute the three-stage pipeline for a single scenario.\n\n        Args:\n            scenario: Generated scenario object\n            scenario_id: Unique scenario identifier\n\n        Returns:\n            ScenarioResult with complete execution data\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioResult",
            "ScenarioType",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_reset_bluesky_simulation",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 837,
          "line_end": 844,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Reset BlueSky simulation to clean state - only if scenario doesn't include RESET",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_load_scenario_commands",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 846,
          "line_end": 885,
          "args": [
            "self",
            "scenario"
          ],
          "decorators": [],
          "docstring": "Load scenario commands into BlueSky",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_extract_ground_truth_conflicts",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 887,
          "line_end": 907,
          "args": [
            "self",
            "scenario"
          ],
          "decorators": [],
          "docstring": "Extract ground truth conflicts from scenario",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_detect_conflicts",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 909,
          "line_end": 1035,
          "args": [
            "self",
            "scenario"
          ],
          "decorators": [],
          "docstring": "Perform conflict detection using multiple BlueSky methods for validation",
          "imports_used": [],
          "intra_repo_calls": [
            "EnhancedConflictDetector"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_basic_conflict_detection_fallback",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1037,
          "line_end": 1058,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Basic conflict detection fallback when enhanced detection fails",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_get_aircraft_states_for_llm",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1060,
          "line_end": 1093,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get current aircraft states formatted for LLM",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_validate_llm_conflicts_with_bluesky",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1095,
          "line_end": 1149,
          "args": [
            "self",
            "llm_pairs",
            "bluesky_conflicts"
          ],
          "decorators": [],
          "docstring": "\n        Validate LLM-detected conflicts against BlueSky ground truth\n        to eliminate false positives.\n\n        Args:\n            llm_pairs: Aircraft pairs detected by LLM\n            bluesky_conflicts: Conflicts detected by BlueSky\n\n        Returns:\n            Validated aircraft pairs confirmed by BlueSky\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_resolve_conflicts",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1151,
          "line_end": 1224,
          "args": [
            "self",
            "conflicts",
            "scenario"
          ],
          "decorators": [],
          "docstring": "Generate LLM-based conflict resolutions",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_is_valid_bluesky_command",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1226,
          "line_end": 1253,
          "args": [
            "self",
            "command"
          ],
          "decorators": [],
          "docstring": "Validate if a command is a valid BlueSky command",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_format_conflict_for_llm",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1255,
          "line_end": 1285,
          "args": [
            "self",
            "conflict",
            "scenario"
          ],
          "decorators": [],
          "docstring": "Format conflict data for LLM prompt engine",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_verify_resolutions",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1287,
          "line_end": 1379,
          "args": [
            "self",
            "scenario",
            "resolutions"
          ],
          "decorators": [],
          "docstring": "Verify resolution effectiveness by stepping simulation with adaptive time stepping",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_calculate_all_separations",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1381,
          "line_end": 1417,
          "args": [
            "self",
            "aircraft_info"
          ],
          "decorators": [],
          "docstring": "Calculate separations between all aircraft pairs",
          "imports_used": [],
          "intra_repo_calls": [
            "aircraft_list"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_calculate_scenario_metrics",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1419,
          "line_end": 1460,
          "args": [
            "self",
            "ground_truth",
            "detected",
            "resolutions",
            "verification"
          ],
          "decorators": [],
          "docstring": "Calculate performance metrics for scenario",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_create_error_result",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1462,
          "line_end": 1517,
          "args": [
            "self",
            "scenario_id",
            "scenario_type",
            "complexity_tier",
            "shift_level",
            "error"
          ],
          "decorators": [],
          "docstring": "Create error result for failed scenarios",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioResult",
            "ScenarioType",
            "ComplexityTier"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_generate_summary",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1519,
          "line_end": 1612,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Generate comprehensive summary statistics from all results",
          "imports_used": [],
          "intra_repo_calls": [
            "MonteCarloResultsAnalyzer"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_get_serializable_config",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1614,
          "line_end": 1639,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get configuration as JSON-serializable dictionary with enum conversion",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_generate_summary_by_group",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1641,
          "line_end": 1669,
          "args": [
            "self",
            "df",
            "group_column"
          ],
          "decorators": [],
          "docstring": "Generate success rate and metrics summary by a specific grouping column",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_generate_combined_summary",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1671,
          "line_end": 1706,
          "args": [
            "self",
            "df"
          ],
          "decorators": [],
          "docstring": "Generate summary across all combinations of scenario type, complexity, and shift",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_print_detailed_analysis",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1708,
          "line_end": 1729,
          "args": [
            "self",
            "analysis"
          ],
          "decorators": [],
          "docstring": "Print detailed analysis results to console",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_generate_visualizations",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1731,
          "line_end": 1759,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Generate comprehensive visualizations of results",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_plot_detection_performance",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1761,
          "line_end": 1833,
          "args": [
            "self",
            "df",
            "fig_size"
          ],
          "decorators": [],
          "docstring": "Plot detection performance metrics",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_plot_safety_margins",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1835,
          "line_end": 1906,
          "args": [
            "self",
            "df",
            "fig_size"
          ],
          "decorators": [],
          "docstring": "Plot safety margin distributions",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_plot_efficiency_metrics",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1908,
          "line_end": 1972,
          "args": [
            "self",
            "df",
            "fig_size"
          ],
          "decorators": [],
          "docstring": "Plot efficiency and cost metrics",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_plot_performance_by_type",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 1974,
          "line_end": 2026,
          "args": [
            "self",
            "df",
            "fig_size"
          ],
          "decorators": [],
          "docstring": "Plot performance metrics by scenario type",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_plot_distribution_shift_impact",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 2028,
          "line_end": 2109,
          "args": [
            "self",
            "df",
            "fig_size"
          ],
          "decorators": [],
          "docstring": "Plot impact of distribution shift on performance",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_save_results",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 2111,
          "line_end": 2154,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Save results in multiple formats",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_run_enhanced_scenario",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 2159,
          "line_end": 2188,
          "args": [
            "self",
            "scenario",
            "scenario_id"
          ],
          "decorators": [],
          "docstring": "Enhanced scenario execution with detailed logging",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioResult"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_create_detection_comparison",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 2190,
          "line_end": 2253,
          "args": [
            "self",
            "scenario",
            "scenario_id",
            "result",
            "execution_time"
          ],
          "decorators": [],
          "docstring": "Create detection comparison record",
          "imports_used": [],
          "intra_repo_calls": [
            "DetectionComparison",
            "ScenarioResult"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_write_csv_row",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 2255,
          "line_end": 2287,
          "args": [
            "self",
            "comparison"
          ],
          "decorators": [],
          "docstring": "Write detection comparison to CSV",
          "imports_used": [],
          "intra_repo_calls": [
            "DetectionComparison"
          ],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "_save_detection_analysis",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 2289,
          "line_end": 2333,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Save detection analysis summary",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloBenchmark"
        },
        {
          "name": "run_benchmark_with_config",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 2336,
          "line_end": 2368,
          "args": [
            "config_path"
          ],
          "decorators": [],
          "docstring": "\n    Run Monte Carlo benchmark with configuration file.\n\n    Args:\n        config_path: Path to JSON configuration file\n\n    Returns:\n        Benchmark summary results\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "MonteCarloBenchmark",
            "BenchmarkConfiguration",
            "ScenarioType",
            "ComplexityTier"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "main",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 2371,
          "line_end": 2439,
          "args": [],
          "decorators": [],
          "docstring": "Main entry point for standalone execution",
          "imports_used": [],
          "intra_repo_calls": [
            "MonteCarloBenchmark",
            "BenchmarkConfiguration",
            "run_benchmark_with_config"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "scenarios\\scenario_generator.py": [
        {
          "name": "__post_init__",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 86,
          "line_end": 93,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize optional fields to sensible defaults",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scenario"
        },
        {
          "name": "to_dict",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 95,
          "line_end": 97,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Convert to dictionary for compatibility with existing code",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "Scenario"
        },
        {
          "name": "__init__",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 106,
          "line_end": 142,
          "args": [
            "self",
            "ranges_file",
            "distribution_shift_file"
          ],
          "decorators": [],
          "docstring": "\n        Initialize scenario generator.\n\n        Args:\n            ranges_file: Path to scenario ranges YAML\n            distribution_shift_file: Path to distribution shift config YAML\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyScenarioGenerator"
          ],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "generate_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 144,
          "line_end": 162,
          "args": [
            "self",
            "scenario_type"
          ],
          "decorators": [],
          "docstring": "\n        Dispatcher method to generate scenarios by type.\n\n        Args:\n            scenario_type: Type of scenario to generate\n            **kwargs: Type-specific arguments\n\n        Returns:\n            Generated scenario with ground truth\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioType",
            "Scenario"
          ],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "generate_horizontal_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 164,
          "line_end": 275,
          "args": [
            "self",
            "n_aircraft",
            "conflict",
            "complexity_tier",
            "distribution_shift_tier"
          ],
          "decorators": [],
          "docstring": "\n        Generate horizontal conflict scenario.\n\n        All aircraft at same flight level to eliminate vertical separation.\n        Adjust headings to create/avoid horizontal conflicts.\n\n        Args:\n            n_aircraft: Number of aircraft (2-5)\n            conflict: Whether to create conflicts (True) or safe scenarios (False)\n            complexity_tier: Scenario complexity\n            distribution_shift_tier: Distribution shift level\n\n        Returns:\n            Horizontal conflict scenario with ground truth\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "BlueSkyScenarioGenerator",
            "ScenarioType",
            "ComplexityTier",
            "Scenario"
          ],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "generate_vertical_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 277,
          "line_end": 415,
          "args": [
            "self",
            "n_aircraft",
            "conflict",
            "climb_rates",
            "crossing_altitudes",
            "complexity_tier",
            "distribution_shift_tier"
          ],
          "decorators": [],
          "docstring": "\n        Generate vertical conflict scenario.\n\n        Aircraft at different altitudes with climb/descent creating vertical conflicts.\n\n        Args:\n            n_aircraft: Number of aircraft (2-5)\n            conflict: Whether to create conflicts (True) or safe scenarios (False)\n            climb_rates: List of climb/descent rates in fpm (default: [-1500, 0, 1500])\n            crossing_altitudes: List of target altitudes for vertical maneuvers (default: auto-generated)\n            complexity_tier: Scenario complexity\n            distribution_shift_tier: Distribution shift level\n\n        Returns:\n            Vertical conflict scenario with ground truth\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioType",
            "ComplexityTier",
            "Scenario"
          ],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "generate_sector_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 417,
          "line_end": 490,
          "args": [
            "self",
            "complexity",
            "shift_level",
            "force_conflicts"
          ],
          "decorators": [],
          "docstring": "\n        Generate realistic sector scenario.\n\n        Uses full Monte Carlo generation for organic sector scenarios.\n\n        Args:\n            complexity: Scenario complexity tier\n            shift_level: Distribution shift level\n            force_conflicts: Whether to force conflicts (False for realistic scenarios)\n\n        Returns:\n            Sector scenario with ground truth\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioType",
            "ComplexityTier",
            "Scenario"
          ],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_create_horizontal_conflicts",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 492,
          "line_end": 522,
          "args": [
            "self",
            "aircraft_states"
          ],
          "decorators": [],
          "docstring": "Create heading adjustments to generate horizontal conflicts",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_avoid_horizontal_conflicts",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 524,
          "line_end": 539,
          "args": [
            "self",
            "aircraft_states"
          ],
          "decorators": [],
          "docstring": "Create heading adjustments to avoid horizontal conflicts",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_create_vertical_conflicts",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 541,
          "line_end": 568,
          "args": [
            "self",
            "aircraft_states"
          ],
          "decorators": [],
          "docstring": "Create altitude/climb commands to generate vertical conflicts",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_avoid_vertical_conflicts",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 570,
          "line_end": 583,
          "args": [
            "self",
            "aircraft_states"
          ],
          "decorators": [],
          "docstring": "Ensure safe vertical separation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_create_vertical_conflicts_enhanced",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 585,
          "line_end": 614,
          "args": [
            "self",
            "aircraft_states",
            "climb_rates"
          ],
          "decorators": [],
          "docstring": "Enhanced vertical conflict creation with configurable climb rates",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_avoid_vertical_conflicts_enhanced",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 616,
          "line_end": 655,
          "args": [
            "self",
            "aircraft_states",
            "climb_rates"
          ],
          "decorators": [],
          "docstring": "Enhanced vertical conflict avoidance ensuring >1000ft separation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_optimize_conflict_timing",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 657,
          "line_end": 681,
          "args": [
            "self",
            "aircraft_states",
            "commands"
          ],
          "decorators": [],
          "docstring": "Optimize timing to create near-threshold vertical conflicts",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_add_environmental_commands",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 683,
          "line_end": 701,
          "args": [
            "self",
            "env_conditions"
          ],
          "decorators": [],
          "docstring": "Add environmental condition commands",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_calculate_horizontal_ground_truth",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 703,
          "line_end": 734,
          "args": [
            "self",
            "aircraft_states",
            "expect_conflicts"
          ],
          "decorators": [],
          "docstring": "Calculate ground truth conflicts for horizontal scenarios with time-based analysis",
          "imports_used": [],
          "intra_repo_calls": [
            "GroundTruthConflict"
          ],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_analyze_aircraft_pair_trajectory",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 736,
          "line_end": 880,
          "args": [
            "self",
            "ac1",
            "ac2"
          ],
          "decorators": [],
          "docstring": "\n        Analyze aircraft pair for potential conflicts using proper trajectory projection.\n\n        Returns:\n            dict: Analysis results including conflict status, CPA time, minimum separation\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_calculate_vertical_ground_truth",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 882,
          "line_end": 929,
          "args": [
            "self",
            "aircraft_states",
            "expect_conflicts"
          ],
          "decorators": [],
          "docstring": "Calculate ground truth conflicts for vertical scenarios",
          "imports_used": [],
          "intra_repo_calls": [
            "GroundTruthConflict"
          ],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_calculate_sector_ground_truth",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 931,
          "line_end": 976,
          "args": [
            "self",
            "aircraft_states",
            "base_scenario"
          ],
          "decorators": [],
          "docstring": "Calculate ground truth conflicts for sector scenarios using trajectory analysis",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioConfiguration",
            "GroundTruthConflict"
          ],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_analyze_trajectory_conflict",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 978,
          "line_end": 1063,
          "args": [
            "self",
            "ac1",
            "ac2"
          ],
          "decorators": [],
          "docstring": "Analyze if two aircraft trajectories will conflict",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_determine_conflict_severity",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1065,
          "line_end": 1084,
          "args": [
            "self",
            "horizontal_sep",
            "vertical_sep"
          ],
          "decorators": [],
          "docstring": "Determine conflict severity based on separation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_calculate_bearing",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1086,
          "line_end": 1102,
          "args": [
            "self",
            "lat1",
            "lon1",
            "lat2",
            "lon2"
          ],
          "decorators": [],
          "docstring": "Calculate bearing between two points",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_calculate_distance_nm",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1104,
          "line_end": 1122,
          "args": [
            "self",
            "lat1",
            "lon1",
            "lat2",
            "lon2"
          ],
          "decorators": [],
          "docstring": "Calculate distance between two points in nautical miles",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_are_headings_convergent",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1124,
          "line_end": 1147,
          "args": [
            "self",
            "lat1",
            "lon1",
            "hdg1",
            "lat2",
            "lon2",
            "hdg2"
          ],
          "decorators": [],
          "docstring": "Check if two aircraft headings are convergent",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "_project_position",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1149,
          "line_end": 1181,
          "args": [
            "self",
            "lat",
            "lon",
            "heading",
            "speed_kts",
            "time_min"
          ],
          "decorators": [],
          "docstring": "Project aircraft position based on heading and speed",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "ScenarioGenerator"
        },
        {
          "name": "__init__",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1188,
          "line_end": 1189,
          "args": [
            "self",
            "generator"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioGenerator"
          ],
          "is_method": true,
          "class_name": "HorizontalCREnv"
        },
        {
          "name": "generate_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1191,
          "line_end": 1199,
          "args": [
            "self",
            "n_aircraft",
            "conflict"
          ],
          "decorators": [],
          "docstring": "Generate horizontal conflict scenario",
          "imports_used": [],
          "intra_repo_calls": [
            "Scenario"
          ],
          "is_method": true,
          "class_name": "HorizontalCREnv"
        },
        {
          "name": "__init__",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1205,
          "line_end": 1206,
          "args": [
            "self",
            "generator"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioGenerator"
          ],
          "is_method": true,
          "class_name": "VerticalCREnv"
        },
        {
          "name": "generate_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1208,
          "line_end": 1216,
          "args": [
            "self",
            "n_aircraft",
            "conflict"
          ],
          "decorators": [],
          "docstring": "Generate vertical conflict scenario",
          "imports_used": [],
          "intra_repo_calls": [
            "Scenario"
          ],
          "is_method": true,
          "class_name": "VerticalCREnv"
        },
        {
          "name": "__init__",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1222,
          "line_end": 1223,
          "args": [
            "self",
            "generator"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioGenerator"
          ],
          "is_method": true,
          "class_name": "SectorCREnv"
        },
        {
          "name": "generate_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1225,
          "line_end": 1238,
          "args": [
            "self",
            "complexity",
            "shift_level",
            "force_conflicts"
          ],
          "decorators": [],
          "docstring": "Generate sector scenario",
          "imports_used": [],
          "intra_repo_calls": [
            "ComplexityTier",
            "Scenario"
          ],
          "is_method": true,
          "class_name": "SectorCREnv"
        },
        {
          "name": "generate_horizontal_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1242,
          "line_end": 1247,
          "args": [
            "n_aircraft",
            "conflict"
          ],
          "decorators": [],
          "docstring": "Generate horizontal scenario - convenience function",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioGenerator",
            "Scenario"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "generate_vertical_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1250,
          "line_end": 1255,
          "args": [
            "n_aircraft",
            "conflict"
          ],
          "decorators": [],
          "docstring": "Generate vertical scenario - convenience function",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioGenerator",
            "Scenario"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "generate_sector_scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1258,
          "line_end": 1268,
          "args": [
            "complexity",
            "shift_level",
            "force_conflicts"
          ],
          "decorators": [],
          "docstring": "Generate sector scenario - convenience function",
          "imports_used": [],
          "intra_repo_calls": [
            "ScenarioGenerator",
            "ComplexityTier",
            "Scenario"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "llm_interface\\ensemble.py": [
        {
          "name": "__init__",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 57,
          "line_end": 60,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_initialize_models",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 62,
          "line_end": 167,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize model ensemble configuration",
          "imports_used": [],
          "intra_repo_calls": [
            "ModelConfig",
            "ModelRole"
          ],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_get_available_models",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 169,
          "line_end": 227,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get list of available Ollama models",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "query_ensemble",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 229,
          "line_end": 305,
          "args": [
            "self",
            "prompt",
            "context",
            "require_json",
            "timeout"
          ],
          "decorators": [],
          "docstring": "Query ensemble of models and return consensus response",
          "imports_used": [],
          "intra_repo_calls": [
            "EnsembleResponse"
          ],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_create_role_specific_prompts",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 307,
          "line_end": 367,
          "args": [
            "self",
            "base_prompt",
            "context"
          ],
          "decorators": [],
          "docstring": "Create role-specific prompts for different models",
          "imports_used": [],
          "intra_repo_calls": [
            "ModelRole"
          ],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_query_single_model",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 369,
          "line_end": 418,
          "args": [
            "self",
            "model_config",
            "prompt",
            "require_json"
          ],
          "decorators": [],
          "docstring": "Query a single model in the ensemble",
          "imports_used": [],
          "intra_repo_calls": [
            "ModelConfig"
          ],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_analyze_safety_flags",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 420,
          "line_end": 469,
          "args": [
            "self",
            "responses"
          ],
          "decorators": [],
          "docstring": "Analyze responses for safety flags and concerns",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_calculate_consensus",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 471,
          "line_end": 554,
          "args": [
            "self",
            "responses"
          ],
          "decorators": [],
          "docstring": "Calculate consensus decision from ensemble responses",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_calculate_uncertainty_metrics",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 556,
          "line_end": 587,
          "args": [
            "self",
            "responses"
          ],
          "decorators": [],
          "docstring": "Calculate uncertainty metrics from ensemble responses",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_create_error_response",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 589,
          "line_end": 603,
          "args": [
            "self",
            "error_msg",
            "response_time"
          ],
          "decorators": [],
          "docstring": "Create error response when ensemble fails",
          "imports_used": [],
          "intra_repo_calls": [
            "EnsembleResponse"
          ],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "get_ensemble_statistics",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 605,
          "line_end": 645,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get statistics about ensemble performance",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_clean_json_response",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 647,
          "line_end": 670,
          "args": [
            "self",
            "json_str"
          ],
          "decorators": [],
          "docstring": "Clean and repair common JSON formatting issues",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_create_valid_response_structure",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 672,
          "line_end": 682,
          "args": [
            "self",
            "raw_content"
          ],
          "decorators": [],
          "docstring": "Create a valid response structure from failed JSON parsing",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "_extract_partial_response_data",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 684,
          "line_end": 708,
          "args": [
            "self",
            "raw_content"
          ],
          "decorators": [],
          "docstring": "Extract partial response data from malformed JSON",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "OllamaEnsembleClient"
        },
        {
          "name": "__init__",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 714,
          "line_end": 715,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "RAGValidator"
        },
        {
          "name": "_initialize_knowledge_base",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 717,
          "line_end": 767,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize aviation knowledge base",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "RAGValidator"
        },
        {
          "name": "validate_response",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 769,
          "line_end": 842,
          "args": [
            "self",
            "response",
            "context"
          ],
          "decorators": [],
          "docstring": "Validate response against aviation knowledge base",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "RAGValidator"
        }
      ],
      "llm_interface\\filter_sort.py": [
        {
          "name": "get_llm_client",
          "file_path": "llm_interface\\filter_sort.py",
          "line_start": 13,
          "line_end": 17,
          "args": [],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [
            "LLMClient"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "get_llm_stats",
          "file_path": "llm_interface\\filter_sort.py",
          "line_start": 20,
          "line_end": 27,
          "args": [],
          "decorators": [],
          "docstring": "Get LLM timing statistics.",
          "imports_used": [],
          "intra_repo_calls": [
            "get_llm_client"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "select_best_solution",
          "file_path": "llm_interface\\filter_sort.py",
          "line_start": 30,
          "line_end": 118,
          "args": [
            "candidates",
            "policies"
          ],
          "decorators": [],
          "docstring": "Filter and select the best solution using LLM based on policies.",
          "imports_used": [],
          "intra_repo_calls": [
            "get_llm_client"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "llm_interface\\llm_client.py": [
        {
          "name": "__init__",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 31,
          "line_end": 58,
          "args": [
            "self",
            "model",
            "max_retries",
            "timeout",
            "enable_streaming",
            "enable_caching",
            "cache_size",
            "enable_optimized_prompts"
          ],
          "decorators": [],
          "docstring": null,
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "create_chat_messages",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 60,
          "line_end": 91,
          "args": [
            "self",
            "system_prompt",
            "user_prompt",
            "context"
          ],
          "decorators": [],
          "docstring": "\n        Create properly formatted Ollama chat messages.\n\n        Args:\n            system_prompt: System instructions\n            user_prompt: User query\n            context: Optional conversation context\n\n        Returns:\n            List of message dictionaries for Ollama\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ChatMessage"
          ],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "ask",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 93,
          "line_end": 181,
          "args": [
            "self",
            "prompt",
            "expect_json",
            "enable_function_calls",
            "system_prompt",
            "priority"
          ],
          "decorators": [],
          "docstring": "Ask the LLM a question with retry logic and error handling - OPTIMIZED VERSION.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "ask_optimized",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 183,
          "line_end": 264,
          "args": [
            "self",
            "user_prompt",
            "system_prompt",
            "expect_json",
            "context",
            "priority"
          ],
          "decorators": [],
          "docstring": "\n        High-performance LLM query with proper chat formatting.\n\n        Args:\n            user_prompt: User question/request\n            system_prompt: System instructions (ATC role, requirements)\n            expect_json: Whether to expect JSON response\n            context: Conversation context\n            priority: Request priority for timeout adjustment\n\n        Returns:\n            LLMResponse with content and performance metrics\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "ChatMessage",
            "LLMResponse"
          ],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_execute_chat_request",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 266,
          "line_end": 302,
          "args": [
            "self",
            "messages",
            "timeout",
            "expect_json"
          ],
          "decorators": [],
          "docstring": "Execute the actual chat request to Ollama",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_enhance_prompt_for_function_calling",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 304,
          "line_end": 327,
          "args": [
            "self",
            "original_prompt"
          ],
          "decorators": [],
          "docstring": "Enhance prompt to include function calling instructions",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_process_function_calls",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 329,
          "line_end": 362,
          "args": [
            "self",
            "content"
          ],
          "decorators": [],
          "docstring": "Process function calls from LLM response",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_execute_function_call",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 364,
          "line_end": 386,
          "args": [
            "self",
            "function_name",
            "arguments"
          ],
          "decorators": [],
          "docstring": "Execute a function call and return the result",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "chat_with_function_calling",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 388,
          "line_end": 448,
          "args": [
            "self",
            "messages",
            "max_function_calls"
          ],
          "decorators": [],
          "docstring": "\n        Extended chat interface with function calling support\n\n        Args:\n            messages: List of message dictionaries with 'role' and 'content'\n            max_function_calls: Maximum number of function calls allowed in a single chat\n\n        Returns:\n            Final response with function call history\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_format_conversation_for_prompt",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 450,
          "line_end": 465,
          "args": [
            "self",
            "messages"
          ],
          "decorators": [],
          "docstring": "Format conversation history into a single prompt",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "get_average_inference_time",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 467,
          "line_end": 471,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get average inference time per LLM call.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "get_total_inference_time",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 473,
          "line_end": 475,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get total inference time across all calls.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "get_inference_count",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 477,
          "line_end": 479,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get total number of LLM calls made.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "validate_response",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 481,
          "line_end": 491,
          "args": [
            "self",
            "response",
            "expected_keys"
          ],
          "decorators": [],
          "docstring": "Validate LLM response format and content.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_parse_json_response_fast",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 495,
          "line_end": 537,
          "args": [
            "self",
            "content"
          ],
          "decorators": [],
          "docstring": "Enhanced JSON parsing with robust fallback and validation",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_fix_common_json_issues",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 539,
          "line_end": 557,
          "args": [
            "self",
            "json_str"
          ],
          "decorators": [],
          "docstring": "Fix common JSON formatting issues",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_validate_atc_json_structure",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 559,
          "line_end": 576,
          "args": [
            "self",
            "parsed_json"
          ],
          "decorators": [],
          "docstring": "Validate that parsed JSON has expected ATC structure",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "get_safe_default_resolution",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 578,
          "line_end": 587,
          "args": [
            "self",
            "scenario_type"
          ],
          "decorators": [],
          "docstring": "Provide a safe default resolution when LLM fails",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_create_cache_key",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 589,
          "line_end": 594,
          "args": [
            "self",
            "user_prompt",
            "system_prompt"
          ],
          "decorators": [],
          "docstring": "Create cache key from prompts",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_cache_response",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 596,
          "line_end": 606,
          "args": [
            "self",
            "cache_key",
            "response"
          ],
          "decorators": [],
          "docstring": "Cache response with size limit",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "_get_priority_timeout",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 608,
          "line_end": 615,
          "args": [
            "self",
            "priority"
          ],
          "decorators": [],
          "docstring": "Get timeout based on request priority",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "get_conflict_resolution_system_prompt",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 618,
          "line_end": 635,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Concise system prompt for conflict resolution",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "get_conflict_detection_system_prompt",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 637,
          "line_end": 652,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Concise system prompt for conflict detection",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "get_performance_stats",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 655,
          "line_end": 673,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Get client performance statistics",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "reset_stats",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 675,
          "line_end": 679,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Reset performance tracking",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "LLMClient"
        },
        {
          "name": "quick_conflict_resolution",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 683,
          "line_end": 717,
          "args": [
            "aircraft_1",
            "aircraft_2",
            "time_to_conflict",
            "client"
          ],
          "decorators": [],
          "docstring": "\n    Quick conflict resolution with minimal prompt overhead.\n\n    Args:\n        aircraft_1: First aircraft data\n        aircraft_2: Second aircraft data\n        time_to_conflict: Time to conflict in seconds\n        client: Optional client instance\n\n    Returns:\n        LLMResponse with resolution command\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "LLMResponse",
            "LLMClient"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "quick_conflict_detection",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 720,
          "line_end": 752,
          "args": [
            "aircraft_states",
            "client"
          ],
          "decorators": [],
          "docstring": "\n    Quick conflict detection with minimal overhead.\n\n    Args:\n        aircraft_states: List of aircraft data\n        client: Optional client instance\n\n    Returns:\n        LLMResponse with detection results (JSON)\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "LLMResponse",
            "LLMClient"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "analysis\\enhanced_hallucination_detection.py": [
        {
          "name": "__init__",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 42,
          "line_end": 51,
          "args": [
            "self",
            "prompt_engine"
          ],
          "decorators": [],
          "docstring": "\n        Initialize the enhanced hallucination detector.\n\n        Args:\n            prompt_engine: Optional LLMPromptEngine instance for sophisticated prompts\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "_init_detection_patterns",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 53,
          "line_end": 88,
          "args": [
            "self"
          ],
          "decorators": [],
          "docstring": "Initialize detection patterns for various hallucination types",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "detect_hallucinations",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 90,
          "line_end": 186,
          "args": [
            "self",
            "llm_response",
            "baseline_response",
            "context"
          ],
          "decorators": [],
          "docstring": "\n        Detect hallucinations in LLM response using multiple strategies\n\n        Args:\n            llm_response: Response from LLM model\n            baseline_response: Baseline/ground truth response\n            context: Context information including scenario data\n\n        Returns:\n            HallucinationResult with detection details\n        ",
          "imports_used": [],
          "intra_repo_calls": [
            "HallucinationResult",
            "HallucinationType"
          ],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "_check_aircraft_existence",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 188,
          "line_end": 211,
          "args": [
            "self",
            "response_text",
            "context"
          ],
          "decorators": [],
          "docstring": "Check if response references non-existent aircraft",
          "imports_used": [],
          "intra_repo_calls": [
            "aircraft_list"
          ],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "_check_altitude_validity",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 213,
          "line_end": 233,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Check for invalid altitude values",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "_check_heading_validity",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 235,
          "line_end": 251,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Check for invalid heading values",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "_check_protocol_violations",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 253,
          "line_end": 274,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Check for ATC protocol violations",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "_check_impossible_maneuvers",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 276,
          "line_end": 307,
          "args": [
            "self",
            "response_text",
            "context"
          ],
          "decorators": [],
          "docstring": "Check for physically impossible maneuvers",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "_check_nonsensical_response",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 309,
          "line_end": 326,
          "args": [
            "self",
            "response_text"
          ],
          "decorators": [],
          "docstring": "Check for nonsensical or gibberish responses",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "_determine_severity",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 328,
          "line_end": 350,
          "args": [
            "self",
            "detected_types"
          ],
          "decorators": [],
          "docstring": "Determine severity based on detected hallucination types",
          "imports_used": [],
          "intra_repo_calls": [
            "HallucinationType"
          ],
          "is_method": true,
          "class_name": "EnhancedHallucinationDetector"
        },
        {
          "name": "create_enhanced_detector",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 353,
          "line_end": 355,
          "args": [],
          "decorators": [],
          "docstring": "Factory function to create enhanced hallucination detector",
          "imports_used": [],
          "intra_repo_calls": [
            "EnhancedHallucinationDetector"
          ],
          "is_method": false,
          "class_name": null
        }
      ],
      "analysis\\visualisation.py": [
        {
          "name": "__init__",
          "file_path": "analysis\\visualisation.py",
          "line_start": 97,
          "line_end": 136,
          "args": [
            "self",
            "output_dir",
            "style",
            "dpi"
          ],
          "decorators": [],
          "docstring": "\n        Initialize the visualizer.\n\n        Args:\n            output_dir: Directory to save visualizations\n            style: Matplotlib style to use\n            dpi: Resolution for saved figures\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "generate_comprehensive_report",
          "file_path": "analysis\\visualisation.py",
          "line_start": 138,
          "line_end": 179,
          "args": [
            "self",
            "data",
            "title"
          ],
          "decorators": [],
          "docstring": "\n        Generate a complete visualization report with all chart types.\n\n        Args:\n            data: DataFrame with Monte Carlo results\n            title: Report title\n\n        Returns:\n            Path to generated HTML report\n        ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_generate_distribution_analysis",
          "file_path": "analysis\\visualisation.py",
          "line_start": 181,
          "line_end": 195,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Generate distribution analysis visualizations.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_generate_trend_analysis",
          "file_path": "analysis\\visualisation.py",
          "line_start": 197,
          "line_end": 210,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Generate trend and cumulative analysis visualizations.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_generate_sensitivity_analysis",
          "file_path": "analysis\\visualisation.py",
          "line_start": 212,
          "line_end": 224,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Generate sensitivity and uncertainty visualizations.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_plot_metric_distributions",
          "file_path": "analysis\\visualisation.py",
          "line_start": 226,
          "line_end": 333,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Create histograms and KDEs for key metrics.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_plot_shift_comparisons",
          "file_path": "analysis\\visualisation.py",
          "line_start": 335,
          "line_end": 408,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Create side-by-side density comparisons for distribution shifts.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_plot_violin_comparisons",
          "file_path": "analysis\\visualisation.py",
          "line_start": 410,
          "line_end": 494,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Create violin plots for separation margins by categories.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_plot_ridge_plots",
          "file_path": "analysis\\visualisation.py",
          "line_start": 496,
          "line_end": 593,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Create ridge plots (joy plots) for metric distributions.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_plot_cumulative_error_curves",
          "file_path": "analysis\\visualisation.py",
          "line_start": 595,
          "line_end": 668,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Create cumulative false-positive/negative curves (ECDFs).",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_plot_time_series_analysis",
          "file_path": "analysis\\visualisation.py",
          "line_start": 670,
          "line_end": 758,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Create time-series analysis of conflict events.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_plot_performance_evolution",
          "file_path": "analysis\\visualisation.py",
          "line_start": 760,
          "line_end": 819,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Plot performance metrics evolution.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "_plot_tornado_sensitivity",
          "file_path": "analysis\\visualisation.py",
          "line_start": 821,
          "line_end": 924,
          "args": [
            "self",
            "data"
          ],
          "decorators": [],
          "docstring": "Create tornado chart for sensitivity analysis.",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": true,
          "class_name": "MonteCarloVisualizer"
        },
        {
          "name": "plot_cd_timeline",
          "file_path": "analysis\\visualisation.py",
          "line_start": 927,
          "line_end": 962,
          "args": [
            "df",
            "sim_id",
            "output_dir"
          ],
          "decorators": [],
          "docstring": "\n    Plot conflict detection timeline for a specific simulation.\n\n    Args:\n        df: DataFrame with simulation results\n        sim_id: Simulation ID to plot\n        output_dir: Output directory for plots\n        **kwargs: Additional arguments\n\n    Returns:\n        Path to generated plot file, or None if visualization failed\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "MonteCarloVisualizer"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "plot_cr_flowchart",
          "file_path": "analysis\\visualisation.py",
          "line_start": 965,
          "line_end": 986,
          "args": [
            "sim_id",
            "tier",
            "output_dir"
          ],
          "decorators": [],
          "docstring": "\n    Plot conflict resolution flowchart.\n\n    Args:\n        sim_id: Simulation ID\n        tier: Distribution shift tier\n        output_dir: Output directory for plots\n        **kwargs: Additional arguments\n\n    Returns:\n        Path to generated flowchart file, or None if visualization failed\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "MonteCarloVisualizer"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "plot_tier_comparison",
          "file_path": "analysis\\visualisation.py",
          "line_start": 989,
          "line_end": 1014,
          "args": [
            "df",
            "output_dir"
          ],
          "decorators": [],
          "docstring": "\n    Plot tier comparison analysis.\n\n    Args:\n        df: DataFrame with results across tiers\n        output_dir: Output directory for plots\n        **kwargs: Additional arguments\n\n    Returns:\n        Path to generated comparison plot, or None if visualization failed\n    ",
          "imports_used": [],
          "intra_repo_calls": [
            "MonteCarloVisualizer"
          ],
          "is_method": false,
          "class_name": null
        },
        {
          "name": "create_visualization_summary",
          "file_path": "analysis\\visualisation.py",
          "line_start": 1017,
          "line_end": 1087,
          "args": [
            "output_dir"
          ],
          "decorators": [],
          "docstring": "\n    Create a comprehensive visualization summary.\n\n    Args:\n        output_dir: Output directory for summary\n        **kwargs: Additional arguments\n\n    Returns:\n        Path to generated summary file, or None if creation failed\n    ",
          "imports_used": [],
          "intra_repo_calls": [],
          "is_method": false,
          "class_name": null
        }
      ]
    },
    "imports": {
      "llm_atc\\agents\\executor.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 6,
          "file_path": "llm_atc\\agents\\executor.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_atc\\agents\\executor.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\agents\\executor.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\agents\\executor.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Callable",
            "Optional"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "llm_atc\\agents\\executor.py",
          "is_from_import": true
        },
        {
          "module": "planner",
          "names": [
            "ActionPlan"
          ],
          "alias": null,
          "line_number": 12,
          "file_path": "llm_atc\\agents\\executor.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\agents\\planner.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 6,
          "file_path": "llm_atc\\agents\\planner.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_atc\\agents\\planner.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\agents\\planner.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\agents\\planner.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "llm_atc\\agents\\planner.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\agents\\scratchpad.py": [
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 6,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "asdict",
            "dataclass"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional",
            "Union"
          ],
          "alias": null,
          "line_number": 11,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": true
        },
        {
          "module": "executor",
          "names": [
            "ExecutionResult"
          ],
          "alias": null,
          "line_number": 13,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": true
        },
        {
          "module": "planner",
          "names": [
            "ActionPlan",
            "ConflictAssessment"
          ],
          "alias": null,
          "line_number": 14,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": true
        },
        {
          "module": "verifier",
          "names": [
            "VerificationResult"
          ],
          "alias": null,
          "line_number": 15,
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\agents\\verifier.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 6,
          "file_path": "llm_atc\\agents\\verifier.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_atc\\agents\\verifier.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\agents\\verifier.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\agents\\verifier.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "llm_atc\\agents\\verifier.py",
          "is_from_import": true
        },
        {
          "module": "executor",
          "names": [
            "ExecutionResult",
            "ExecutionStatus"
          ],
          "alias": null,
          "line_number": 12,
          "file_path": "llm_atc\\agents\\verifier.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\memory\\experience_integrator.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "is_from_import": false
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "is_from_import": true
        },
        {
          "module": "analysis.enhanced_hallucination_detection",
          "names": [
            "EnhancedHallucinationDetector"
          ],
          "alias": null,
          "line_number": 11,
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "is_from_import": true
        },
        {
          "module": "llm_atc.memory.replay_store",
          "names": [
            "ConflictExperience",
            "SimilarityResult",
            "VectorReplayStore"
          ],
          "alias": null,
          "line_number": 12,
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "is_from_import": true
        },
        {
          "module": "llm_atc.metrics.safety_margin_quantifier",
          "names": [
            "SafetyMarginQuantifier"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "is_from_import": true
        },
        {
          "module": "llm_atc.memory.replay_store",
          "names": [
            "ConflictExperience"
          ],
          "alias": null,
          "line_number": 488,
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\memory\\replay_store.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": false
        },
        {
          "module": "os",
          "names": [
            "os"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 11,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 12,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": true
        },
        {
          "module": "chromadb",
          "names": [
            "chromadb"
          ],
          "alias": null,
          "line_number": 14,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": false
        },
        {
          "module": "numpy",
          "names": [
            "numpy"
          ],
          "alias": "np",
          "line_number": 15,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": false
        },
        {
          "module": "chromadb.config",
          "names": [
            "Settings"
          ],
          "alias": null,
          "line_number": 16,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": true
        },
        {
          "module": "sentence_transformers",
          "names": [
            "SentenceTransformer"
          ],
          "alias": null,
          "line_number": 20,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": true
        },
        {
          "module": "uuid",
          "names": [
            "uuid"
          ],
          "alias": null,
          "line_number": 181,
          "file_path": "llm_atc\\memory\\replay_store.py",
          "is_from_import": false
        }
      ],
      "llm_atc\\metrics\\monte_carlo_analysis.py": [
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 15,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 16,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": false
        },
        {
          "module": "pathlib",
          "names": [
            "Path"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional",
            "Union"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": true
        },
        {
          "module": "numpy",
          "names": [
            "numpy"
          ],
          "alias": "np",
          "line_number": 20,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": false
        },
        {
          "module": "pandas",
          "names": [
            "pandas"
          ],
          "alias": "pd",
          "line_number": 21,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": false
        },
        {
          "module": "matplotlib.pyplot",
          "names": [
            "matplotlib.pyplot"
          ],
          "alias": "plt",
          "line_number": 25,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": false
        },
        {
          "module": "safety_margin_quantifier",
          "names": [
            "calc_efficiency_penalty",
            "calc_separation_margin"
          ],
          "alias": null,
          "line_number": 33,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": true
        },
        {
          "module": "tempfile",
          "names": [
            "tempfile"
          ],
          "alias": null,
          "line_number": 1360,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": false
        },
        {
          "module": "safety_margin_quantifier",
          "names": [
            "calc_efficiency_penalty",
            "calc_separation_margin"
          ],
          "alias": null,
          "line_number": 40,
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\metrics\\safety_margin_quantifier.py": [
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": false
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 11,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": true
        },
        {
          "module": "pathlib",
          "names": [
            "Path"
          ],
          "alias": null,
          "line_number": 12,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 13,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": true
        },
        {
          "module": "numpy",
          "names": [
            "numpy"
          ],
          "alias": "np",
          "line_number": 15,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 720,
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "is_from_import": false
        }
      ],
      "llm_atc\\metrics\\__init__.py": [
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 2,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 3,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 4,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        },
        {
          "module": "os",
          "names": [
            "os"
          ],
          "alias": null,
          "line_number": 5,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        },
        {
          "module": "sys",
          "names": [
            "sys"
          ],
          "alias": null,
          "line_number": 6,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        },
        {
          "module": "collections",
          "names": [
            "defaultdict"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": true
        },
        {
          "module": "pathlib",
          "names": [
            "Path"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": true
        },
        {
          "module": "numpy",
          "names": [
            "numpy"
          ],
          "alias": "np",
          "line_number": 11,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        },
        {
          "module": "pandas",
          "names": [
            "pandas"
          ],
          "alias": "pd",
          "line_number": 12,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        },
        {
          "module": "matplotlib.pyplot",
          "names": [
            "matplotlib.pyplot"
          ],
          "alias": "plt",
          "line_number": 16,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        },
        {
          "module": "seaborn",
          "names": [
            "seaborn"
          ],
          "alias": "sns",
          "line_number": 17,
          "file_path": "llm_atc\\metrics\\__init__.py",
          "is_from_import": false
        }
      ],
      "llm_atc\\tools\\bluesky_tools.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 6,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "os",
          "names": [
            "os"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "socket",
          "names": [
            "socket"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": true
        },
        {
          "module": "yaml",
          "names": [
            "yaml"
          ],
          "alias": null,
          "line_number": 13,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "bluesky",
          "names": [
            "bluesky"
          ],
          "alias": "bs",
          "line_number": 22,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "bluesky",
          "names": [
            "sim",
            "stack",
            "traf"
          ],
          "alias": null,
          "line_number": 23,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": true
        },
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 111,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 121,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": false
        },
        {
          "module": "bluesky",
          "names": [
            "sim",
            "traf"
          ],
          "alias": null,
          "line_number": 188,
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\tools\\enhanced_conflict_detector.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 16,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "is_from_import": false
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 19,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 20,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "List",
            "Optional",
            "Tuple"
          ],
          "alias": null,
          "line_number": 21,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "is_from_import": true
        },
        {
          "module": "bluesky",
          "names": [
            "bluesky"
          ],
          "alias": "bs",
          "line_number": 24,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "is_from_import": false
        },
        {
          "module": "bluesky",
          "names": [
            "sim",
            "stack",
            "traf"
          ],
          "alias": null,
          "line_number": 25,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\tools\\enhanced_conflict_detector_clean.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 16,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "is_from_import": false
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 19,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 20,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "List",
            "Optional",
            "Tuple"
          ],
          "alias": null,
          "line_number": 21,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "is_from_import": true
        },
        {
          "module": "bluesky",
          "names": [
            "bluesky"
          ],
          "alias": "bs",
          "line_number": 24,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "is_from_import": false
        },
        {
          "module": "bluesky",
          "names": [
            "sim",
            "stack",
            "traf"
          ],
          "alias": null,
          "line_number": 25,
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\tools\\llm_prompt_engine.py": [
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 15,
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 16,
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "is_from_import": false
        },
        {
          "module": "re",
          "names": [
            "re"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional",
            "List"
          ],
          "alias": null,
          "line_number": 19,
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "is_from_import": true
        },
        {
          "module": "llm_interface.llm_client",
          "names": [
            "LLMClient"
          ],
          "alias": null,
          "line_number": 21,
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "is_from_import": true
        }
      ],
      "llm_atc\\tools\\__init__.py": [
        {
          "module": "bluesky_tools",
          "names": [
            "TOOL_REGISTRY",
            "AircraftInfo",
            "BlueSkyToolsError",
            "ConflictInfo",
            "check_separation_violation",
            "continue_monitoring",
            "execute_tool",
            "get_airspace_info",
            "get_all_aircraft_info",
            "get_available_tools",
            "get_conflict_info",
            "get_distance",
            "get_minimum_separation",
            "get_weather_info",
            "reset_simulation",
            "search_experience_library",
            "send_command",
            "step_simulation"
          ],
          "alias": null,
          "line_number": 6,
          "file_path": "llm_atc\\tools\\__init__.py",
          "is_from_import": true
        },
        {
          "module": "llm_prompt_engine",
          "names": [
            "ConflictPromptData",
            "LLMPromptEngine",
            "ResolutionResponse"
          ],
          "alias": null,
          "line_number": 26,
          "file_path": "llm_atc\\tools\\__init__.py",
          "is_from_import": true
        }
      ],
      "BSKY_GYM_LLM\\merge_lora_and_convert.py": [
        {
          "module": "os",
          "names": [
            "os"
          ],
          "alias": null,
          "line_number": 11,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        },
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 12,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 13,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        },
        {
          "module": "torch",
          "names": [
            "torch"
          ],
          "alias": null,
          "line_number": 14,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        },
        {
          "module": "subprocess",
          "names": [
            "subprocess"
          ],
          "alias": null,
          "line_number": 15,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        },
        {
          "module": "shutil",
          "names": [
            "shutil"
          ],
          "alias": null,
          "line_number": 16,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        },
        {
          "module": "pathlib",
          "names": [
            "Path"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": true
        },
        {
          "module": "transformers",
          "names": [
            "AutoTokenizer",
            "AutoModelForCausalLM"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": true
        },
        {
          "module": "peft",
          "names": [
            "PeftModel"
          ],
          "alias": null,
          "line_number": 19,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": true
        },
        {
          "module": "tempfile",
          "names": [
            "tempfile"
          ],
          "alias": null,
          "line_number": 20,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        },
        {
          "module": "traceback",
          "names": [
            "traceback"
          ],
          "alias": null,
          "line_number": 124,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        },
        {
          "module": "traceback",
          "names": [
            "traceback"
          ],
          "alias": null,
          "line_number": 311,
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "is_from_import": false
        }
      ],
      "scenarios\\monte_carlo_framework.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 15,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": false
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 16,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": false
        },
        {
          "module": "random",
          "names": [
            "random"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 19,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 20,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 21,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": true
        },
        {
          "module": "yaml",
          "names": [
            "yaml"
          ],
          "alias": null,
          "line_number": 23,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": false
        },
        {
          "module": "bluesky",
          "names": [
            "sim",
            "stack",
            "traf"
          ],
          "alias": null,
          "line_number": 27,
          "file_path": "scenarios\\monte_carlo_framework.py",
          "is_from_import": true
        }
      ],
      "scenarios\\monte_carlo_runner.py": [
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 23,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 24,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 25,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "os",
          "names": [
            "os"
          ],
          "alias": null,
          "line_number": 26,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 27,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "traceback",
          "names": [
            "traceback"
          ],
          "alias": null,
          "line_number": 28,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "uuid",
          "names": [
            "uuid"
          ],
          "alias": null,
          "line_number": 29,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "csv",
          "names": [
            "csv"
          ],
          "alias": null,
          "line_number": 30,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "asdict",
            "dataclass"
          ],
          "alias": null,
          "line_number": 31,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "datetime",
          "names": [
            "datetime"
          ],
          "alias": null,
          "line_number": 32,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "pathlib",
          "names": [
            "Path"
          ],
          "alias": null,
          "line_number": 33,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional",
            "List",
            "Dict"
          ],
          "alias": null,
          "line_number": 34,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "matplotlib.pyplot",
          "names": [
            "matplotlib.pyplot"
          ],
          "alias": "plt",
          "line_number": 36,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "pandas",
          "names": [
            "pandas"
          ],
          "alias": "pd",
          "line_number": 37,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "llm_atc.tools",
          "names": [
            "bluesky_tools"
          ],
          "alias": null,
          "line_number": 39,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "llm_atc.tools.llm_prompt_engine",
          "names": [
            "LLMPromptEngine"
          ],
          "alias": null,
          "line_number": 40,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "scenarios.monte_carlo_framework",
          "names": [
            "ComplexityTier"
          ],
          "alias": null,
          "line_number": 41,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "scenarios.scenario_generator",
          "names": [
            "ScenarioGenerator",
            "ScenarioType",
            "generate_horizontal_scenario",
            "generate_sector_scenario",
            "generate_vertical_scenario"
          ],
          "alias": null,
          "line_number": 46,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "llm_atc.metrics.monte_carlo_analysis",
          "names": [
            "MonteCarloResultsAnalyzer"
          ],
          "alias": null,
          "line_number": 56,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "argparse",
          "names": [
            "argparse"
          ],
          "alias": null,
          "line_number": 2373,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": false
        },
        {
          "module": "llm_atc.tools.bluesky_tools",
          "names": [
            "set_strict_mode"
          ],
          "alias": null,
          "line_number": 267,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        },
        {
          "module": "llm_atc.tools.enhanced_conflict_detector",
          "names": [
            "EnhancedConflictDetector"
          ],
          "alias": null,
          "line_number": 915,
          "file_path": "scenarios\\monte_carlo_runner.py",
          "is_from_import": true
        }
      ],
      "scenarios\\scenario_generator.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": false
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 19,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": false
        },
        {
          "module": "random",
          "names": [
            "random"
          ],
          "alias": null,
          "line_number": 20,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 21,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "asdict",
            "dataclass"
          ],
          "alias": null,
          "line_number": 22,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 23,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 24,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": true
        },
        {
          "module": "scenarios.monte_carlo_framework",
          "names": [
            "BlueSkyScenarioGenerator",
            "ComplexityTier",
            "ScenarioConfiguration"
          ],
          "alias": null,
          "line_number": 27,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": true
        },
        {
          "module": "math",
          "names": [
            "math"
          ],
          "alias": null,
          "line_number": 761,
          "file_path": "scenarios\\scenario_generator.py",
          "is_from_import": false
        }
      ],
      "llm_interface\\ensemble.py": [
        {
          "module": "contextlib",
          "names": [
            "contextlib"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "concurrent.futures",
          "names": [
            "ThreadPoolExecutor",
            "as_completed"
          ],
          "alias": null,
          "line_number": 11,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": true
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 12,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 13,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any"
          ],
          "alias": null,
          "line_number": 14,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": true
        },
        {
          "module": "numpy",
          "names": [
            "numpy"
          ],
          "alias": "np",
          "line_number": 16,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "ollama",
          "names": [
            "ollama"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "re",
          "names": [
            "re"
          ],
          "alias": null,
          "line_number": 649,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "re",
          "names": [
            "re"
          ],
          "alias": null,
          "line_number": 686,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "requests",
          "names": [
            "requests"
          ],
          "alias": null,
          "line_number": 212,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        },
        {
          "module": "re",
          "names": [
            "re"
          ],
          "alias": null,
          "line_number": 400,
          "file_path": "llm_interface\\ensemble.py",
          "is_from_import": false
        }
      ],
      "llm_interface\\filter_sort.py": [
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 2,
          "file_path": "llm_interface\\filter_sort.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 3,
          "file_path": "llm_interface\\filter_sort.py",
          "is_from_import": false
        },
        {
          "module": "llm_client",
          "names": [
            "LLMClient"
          ],
          "alias": null,
          "line_number": 5,
          "file_path": "llm_interface\\filter_sort.py",
          "is_from_import": true
        }
      ],
      "llm_interface\\llm_client.py": [
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 1,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": false
        },
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 2,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": false
        },
        {
          "module": "time",
          "names": [
            "time"
          ],
          "alias": null,
          "line_number": 3,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": false
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional",
            "Dict",
            "List"
          ],
          "alias": null,
          "line_number": 4,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": true
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 5,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": true
        },
        {
          "module": "functools",
          "names": [
            "lru_cache"
          ],
          "alias": null,
          "line_number": 6,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": true
        },
        {
          "module": "ollama",
          "names": [
            "ollama"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": false
        },
        {
          "module": "re",
          "names": [
            "re"
          ],
          "alias": null,
          "line_number": 497,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": false
        },
        {
          "module": "re",
          "names": [
            "re"
          ],
          "alias": null,
          "line_number": 541,
          "file_path": "llm_interface\\llm_client.py",
          "is_from_import": false
        }
      ],
      "analysis\\enhanced_hallucination_detection.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 7,
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "is_from_import": false
        },
        {
          "module": "re",
          "names": [
            "re"
          ],
          "alias": null,
          "line_number": 8,
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "dataclass"
          ],
          "alias": null,
          "line_number": 9,
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "is_from_import": true
        },
        {
          "module": "enum",
          "names": [
            "Enum"
          ],
          "alias": null,
          "line_number": 10,
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Optional"
          ],
          "alias": null,
          "line_number": 11,
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "is_from_import": true
        }
      ],
      "analysis\\visualisation.py": [
        {
          "module": "logging",
          "names": [
            "logging"
          ],
          "alias": null,
          "line_number": 17,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "warnings",
          "names": [
            "warnings"
          ],
          "alias": null,
          "line_number": 18,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "pathlib",
          "names": [
            "Path"
          ],
          "alias": null,
          "line_number": 19,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "typing",
          "names": [
            "Any",
            "Dict",
            "List",
            "Optional",
            "Tuple",
            "Union"
          ],
          "alias": null,
          "line_number": 20,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "json",
          "names": [
            "json"
          ],
          "alias": null,
          "line_number": 21,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "dataclasses",
          "names": [
            "asdict"
          ],
          "alias": null,
          "line_number": 22,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "collections",
          "names": [
            "defaultdict"
          ],
          "alias": null,
          "line_number": 23,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "numpy",
          "names": [
            "numpy"
          ],
          "alias": "np",
          "line_number": 25,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "pandas",
          "names": [
            "pandas"
          ],
          "alias": "pd",
          "line_number": 26,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "matplotlib.pyplot",
          "names": [
            "matplotlib.pyplot"
          ],
          "alias": "plt",
          "line_number": 30,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "matplotlib.patches",
          "names": [
            "matplotlib.patches"
          ],
          "alias": "patches",
          "line_number": 31,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "matplotlib.animation",
          "names": [
            "FuncAnimation"
          ],
          "alias": null,
          "line_number": 32,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "seaborn",
          "names": [
            "seaborn"
          ],
          "alias": "sns",
          "line_number": 33,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "plotly.graph_objects",
          "names": [
            "plotly.graph_objects"
          ],
          "alias": "go",
          "line_number": 46,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "plotly.express",
          "names": [
            "plotly.express"
          ],
          "alias": "px",
          "line_number": 47,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "plotly.subplots",
          "names": [
            "make_subplots"
          ],
          "alias": null,
          "line_number": 48,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "plotly.figure_factory",
          "names": [
            "plotly.figure_factory"
          ],
          "alias": "ff",
          "line_number": 49,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "folium",
          "names": [
            "folium"
          ],
          "alias": null,
          "line_number": 59,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "geopandas",
          "names": [
            "geopandas"
          ],
          "alias": "gpd",
          "line_number": 60,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": false
        },
        {
          "module": "shapely.geometry",
          "names": [
            "Point",
            "LineString"
          ],
          "alias": null,
          "line_number": 61,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "scipy",
          "names": [
            "stats"
          ],
          "alias": null,
          "line_number": 70,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "scipy.spatial.distance",
          "names": [
            "pdist",
            "squareform"
          ],
          "alias": null,
          "line_number": 71,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        },
        {
          "module": "sklearn.preprocessing",
          "names": [
            "StandardScaler"
          ],
          "alias": null,
          "line_number": 72,
          "file_path": "analysis\\visualisation.py",
          "is_from_import": true
        }
      ]
    },
    "classes": {
      "llm_atc\\agents\\executor.py": [
        {
          "name": "ExecutionStatus",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 18,
          "line_end": 23,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "ExecutionResult",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 27,
          "line_end": 38,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "Executor",
          "file_path": "llm_atc\\agents\\executor.py",
          "line_start": 41,
          "line_end": 325,
          "methods": [
            "__init__",
            "send_plan",
            "_send_command",
            "_simulate_command_execution",
            "cancel_execution",
            "get_execution_status",
            "get_active_executions",
            "get_execution_history",
            "get_execution_metrics",
            "set_command_sender"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\agents\\planner.py": [
        {
          "name": "PlanType",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 31,
          "line_end": 37,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "ConflictAssessment",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 41,
          "line_end": 51,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "ActionPlan",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 55,
          "line_end": 67,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "Planner",
          "file_path": "llm_atc\\agents\\planner.py",
          "line_start": 70,
          "line_end": 404,
          "methods": [
            "__init__",
            "assess_conflict",
            "generate_action_plan",
            "_detect_proximity_conflicts",
            "_calculate_separation",
            "_assess_severity",
            "_estimate_time_to_conflict",
            "_prioritize_conflicts",
            "_generate_assessment",
            "_determine_recommended_action",
            "_generate_reasoning",
            "_generate_commands",
            "_calculate_expected_outcome",
            "_calculate_priority",
            "get_assessment_history",
            "get_plan_history"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\agents\\scratchpad.py": [
        {
          "name": "StepType",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 18,
          "line_end": 25,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "ReasoningStep",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 29,
          "line_end": 40,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "SessionSummary",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 44,
          "line_end": 57,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "Scratchpad",
          "file_path": "llm_atc\\agents\\scratchpad.py",
          "line_start": 60,
          "line_end": 473,
          "methods": [
            "__init__",
            "log_step",
            "log_assessment_step",
            "log_planning_step",
            "log_execution_step",
            "log_verification_step",
            "log_error_step",
            "log_monitoring_step",
            "get_history",
            "get_step_by_id",
            "get_steps_by_type",
            "get_recent_steps",
            "complete_session",
            "start_new_session",
            "_generate_session_summary",
            "_calculate_average_confidence",
            "_extract_key_decisions",
            "_extract_lessons_learned",
            "export_session_data",
            "set_session_metadata",
            "get_session_metrics"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\agents\\verifier.py": [
        {
          "name": "VerificationStatus",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 15,
          "line_end": 19,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "VerificationResult",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 23,
          "line_end": 36,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "Verifier",
          "file_path": "llm_atc\\agents\\verifier.py",
          "line_start": 39,
          "line_end": 375,
          "methods": [
            "__init__",
            "check",
            "_check_execution_status",
            "_check_execution_timing",
            "_check_command_success_rate",
            "_check_safety_compliance",
            "_check_response_validity",
            "_is_unsafe_command",
            "_is_valid_response",
            "_calculate_safety_score",
            "_calculate_confidence",
            "get_verification_history",
            "get_verification_metrics",
            "update_safety_thresholds"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\memory\\experience_integrator.py": [
        {
          "name": "ExperienceIntegrator",
          "file_path": "llm_atc\\memory\\experience_integrator.py",
          "line_start": 20,
          "line_end": 518,
          "methods": [
            "__init__",
            "process_conflict_resolution",
            "_find_relevant_experiences",
            "_extract_lessons",
            "_check_hallucination_patterns",
            "_enhance_decision_with_experience",
            "record_resolution_outcome",
            "get_experience_summary",
            "_generate_learning_insights",
            "store_experience"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\memory\\replay_store.py": [
        {
          "name": "ConflictExperience",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 30,
          "line_end": 78,
          "methods": [
            "__post_init__"
          ],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "SimilarityResult",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 82,
          "line_end": 87,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "RetrievedExperience",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 91,
          "line_end": 97,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "VectorReplayStore",
          "file_path": "llm_atc\\memory\\replay_store.py",
          "line_start": 100,
          "line_end": 487,
          "methods": [
            "__init__",
            "store_experience",
            "retrieve_experience",
            "get_all_experiences",
            "get_stats",
            "delete_experience",
            "clear_all"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\metrics\\monte_carlo_analysis.py": [
        {
          "name": "MonteCarloResultsAnalyzer",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 53,
          "line_end": 992,
          "methods": [
            "__init__",
            "read_results_file",
            "_read_json_results",
            "compute_false_positive_negative_rates",
            "_conflicts_to_set",
            "compute_success_rates_by_scenario",
            "compute_success_rates_by_group",
            "compute_average_separation_margins",
            "compute_efficiency_penalties",
            "generate_report",
            "_generate_executive_summary",
            "_assess_detection_performance",
            "_assess_safety_margins",
            "_assess_efficiency_performance",
            "_format_grouped_success_table",
            "_format_distribution_shift_analysis",
            "_generate_recommendations",
            "aggregate_monte_carlo_metrics",
            "_analyze_distribution_shift_performance",
            "_create_empty_aggregated_metrics"
          ],
          "base_classes": [],
          "decorators": []
        },
        {
          "name": "MonteCarloVisualizer",
          "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
          "line_start": 995,
          "line_end": 1300,
          "methods": [
            "__init__",
            "create_performance_summary_charts",
            "create_distribution_shift_plots",
            "_create_success_rate_chart",
            "_create_detection_performance_chart",
            "_create_safety_margins_chart",
            "_create_shift_performance_scatter"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\metrics\\safety_margin_quantifier.py": [
        {
          "name": "SeparationStandard",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 23,
          "line_end": 28,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "SafetyMargin",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 32,
          "line_end": 41,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "ConflictGeometry",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 45,
          "line_end": 56,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "SafetyMarginQuantifier",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 59,
          "line_end": 417,
          "methods": [
            "__init__",
            "calculate_safety_margins",
            "_apply_resolution_maneuver",
            "_predict_position",
            "_calculate_closest_approach",
            "_calculate_horizontal_margin",
            "_calculate_vertical_margin",
            "_calculate_temporal_margin",
            "_calculate_effective_margin",
            "_calculate_total_uncertainty",
            "_calculate_baseline_margin",
            "_determine_safety_level",
            "_create_default_safety_margin"
          ],
          "base_classes": [],
          "decorators": []
        },
        {
          "name": "SafetyMetricsAggregator",
          "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
          "line_start": 420,
          "line_end": 564,
          "methods": [
            "__init__",
            "add_conflict_resolution",
            "generate_safety_summary",
            "export_detailed_metrics"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\tools\\bluesky_tools.py": [
        {
          "name": "BlueSkyConfig",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 38,
          "line_end": 156,
          "methods": [
            "__init__",
            "_find_config_file",
            "_create_default_config",
            "_load_config",
            "_get_default_config",
            "get"
          ],
          "base_classes": [],
          "decorators": []
        },
        {
          "name": "BlueSkyInterface",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 163,
          "line_end": 745,
          "methods": [
            "__init__",
            "_initialize_bluesky",
            "_setup_simulation",
            "_test_network_connection",
            "is_available",
            "get_aircraft_data",
            "get_conflict_data",
            "_calculate_horizontal_separation",
            "_assess_conflict_severity",
            "send_bluesky_command",
            "step_simulation_real",
            "reset_simulation_real",
            "_get_mock_aircraft_data",
            "_get_mock_conflict_data",
            "_simulate_command_execution",
            "_simulate_step",
            "_simulate_reset"
          ],
          "base_classes": [],
          "decorators": []
        },
        {
          "name": "AircraftInfo",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 760,
          "line_end": 771,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "ConflictInfo",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 775,
          "line_end": 784,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "BlueSkyToolsError",
          "file_path": "llm_atc\\tools\\bluesky_tools.py",
          "line_start": 787,
          "line_end": 788,
          "methods": [],
          "base_classes": [
            "Exception"
          ],
          "decorators": []
        }
      ],
      "llm_atc\\tools\\enhanced_conflict_detector.py": [
        {
          "name": "ConflictDetectionMethod",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 33,
          "line_end": 38,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "ConflictData",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 42,
          "line_end": 57,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "EnhancedConflictDetector",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
          "line_start": 60,
          "line_end": 586,
          "methods": [
            "__init__",
            "detect_conflicts_comprehensive",
            "_detect_with_swarm",
            "_detect_with_statebased",
            "_detect_with_enhanced_analysis",
            "_analyze_aircraft_pair",
            "_calculate_cpa",
            "_calculate_horizontal_distance",
            "_assess_conflict_severity",
            "_calculate_confidence",
            "_cross_validate_conflicts",
            "_merge_conflict_detections",
            "_get_aircraft_pair_key",
            "validate_llm_conflicts",
            "_mock_conflict_detection"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\tools\\enhanced_conflict_detector_clean.py": [
        {
          "name": "ConflictDetectionMethod",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 33,
          "line_end": 38,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "ConflictData",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 42,
          "line_end": 57,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "EnhancedConflictDetector",
          "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
          "line_start": 60,
          "line_end": 586,
          "methods": [
            "__init__",
            "detect_conflicts_comprehensive",
            "_detect_with_swarm",
            "_detect_with_statebased",
            "_detect_with_enhanced_analysis",
            "_analyze_aircraft_pair",
            "_calculate_cpa",
            "_calculate_horizontal_distance",
            "_assess_conflict_severity",
            "_calculate_confidence",
            "_cross_validate_conflicts",
            "_merge_conflict_detections",
            "_get_aircraft_pair_key",
            "validate_llm_conflicts",
            "_mock_conflict_detection"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_atc\\tools\\llm_prompt_engine.py": [
        {
          "name": "ConflictPromptData",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 25,
          "line_end": 35,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "ResolutionResponse",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 39,
          "line_end": 47,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "LLMPromptEngine",
          "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
          "line_start": 50,
          "line_end": 1945,
          "methods": [
            "__init__",
            "_init_prompt_templates",
            "format_conflict_prompt",
            "format_detector_prompt",
            "parse_resolution_response",
            "parse_detector_response",
            "_is_distilled_model_response",
            "_parse_distilled_model_response",
            "_parse_detector_response_legacy",
            "_validate_detector_response",
            "_validate_aircraft_pairs",
            "_validate_confidence",
            "_validate_priority",
            "_validate_sector_response",
            "_validate_calculation_details",
            "_extract_json_from_response",
            "get_conflict_resolution",
            "get_conflict_resolution_with_prompts",
            "detect_conflict_via_llm",
            "detect_conflict_via_llm_with_prompts",
            "assess_resolution_safety",
            "_get_fallback_conflict_prompt",
            "_parse_function_call_response",
            "_extract_bluesky_command",
            "_normalize_bluesky_command",
            "_extract_aircraft_id",
            "_determine_maneuver_type",
            "_parse_aircraft_pairs",
            "_parse_time_values",
            "_parse_safety_response",
            "format_conflict_resolution_prompt_optimized",
            "format_conflict_detection_prompt_optimized",
            "get_conflict_resolution_optimized",
            "get_conflict_detection_optimized",
            "_parse_resolution_response_fast",
            "_parse_detection_response_fast",
            "_extract_aircraft_id_fast",
            "_determine_maneuver_type_fast",
            "get_performance_stats",
            "reset_performance_stats"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "BSKY_GYM_LLM\\merge_lora_and_convert.py": [
        {
          "name": "LoRAMerger",
          "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
          "line_start": 29,
          "line_end": 508,
          "methods": [
            "__init__",
            "check_prerequisites",
            "merge_lora_adapter",
            "_save_model_metadata",
            "convert_to_gguf",
            "create_ollama_model",
            "_create_enhanced_modelfile",
            "_verify_ollama_model",
            "run_complete_pipeline"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "scenarios\\monte_carlo_framework.py": [
        {
          "name": "ComplexityTier",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 35,
          "line_end": 41,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "ScenarioConfiguration",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 45,
          "line_end": 115,
          "methods": [
            "aircraft_list",
            "environmental"
          ],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "BlueSkyScenarioGenerator",
          "file_path": "scenarios\\monte_carlo_framework.py",
          "line_start": 118,
          "line_end": 947,
          "methods": [
            "__init__",
            "_load_ranges",
            "_load_distribution_shift_config",
            "_get_default_ranges",
            "sample_from_range",
            "weighted_choice",
            "apply_distribution_shift",
            "generate_scenario",
            "_generate_environmental_conditions",
            "_generate_bluesky_commands",
            "_generate_conflict_commands",
            "_calculate_bearing",
            "execute_scenario",
            "_mock_execution",
            "generate_scenario_batch",
            "get_command_log",
            "validate_ranges"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "scenarios\\monte_carlo_runner.py": [
        {
          "name": "DetectionComparison",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 67,
          "line_end": 101,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "BenchmarkConfiguration",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 105,
          "line_end": 168,
          "methods": [
            "__post_init__"
          ],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "ScenarioResult",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 172,
          "line_end": 244,
          "methods": [
            "__post_init__"
          ],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "MonteCarloBenchmark",
          "file_path": "scenarios\\monte_carlo_runner.py",
          "line_start": 247,
          "line_end": 2333,
          "methods": [
            "__init__",
            "_setup_output_directory",
            "_setup_logging",
            "_setup_enhanced_logging",
            "_init_csv_file",
            "run",
            "_calculate_total_scenarios",
            "_run_scenario_batch",
            "_generate_scenario",
            "_get_aircraft_count_for_complexity",
            "_run_single_scenario",
            "_execute_scenario_pipeline",
            "_reset_bluesky_simulation",
            "_load_scenario_commands",
            "_extract_ground_truth_conflicts",
            "_detect_conflicts",
            "_basic_conflict_detection_fallback",
            "_get_aircraft_states_for_llm",
            "_validate_llm_conflicts_with_bluesky",
            "_resolve_conflicts",
            "_is_valid_bluesky_command",
            "_format_conflict_for_llm",
            "_verify_resolutions",
            "_calculate_all_separations",
            "_calculate_scenario_metrics",
            "_create_error_result",
            "_generate_summary",
            "_get_serializable_config",
            "_generate_summary_by_group",
            "_generate_combined_summary",
            "_print_detailed_analysis",
            "_generate_visualizations",
            "_plot_detection_performance",
            "_plot_safety_margins",
            "_plot_efficiency_metrics",
            "_plot_performance_by_type",
            "_plot_distribution_shift_impact",
            "_save_results",
            "_run_enhanced_scenario",
            "_create_detection_comparison",
            "_write_csv_row",
            "_save_detection_analysis"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "scenarios\\scenario_generator.py": [
        {
          "name": "ScenarioType",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 34,
          "line_end": 39,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "GroundTruthConflict",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 43,
          "line_end": 51,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "Scenario",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 55,
          "line_end": 97,
          "methods": [
            "__post_init__",
            "to_dict"
          ],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "ScenarioGenerator",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 100,
          "line_end": 1181,
          "methods": [
            "__init__",
            "generate_scenario",
            "generate_horizontal_scenario",
            "generate_vertical_scenario",
            "generate_sector_scenario",
            "_create_horizontal_conflicts",
            "_avoid_horizontal_conflicts",
            "_create_vertical_conflicts",
            "_avoid_vertical_conflicts",
            "_create_vertical_conflicts_enhanced",
            "_avoid_vertical_conflicts_enhanced",
            "_optimize_conflict_timing",
            "_add_environmental_commands",
            "_calculate_horizontal_ground_truth",
            "_analyze_aircraft_pair_trajectory",
            "_calculate_vertical_ground_truth",
            "_calculate_sector_ground_truth",
            "_analyze_trajectory_conflict",
            "_determine_conflict_severity",
            "_calculate_bearing",
            "_calculate_distance_nm",
            "_are_headings_convergent",
            "_project_position"
          ],
          "base_classes": [],
          "decorators": []
        },
        {
          "name": "HorizontalCREnv",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1185,
          "line_end": 1199,
          "methods": [
            "__init__",
            "generate_scenario"
          ],
          "base_classes": [],
          "decorators": []
        },
        {
          "name": "VerticalCREnv",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1202,
          "line_end": 1216,
          "methods": [
            "__init__",
            "generate_scenario"
          ],
          "base_classes": [],
          "decorators": []
        },
        {
          "name": "SectorCREnv",
          "file_path": "scenarios\\scenario_generator.py",
          "line_start": 1219,
          "line_end": 1238,
          "methods": [
            "__init__",
            "generate_scenario"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_interface\\ensemble.py": [
        {
          "name": "ModelRole",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 20,
          "line_end": 24,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "ModelConfig",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 28,
          "line_end": 37,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "EnsembleResponse",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 41,
          "line_end": 51,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "OllamaEnsembleClient",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 54,
          "line_end": 708,
          "methods": [
            "__init__",
            "_initialize_models",
            "_get_available_models",
            "query_ensemble",
            "_create_role_specific_prompts",
            "_query_single_model",
            "_analyze_safety_flags",
            "_calculate_consensus",
            "_calculate_uncertainty_metrics",
            "_create_error_response",
            "get_ensemble_statistics",
            "_clean_json_response",
            "_create_valid_response_structure",
            "_extract_partial_response_data"
          ],
          "base_classes": [],
          "decorators": []
        },
        {
          "name": "RAGValidator",
          "file_path": "llm_interface\\ensemble.py",
          "line_start": 711,
          "line_end": 842,
          "methods": [
            "__init__",
            "_initialize_knowledge_base",
            "validate_response"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "llm_interface\\llm_client.py": [
        {
          "name": "ChatMessage",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 12,
          "line_end": 16,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "LLMResponse",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 20,
          "line_end": 27,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "LLMClient",
          "file_path": "llm_interface\\llm_client.py",
          "line_start": 30,
          "line_end": 679,
          "methods": [
            "__init__",
            "create_chat_messages",
            "ask",
            "ask_optimized",
            "_execute_chat_request",
            "_enhance_prompt_for_function_calling",
            "_process_function_calls",
            "_execute_function_call",
            "chat_with_function_calling",
            "_format_conversation_for_prompt",
            "get_average_inference_time",
            "get_total_inference_time",
            "get_inference_count",
            "validate_response",
            "_parse_json_response_fast",
            "_fix_common_json_issues",
            "_validate_atc_json_structure",
            "get_safe_default_resolution",
            "_create_cache_key",
            "_cache_response",
            "_get_priority_timeout",
            "get_conflict_resolution_system_prompt",
            "get_conflict_detection_system_prompt",
            "get_performance_stats",
            "reset_stats"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "analysis\\enhanced_hallucination_detection.py": [
        {
          "name": "HallucinationType",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 14,
          "line_end": 23,
          "methods": [],
          "base_classes": [
            "Enum"
          ],
          "decorators": []
        },
        {
          "name": "HallucinationResult",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 27,
          "line_end": 34,
          "methods": [],
          "base_classes": [],
          "decorators": [
            "dataclass"
          ]
        },
        {
          "name": "EnhancedHallucinationDetector",
          "file_path": "analysis\\enhanced_hallucination_detection.py",
          "line_start": 37,
          "line_end": 350,
          "methods": [
            "__init__",
            "_init_detection_patterns",
            "detect_hallucinations",
            "_check_aircraft_existence",
            "_check_altitude_validity",
            "_check_heading_validity",
            "_check_protocol_violations",
            "_check_impossible_maneuvers",
            "_check_nonsensical_response",
            "_determine_severity"
          ],
          "base_classes": [],
          "decorators": []
        }
      ],
      "analysis\\visualisation.py": [
        {
          "name": "MonteCarloVisualizer",
          "file_path": "analysis\\visualisation.py",
          "line_start": 85,
          "line_end": 924,
          "methods": [
            "__init__",
            "generate_comprehensive_report",
            "_generate_distribution_analysis",
            "_generate_trend_analysis",
            "_generate_sensitivity_analysis",
            "_plot_metric_distributions",
            "_plot_shift_comparisons",
            "_plot_violin_comparisons",
            "_plot_ridge_plots",
            "_plot_cumulative_error_curves",
            "_plot_time_series_analysis",
            "_plot_performance_evolution",
            "_plot_tornado_sensitivity"
          ],
          "base_classes": [],
          "decorators": []
        }
      ]
    }
  },
  "changes": {
    "removed_functions": [],
    "added_functions": [
      {
        "name": "_parse_resolution_response_fast",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1834,
        "line_end": 1877,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "\n        Fast resolution response parsing with minimal fallback.\n\n        Args:\n            response_text: LLM response content\n\n        Returns:\n            Parsed response dictionary or None\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_extract_partial_response_data",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 684,
        "line_end": 708,
        "args": [
          "self",
          "raw_content"
        ],
        "decorators": [],
        "docstring": "Extract partial response data from malformed JSON",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "select_best_solution",
        "file_path": "llm_interface\\filter_sort.py",
        "line_start": 30,
        "line_end": 118,
        "args": [
          "candidates",
          "policies"
        ],
        "decorators": [],
        "docstring": "Filter and select the best solution using LLM based on policies.",
        "imports_used": [],
        "intra_repo_calls": [
          "get_llm_client"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 65,
        "line_end": 78,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictDetectionMethod"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_print_detailed_analysis",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1708,
        "line_end": 1729,
        "args": [
          "self",
          "analysis"
        ],
        "decorators": [],
        "docstring": "Print detailed analysis results to console",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_validate_confidence",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 905,
        "line_end": 912,
        "args": [
          "self",
          "confidence"
        ],
        "decorators": [],
        "docstring": "Validate and normalize confidence score",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_validate_aircraft_pairs",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 888,
        "line_end": 903,
        "args": [
          "self",
          "pairs"
        ],
        "decorators": [],
        "docstring": "Validate and normalize aircraft pairs",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "execute_scenario",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 729,
        "line_end": 786,
        "args": [
          "self",
          "scenario"
        ],
        "decorators": [],
        "docstring": "\n        Execute scenario in BlueSky and return results.\n\n        Args:\n            scenario: Generated scenario configuration\n\n        Returns:\n            Execution results with conflicts detected and command log\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioConfiguration"
        ],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_parse_detection_response_fast",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1879,
        "line_end": 1911,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "\n        Fast detection response parsing.\n\n        Args:\n            response_text: LLM response content (expected JSON)\n\n        Returns:\n            Parsed detection results or None\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_reset_bluesky_simulation",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 837,
        "line_end": 844,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Reset BlueSky simulation to clean state - only if scenario doesn't include RESET",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "set_command_sender",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 317,
        "line_end": 325,
        "args": [
          "self",
          "command_sender"
        ],
        "decorators": [],
        "docstring": "\n        Set the command sender function for actual BlueSky integration\n\n        Args:\n            command_sender: Function that takes a command string and returns response\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "get_performance_stats",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 655,
        "line_end": 673,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get client performance statistics",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_calculate_horizontal_margin",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 304,
        "line_end": 311,
        "args": [
          "self",
          "geometry"
        ],
        "decorators": [],
        "docstring": "Calculate horizontal separation margin",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictGeometry"
        ],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "_calculate_cpa",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 321,
        "line_end": 414,
        "args": [
          "self",
          "lat1",
          "lon1",
          "alt1",
          "hdg1",
          "spd1",
          "vs1",
          "lat2",
          "lon2",
          "alt2",
          "hdg2",
          "spd2",
          "vs2"
        ],
        "decorators": [],
        "docstring": "\n        Calculate Closest Point of Approach (CPA) for two aircraft\n\n        Returns:\n            Tuple of (time_to_cpa, distance_at_cpa, min_horizontal_sep, min_vertical_sep)\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_calculate_expected_outcome",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 366,
        "line_end": 377,
        "args": [
          "self",
          "_assessment",
          "_commands"
        ],
        "decorators": [],
        "docstring": "Calculate expected outcome of executing the commands",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "get_stats",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 434,
        "line_end": 458,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get statistics about stored experiences",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "VectorReplayStore"
      },
      {
        "name": "_extract_aircraft_id",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1476,
        "line_end": 1496,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Extract aircraft ID from BlueSky command using configurable pattern",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "update_safety_thresholds",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 372,
        "line_end": 375,
        "args": [
          "self",
          "new_thresholds"
        ],
        "decorators": [],
        "docstring": "Update safety thresholds for verification",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "assess_conflict",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 81,
        "line_end": 126,
        "args": [
          "self",
          "aircraft_info"
        ],
        "decorators": [],
        "docstring": "\n        Assess current aircraft situation for potential conflicts\n\n        Args:\n            aircraft_info: Dictionary containing all aircraft information\n\n        Returns:\n            ConflictAssessment or None if no conflicts detected\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "generate_horizontal_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1242,
        "line_end": 1247,
        "args": [
          "n_aircraft",
          "conflict"
        ],
        "decorators": [],
        "docstring": "Generate horizontal scenario - convenience function",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioGenerator",
          "Scenario"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_detect_with_swarm",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 121,
        "line_end": 169,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Detect conflicts using BlueSky SWARM method",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "get_step_by_id",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 269,
        "line_end": 274,
        "args": [
          "self",
          "step_id"
        ],
        "decorators": [],
        "docstring": "Get a specific step by its ID",
        "imports_used": [],
        "intra_repo_calls": [
          "ReasoningStep"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "set_strict_mode",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 752,
        "line_end": 756,
        "args": [
          "enabled"
        ],
        "decorators": [],
        "docstring": "Enable or disable strict mode for BlueSky operations",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_calculate_separation",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 215,
        "line_end": 240,
        "args": [
          "self",
          "ac1_data",
          "ac2_data"
        ],
        "decorators": [],
        "docstring": "Calculate horizontal and vertical separation between aircraft",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "_cache_response",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 596,
        "line_end": 606,
        "args": [
          "self",
          "cache_key",
          "response"
        ],
        "decorators": [],
        "docstring": "Cache response with size limit",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_analyze_aircraft_pair_trajectory",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 736,
        "line_end": 880,
        "args": [
          "self",
          "ac1",
          "ac2"
        ],
        "decorators": [],
        "docstring": "\n        Analyze aircraft pair for potential conflicts using proper trajectory projection.\n\n        Returns:\n            dict: Analysis results including conflict status, CPA time, minimum separation\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_validate_priority",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 914,
        "line_end": 921,
        "args": [
          "self",
          "priority"
        ],
        "decorators": [],
        "docstring": "Validate and normalize priority level",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_enhance_prompt_for_function_calling",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 304,
        "line_end": 327,
        "args": [
          "self",
          "original_prompt"
        ],
        "decorators": [],
        "docstring": "Enhance prompt to include function calling instructions",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "detect_conflict_via_llm",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1109,
        "line_end": 1148,
        "args": [
          "self",
          "aircraft_states",
          "time_horizon",
          "cpa_data"
        ],
        "decorators": [],
        "docstring": "\n        High-level API for LLM-based conflict detection.\n\n        Args:\n            aircraft_states: List of aircraft state dictionaries\n            time_horizon: Time horizon in minutes\n            cpa_data: Optional Closest Point of Approach data with additional context\n                     including timing, separation distances, and severity information\n\n        Returns:\n            Dictionary with detection results\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_parse_json_response_fast",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 495,
        "line_end": 537,
        "args": [
          "self",
          "content"
        ],
        "decorators": [],
        "docstring": "Enhanced JSON parsing with robust fallback and validation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "get_conflict_resolution_optimized",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1752,
        "line_end": 1792,
        "args": [
          "self",
          "conflict_info",
          "priority"
        ],
        "decorators": [],
        "docstring": "\n        High-performance conflict resolution API.\n\n        Args:\n            conflict_info: Conflict scenario data\n            priority: Request priority ('low', 'normal', 'high')\n\n        Returns:\n            ResolutionResponse or None if failed\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ResolutionResponse"
        ],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "get_active_executions",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 281,
        "line_end": 283,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get all currently active executions",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult"
        ],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 44,
        "line_end": 56,
        "args": [
          "self",
          "safety_thresholds"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "_create_default_config",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 65,
        "line_end": 113,
        "args": [
          "self",
          "path"
        ],
        "decorators": [],
        "docstring": "Create a default configuration file",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyConfig"
      },
      {
        "name": "_create_enhanced_modelfile",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 315,
        "line_end": 421,
        "args": [
          "self",
          "model_source",
          "final_loss",
          "eval_loss"
        ],
        "decorators": [],
        "docstring": "Create enhanced Modelfile content.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "_run_single_scenario",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 602,
        "line_end": 679,
        "args": [
          "self",
          "scenario",
          "scenario_id"
        ],
        "decorators": [],
        "docstring": "\n        Execute a single scenario with comprehensive error handling and success tracking.\n\n        Args:\n            scenario: Generated scenario object\n            scenario_id: Unique scenario identifier\n\n        Returns:\n            ScenarioResult with success flag properly set\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioResult",
          "ScenarioType",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_create_error_response",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 589,
        "line_end": 603,
        "args": [
          "self",
          "error_msg",
          "response_time"
        ],
        "decorators": [],
        "docstring": "Create error response when ensemble fails",
        "imports_used": [],
        "intra_repo_calls": [
          "EnsembleResponse"
        ],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "reset_performance_stats",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1943,
        "line_end": 1945,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Reset performance tracking",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_plot_cumulative_error_curves",
        "file_path": "analysis\\visualisation.py",
        "line_start": 595,
        "line_end": 668,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Create cumulative false-positive/negative curves (ECDFs).",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "get_execution_status",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 261,
        "line_end": 279,
        "args": [
          "self",
          "execution_id"
        ],
        "decorators": [],
        "docstring": "\n        Get current status of an execution\n\n        Args:\n            execution_id: ID of execution to check\n\n        Returns:\n            ExecutionStatus or None if not found\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionStatus"
        ],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "parse_detector_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 599,
        "line_end": 694,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "\n        Parse LLM detector response with enhanced validation for sector scenarios.\n\n        Args:\n            response_text: Raw response from LLM\n\n        Returns:\n            Parsed detection results with validation status\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_calculate_consensus",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 471,
        "line_end": 554,
        "args": [
          "self",
          "responses"
        ],
        "decorators": [],
        "docstring": "Calculate consensus decision from ensemble responses",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "get_steps_by_type",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 276,
        "line_end": 278,
        "args": [
          "self",
          "step_type"
        ],
        "decorators": [],
        "docstring": "Get all steps of a specific type",
        "imports_used": [],
        "intra_repo_calls": [
          "StepType",
          "ReasoningStep"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "detect_hallucinations",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 90,
        "line_end": 186,
        "args": [
          "self",
          "llm_response",
          "baseline_response",
          "context"
        ],
        "decorators": [],
        "docstring": "\n        Detect hallucinations in LLM response using multiple strategies\n\n        Args:\n            llm_response: Response from LLM model\n            baseline_response: Baseline/ground truth response\n            context: Context information including scenario data\n\n        Returns:\n            HallucinationResult with detection details\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "HallucinationResult",
          "HallucinationType"
        ],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "_analyze_distribution_shift_performance",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 940,
        "line_end": 969,
        "args": [
          "self",
          "results_df"
        ],
        "decorators": [],
        "docstring": "Analyze performance across different distribution shift levels.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_calculate_all_separations",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1381,
        "line_end": 1417,
        "args": [
          "self",
          "aircraft_info"
        ],
        "decorators": [],
        "docstring": "Calculate separations between all aircraft pairs",
        "imports_used": [],
        "intra_repo_calls": [
          "aircraft_list"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_setup_logging",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 321,
        "line_end": 340,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Setup detailed logging for benchmark execution",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "get_plan_history",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 402,
        "line_end": 404,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get history of generated plans",
        "imports_used": [],
        "intra_repo_calls": [
          "ActionPlan"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "_check_aircraft_existence",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 188,
        "line_end": 211,
        "args": [
          "self",
          "response_text",
          "context"
        ],
        "decorators": [],
        "docstring": "Check if response references non-existent aircraft",
        "imports_used": [],
        "intra_repo_calls": [
          "aircraft_list"
        ],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "get_conflict_resolution_with_prompts",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1045,
        "line_end": 1107,
        "args": [
          "self",
          "conflict_info",
          "use_function_calls"
        ],
        "decorators": [],
        "docstring": "\n        Enhanced API for getting conflict resolution that returns prompt and response data.\n\n        Args:\n            conflict_info: Conflict scenario information\n            use_function_calls: Override function calling setting\n\n        Returns:\n            Dictionary with resolution data including prompt and response\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "print_metrics_summary",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 167,
        "line_end": 196,
        "args": [
          "metrics"
        ],
        "decorators": [],
        "docstring": "Print a formatted summary of the metrics using logging.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_verify_resolutions",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1287,
        "line_end": 1379,
        "args": [
          "self",
          "scenario",
          "resolutions"
        ],
        "decorators": [],
        "docstring": "Verify resolution effectiveness by stepping simulation with adaptive time stepping",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "get_execution_history",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 285,
        "line_end": 287,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get history of all executions",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult"
        ],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "_create_vertical_conflicts",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 541,
        "line_end": 568,
        "args": [
          "self",
          "aircraft_states"
        ],
        "decorators": [],
        "docstring": "Create altitude/climb commands to generate vertical conflicts",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_normalize_bluesky_command",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1424,
        "line_end": 1474,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Normalize and validate BlueSky command format using configurable aircraft ID pattern",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "calc_efficiency_penalty",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 49,
        "line_end": 50,
        "args": [
          "planned",
          "executed"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_merge_conflict_detections",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 494,
        "line_end": 533,
        "args": [
          "self",
          "detections"
        ],
        "decorators": [],
        "docstring": "Merge multiple conflict detections for the same aircraft pair",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "get_conflict_data",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 371,
        "line_end": 438,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get real conflict data from BlueSky",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_project_position",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1149,
        "line_end": 1181,
        "args": [
          "self",
          "lat",
          "lon",
          "heading",
          "speed_kts",
          "time_min"
        ],
        "decorators": [],
        "docstring": "Project aircraft position based on heading and speed",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "create_empty_metrics",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 147,
        "line_end": 164,
        "args": [],
        "decorators": [],
        "docstring": "Create empty metrics structure when no data is available.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_validate_sector_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 923,
        "line_end": 952,
        "args": [
          "self",
          "json_data"
        ],
        "decorators": [],
        "docstring": "Additional validation for sector scenarios",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "merge_lora_adapter",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 64,
        "line_end": 126,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Merge LoRA adapter with base model.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "_create_detection_comparison",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 2190,
        "line_end": 2253,
        "args": [
          "self",
          "scenario",
          "scenario_id",
          "result",
          "execution_time"
        ],
        "decorators": [],
        "docstring": "Create detection comparison record",
        "imports_used": [],
        "intra_repo_calls": [
          "DetectionComparison",
          "ScenarioResult"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_calculate_horizontal_ground_truth",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 703,
        "line_end": 734,
        "args": [
          "self",
          "aircraft_states",
          "expect_conflicts"
        ],
        "decorators": [],
        "docstring": "Calculate ground truth conflicts for horizontal scenarios with time-based analysis",
        "imports_used": [],
        "intra_repo_calls": [
          "GroundTruthConflict"
        ],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "validate_response",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 769,
        "line_end": 842,
        "args": [
          "self",
          "response",
          "context"
        ],
        "decorators": [],
        "docstring": "Validate response against aviation knowledge base",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "RAGValidator"
      },
      {
        "name": "_create_success_rate_chart",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 1098,
        "line_end": 1144,
        "args": [
          "self",
          "success_data",
          "save_path"
        ],
        "decorators": [],
        "docstring": "Create bar chart of success rates by scenario type.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 423,
        "line_end": 425,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "SafetyMarginQuantifier"
        ],
        "is_method": true,
        "class_name": "SafetyMetricsAggregator"
      },
      {
        "name": "_get_aircraft_states_for_llm",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1060,
        "line_end": 1093,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get current aircraft states formatted for LLM",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_parse_safety_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1555,
        "line_end": 1672,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Parse safety assessment response with robust fallbacks for missing fields",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_assess_safety_margins",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 724,
        "line_end": 759,
        "args": [
          "self",
          "margins"
        ],
        "decorators": [],
        "docstring": "Assess safety margin performance.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_check_hallucination_patterns",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 185,
        "line_end": 259,
        "args": [
          "self",
          "scenario_context",
          "environmental_conditions",
          "similar_experiences"
        ],
        "decorators": [],
        "docstring": "Check for hallucination risk patterns",
        "imports_used": [],
        "intra_repo_calls": [
          "SimilarityResult"
        ],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "send_bluesky_command",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 474,
        "line_end": 519,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Send command to BlueSky simulator",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_load_ranges",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 152,
        "line_end": 161,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Load scenario ranges from YAML configuration",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "weighted_choice",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 220,
        "line_end": 226,
        "args": [
          "self",
          "choices",
          "weights"
        ],
        "decorators": [],
        "docstring": "Make a weighted choice from options",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_create_shift_performance_scatter",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 1237,
        "line_end": 1300,
        "args": [
          "self",
          "shift_data",
          "save_path"
        ],
        "decorators": [],
        "docstring": "Create scatter plot of performance vs distribution shift level.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "main",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 510,
        "line_end": 522,
        "args": [],
        "decorators": [],
        "docstring": "Main function to run the LoRA merger.",
        "imports_used": [],
        "intra_repo_calls": [
          "LoRAMerger"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_create_vertical_conflicts_enhanced",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 585,
        "line_end": 614,
        "args": [
          "self",
          "aircraft_states",
          "climb_rates"
        ],
        "decorators": [],
        "docstring": "Enhanced vertical conflict creation with configurable climb rates",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_get_available_models",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 169,
        "line_end": 227,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get list of available Ollama models",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "query_ensemble",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 229,
        "line_end": 305,
        "args": [
          "self",
          "prompt",
          "context",
          "require_json",
          "timeout"
        ],
        "decorators": [],
        "docstring": "Query ensemble of models and return consensus response",
        "imports_used": [],
        "intra_repo_calls": [
          "EnsembleResponse"
        ],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "_assess_detection_performance",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 689,
        "line_end": 722,
        "args": [
          "self",
          "detection"
        ],
        "decorators": [],
        "docstring": "Assess detection performance and provide interpretation.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_assess_conflict_severity",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 459,
        "line_end": 472,
        "args": [
          "self",
          "h_sep",
          "v_sep"
        ],
        "decorators": [],
        "docstring": "Assess conflict severity based on separation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "__init__",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1222,
        "line_end": 1223,
        "args": [
          "self",
          "generator"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioGenerator"
        ],
        "is_method": true,
        "class_name": "SectorCREnv"
      },
      {
        "name": "_calculate_temporal_margin",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 321,
        "line_end": 327,
        "args": [
          "self",
          "geometry"
        ],
        "decorators": [],
        "docstring": "Calculate temporal margin (time available for corrective action)",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictGeometry"
        ],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "compute_success_rates_by_scenario",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 183,
        "line_end": 231,
        "args": [
          "self",
          "results_df"
        ],
        "decorators": [],
        "docstring": "\n        Compute success rates grouped by scenario type.\n\n        Args:\n            results_df: DataFrame with columns 'scenario_type', 'success'\n\n        Returns:\n            Dict mapping scenario types to success metrics\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_query_single_model",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 369,
        "line_end": 418,
        "args": [
          "self",
          "model_config",
          "prompt",
          "require_json"
        ],
        "decorators": [],
        "docstring": "Query a single model in the ensemble",
        "imports_used": [],
        "intra_repo_calls": [
          "ModelConfig"
        ],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "_simulate_command_execution",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 206,
        "line_end": 242,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "\n        Simulate command execution for testing purposes\n\n        Args:\n            command: BlueSky command string\n\n        Returns:\n            Simulated response dictionary\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "_is_valid_bluesky_command",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1226,
        "line_end": 1253,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Validate if a command is a valid BlueSky command",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_format_distribution_shift_analysis",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 799,
        "line_end": 826,
        "args": [
          "self",
          "shift_analysis"
        ],
        "decorators": [],
        "docstring": "Format distribution shift analysis as markdown.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "format_detector_prompt",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 387,
        "line_end": 488,
        "args": [
          "self",
          "aircraft_states",
          "time_horizon",
          "cpa_data"
        ],
        "decorators": [],
        "docstring": "\n        Create a prompt for LLM-based conflict detection with enhanced sector support.\n\n        Args:\n            aircraft_states: List of aircraft state dictionaries\n            time_horizon: Time horizon in minutes for conflict detection\n            cpa_data: Optional Closest Point of Approach data with additional context\n\n        Returns:\n            Formatted detection prompt string\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "aircraft_list"
        ],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_create_error_result",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1462,
        "line_end": 1517,
        "args": [
          "self",
          "scenario_id",
          "scenario_type",
          "complexity_tier",
          "shift_level",
          "error"
        ],
        "decorators": [],
        "docstring": "Create error result for failed scenarios",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioResult",
          "ScenarioType",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 65,
        "line_end": 78,
        "args": [
          "self",
          "session_id"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "SessionSummary",
          "ReasoningStep"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "aggregate_monte_carlo_metrics",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 891,
        "line_end": 938,
        "args": [
          "self",
          "results_df"
        ],
        "decorators": [],
        "docstring": "\n        Compute comprehensive aggregated metrics from Monte Carlo results.\n\n        Args:\n            results_df: DataFrame with simulation results\n\n        Returns:\n            Dict containing all aggregated metrics\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_check_nonsensical_response",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 309,
        "line_end": 326,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Check for nonsensical or gibberish responses",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "log_planning_step",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 148,
        "line_end": 170,
        "args": [
          "self",
          "plan"
        ],
        "decorators": [],
        "docstring": "Log an action planning step",
        "imports_used": [],
        "intra_repo_calls": [
          "ActionPlan"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "log_step",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 80,
        "line_end": 125,
        "args": [
          "self",
          "step_data"
        ],
        "decorators": [],
        "docstring": "\n        Log a reasoning step in the current session\n\n        Args:\n            step_data: Dictionary containing step information\n\n        Returns:\n            step_id of the logged step\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "StepType",
          "ReasoningStep"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "_plot_efficiency_metrics",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1908,
        "line_end": 1972,
        "args": [
          "self",
          "df",
          "fig_size"
        ],
        "decorators": [],
        "docstring": "Plot efficiency and cost metrics",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_save_results",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 2111,
        "line_end": 2154,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Save results in multiple formats",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_assess_conflict_severity",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 436,
        "line_end": 453,
        "args": [
          "self",
          "h_sep",
          "v_sep",
          "time_to_cpa",
          "violates_icao"
        ],
        "decorators": [],
        "docstring": "Assess conflict severity based on ICAO standards and time to conflict",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_optimize_conflict_timing",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 657,
        "line_end": 681,
        "args": [
          "self",
          "aircraft_states",
          "commands"
        ],
        "decorators": [],
        "docstring": "Optimize timing to create near-threshold vertical conflicts",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_execute_function_call",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 364,
        "line_end": 386,
        "args": [
          "self",
          "function_name",
          "arguments"
        ],
        "decorators": [],
        "docstring": "Execute a function call and return the result",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_run_enhanced_scenario",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 2159,
        "line_end": 2188,
        "args": [
          "self",
          "scenario",
          "scenario_id"
        ],
        "decorators": [],
        "docstring": "Enhanced scenario execution with detailed logging",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioResult"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_cross_validate_conflicts",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 477,
        "line_end": 492,
        "args": [
          "self",
          "all_conflicts"
        ],
        "decorators": [],
        "docstring": "Cross-validate conflicts detected by multiple methods",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_calculate_distance_nm",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1104,
        "line_end": 1122,
        "args": [
          "self",
          "lat1",
          "lon1",
          "lat2",
          "lon2"
        ],
        "decorators": [],
        "docstring": "Calculate distance between two points in nautical miles",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "send_plan",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 52,
        "line_end": 164,
        "args": [
          "self",
          "plan"
        ],
        "decorators": [],
        "docstring": "\n        Execute an action plan by sending commands to BlueSky\n\n        Args:\n            plan: ActionPlan to execute\n\n        Returns:\n            ExecutionResult with execution status and details\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult",
          "ActionPlan",
          "ExecutionStatus"
        ],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 75,
        "line_end": 79,
        "args": [
          "self",
          "llm_client"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "ActionPlan",
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "_validate_atc_json_structure",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 559,
        "line_end": 576,
        "args": [
          "self",
          "parsed_json"
        ],
        "decorators": [],
        "docstring": "Validate that parsed JSON has expected ATC structure",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_extract_key_decisions",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 400,
        "line_end": 412,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Extract key decisions from the session",
        "imports_used": [],
        "intra_repo_calls": [
          "StepType"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "_generate_summary_by_group",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1641,
        "line_end": 1669,
        "args": [
          "self",
          "df",
          "group_column"
        ],
        "decorators": [],
        "docstring": "Generate success rate and metrics summary by a specific grouping column",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "main",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 2371,
        "line_end": 2439,
        "args": [],
        "decorators": [],
        "docstring": "Main entry point for standalone execution",
        "imports_used": [],
        "intra_repo_calls": [
          "MonteCarloBenchmark",
          "BenchmarkConfiguration",
          "run_benchmark_with_config"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "validate_ranges",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 892,
        "line_end": 947,
        "args": [
          "self",
          "scenario"
        ],
        "decorators": [],
        "docstring": "\n        Validate that generated scenario parameters are within specified ranges.\n\n        Args:\n            scenario: Generated scenario to validate\n\n        Returns:\n            Dictionary of validation results\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioConfiguration"
        ],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_determine_safety_level",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 397,
        "line_end": 405,
        "args": [
          "self",
          "effective_margin"
        ],
        "decorators": [],
        "docstring": "Determine safety level based on effective margin",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "_run_scenario_batch",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 481,
        "line_end": 556,
        "args": [
          "self",
          "scenario_type",
          "complexity_tier",
          "shift_level"
        ],
        "decorators": [],
        "docstring": "\n        Execute a batch of scenarios for given parameters.\n\n        Args:\n            scenario_type: Type of scenario to generate\n            complexity_tier: Complexity level\n            shift_level: Distribution shift level\n\n        Returns:\n            Number of scenarios successfully executed\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioType",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "generate_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1225,
        "line_end": 1238,
        "args": [
          "self",
          "complexity",
          "shift_level",
          "force_conflicts"
        ],
        "decorators": [],
        "docstring": "Generate sector scenario",
        "imports_used": [],
        "intra_repo_calls": [
          "ComplexityTier",
          "Scenario"
        ],
        "is_method": true,
        "class_name": "SectorCREnv"
      },
      {
        "name": "_create_role_specific_prompts",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 307,
        "line_end": 367,
        "args": [
          "self",
          "base_prompt",
          "context"
        ],
        "decorators": [],
        "docstring": "Create role-specific prompts for different models",
        "imports_used": [],
        "intra_repo_calls": [
          "ModelRole"
        ],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "_estimate_time_to_conflict",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 261,
        "line_end": 265,
        "args": [
          "self",
          "_ac1_data",
          "_ac2_data"
        ],
        "decorators": [],
        "docstring": "Estimate time to conflict in seconds",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "_enhance_decision_with_experience",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 261,
        "line_end": 363,
        "args": [
          "self",
          "llm_decision",
          "baseline_decision",
          "similar_experiences"
        ],
        "decorators": [],
        "docstring": "Enhance current decision using historical experience",
        "imports_used": [],
        "intra_repo_calls": [
          "SimilarityResult"
        ],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "_calculate_effective_margin",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 329,
        "line_end": 350,
        "args": [
          "self",
          "h_margin",
          "v_margin",
          "t_margin"
        ],
        "decorators": [],
        "docstring": "Calculate effective combined margin using weighted approach",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "__init__",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 714,
        "line_end": 715,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "RAGValidator"
      },
      {
        "name": "_parse_detector_response_legacy",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 783,
        "line_end": 848,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Legacy text-based parsing for detector responses",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_generate_recommendations",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 828,
        "line_end": 889,
        "args": [
          "self",
          "metrics"
        ],
        "decorators": [],
        "docstring": "Generate specific recommendations based on the analysis.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_validate_calculation_details",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 954,
        "line_end": 978,
        "args": [
          "self",
          "calc_details"
        ],
        "decorators": [],
        "docstring": "Validate calculation details for mathematical accuracy",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_read_json_results",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 89,
        "line_end": 113,
        "args": [
          "self",
          "file_path"
        ],
        "decorators": [],
        "docstring": "Read results from JSON file format.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_generate_reasoning",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 323,
        "line_end": 333,
        "args": [
          "self",
          "conflict",
          "action"
        ],
        "decorators": [],
        "docstring": "Generate human-readable reasoning for the recommended action",
        "imports_used": [],
        "intra_repo_calls": [
          "PlanType"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "_check_protocol_violations",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 253,
        "line_end": 274,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Check for ATC protocol violations",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "_calculate_horizontal_distance",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 416,
        "line_end": 434,
        "args": [
          "self",
          "lat1",
          "lon1",
          "lat2",
          "lon2"
        ],
        "decorators": [],
        "docstring": "Calculate horizontal distance between two points in nautical miles",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "generate_vertical_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 277,
        "line_end": 415,
        "args": [
          "self",
          "n_aircraft",
          "conflict",
          "climb_rates",
          "crossing_altitudes",
          "complexity_tier",
          "distribution_shift_tier"
        ],
        "decorators": [],
        "docstring": "\n        Generate vertical conflict scenario.\n\n        Aircraft at different altitudes with climb/descent creating vertical conflicts.\n\n        Args:\n            n_aircraft: Number of aircraft (2-5)\n            conflict: Whether to create conflicts (True) or safe scenarios (False)\n            climb_rates: List of climb/descent rates in fpm (default: [-1500, 0, 1500])\n            crossing_altitudes: List of target altitudes for vertical maneuvers (default: auto-generated)\n            complexity_tier: Scenario complexity\n            distribution_shift_tier: Distribution shift level\n\n        Returns:\n            Vertical conflict scenario with ground truth\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioType",
          "ComplexityTier",
          "Scenario"
        ],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_calculate_horizontal_distance",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 416,
        "line_end": 434,
        "args": [
          "self",
          "lat1",
          "lon1",
          "lat2",
          "lon2"
        ],
        "decorators": [],
        "docstring": "Calculate horizontal distance between two points in nautical miles",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "haversine_distance",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 791,
        "line_end": 820,
        "args": [
          "lat1",
          "lon1",
          "lat2",
          "lon2"
        ],
        "decorators": [],
        "docstring": "\n    Calculate the great circle distance between two points on Earth using the Haversine formula\n\n    Args:\n        lat1, lon1: Latitude and longitude of first point in degrees\n        lat2, lon2: Latitude and longitude of second point in degrees\n\n    Returns:\n        Distance in nautical miles\n    ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 56,
        "line_end": 102,
        "args": [
          "self",
          "model",
          "enable_function_calls",
          "aircraft_id_regex",
          "enable_streaming",
          "enable_caching",
          "enable_optimized_prompts"
        ],
        "decorators": [],
        "docstring": "\n        Initialize the LLM prompt engine.\n\n        Args:\n            model: LLM model to use for queries\n            enable_function_calls: Whether to enable function calling capabilities\n            aircraft_id_regex: Regular expression pattern for validating aircraft callsigns.\n                              Default pattern accepts alphanumeric characters and hyphens.\n                              Examples: r'^[A-Z]{2,4}\\d{2,4}[A-Z]?$' for traditional ICAO format,\n                                       r'^[A-Z0-9-]+$' for flexible alphanumeric with hyphens.\n            enable_streaming: Use streaming for faster responses\n            enable_caching: Cache responses for repeated scenarios\n                              enable_optimized_prompts: Use optimized, shorter prompt templates\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "LLMClient"
        ],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "generate_horizontal_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 164,
        "line_end": 275,
        "args": [
          "self",
          "n_aircraft",
          "conflict",
          "complexity_tier",
          "distribution_shift_tier"
        ],
        "decorators": [],
        "docstring": "\n        Generate horizontal conflict scenario.\n\n        All aircraft at same flight level to eliminate vertical separation.\n        Adjust headings to create/avoid horizontal conflicts.\n\n        Args:\n            n_aircraft: Number of aircraft (2-5)\n            conflict: Whether to create conflicts (True) or safe scenarios (False)\n            complexity_tier: Scenario complexity\n            distribution_shift_tier: Distribution shift level\n\n        Returns:\n            Horizontal conflict scenario with ground truth\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyScenarioGenerator",
          "ScenarioType",
          "ComplexityTier",
          "Scenario"
        ],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_avoid_horizontal_conflicts",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 524,
        "line_end": 539,
        "args": [
          "self",
          "aircraft_states"
        ],
        "decorators": [],
        "docstring": "Create heading adjustments to avoid horizontal conflicts",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 46,
        "line_end": 50,
        "args": [
          "self",
          "command_sender"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult"
        ],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "_load_config",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 115,
        "line_end": 126,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Load configuration from file",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyConfig"
      },
      {
        "name": "_calculate_confidence",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 317,
        "line_end": 334,
        "args": [
          "self",
          "verification"
        ],
        "decorators": [],
        "docstring": "Calculate confidence in verification results",
        "imports_used": [],
        "intra_repo_calls": [
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "_plot_time_series_analysis",
        "file_path": "analysis\\visualisation.py",
        "line_start": 670,
        "line_end": 758,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Create time-series analysis of conflict events.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "read_results_file",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 62,
        "line_end": 87,
        "args": [
          "self",
          "file_path"
        ],
        "decorators": [],
        "docstring": "\n        Read Monte Carlo results from JSON or CSV file.\n\n        Args:\n            file_path: Path to results file (.json or .csv)\n\n        Returns:\n            DataFrame with simulation results\n\n        Raises:\n            FileNotFoundError: If file doesn't exist\n            ValueError: If file format not supported\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_merge_conflict_detections",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 494,
        "line_end": 533,
        "args": [
          "self",
          "detections"
        ],
        "decorators": [],
        "docstring": "Merge multiple conflict detections for the same aircraft pair",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_send_command",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 166,
        "line_end": 204,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "\n        Send a single command to BlueSky simulator\n\n        Args:\n            command: BlueSky command string\n\n        Returns:\n            Response dictionary with success status and details\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "reset_stats",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 675,
        "line_end": 679,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Reset performance tracking",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_calculate_confidence",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 455,
        "line_end": 475,
        "args": [
          "self",
          "method",
          "h_sep",
          "v_sep",
          "time_to_cpa"
        ],
        "decorators": [],
        "docstring": "Calculate confidence score for conflict detection",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_extract_aircraft_id_fast",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1913,
        "line_end": 1917,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Fast aircraft ID extraction",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "generate_safety_summary",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 470,
        "line_end": 514,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Generate comprehensive safety summary across all conflicts",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "SafetyMetricsAggregator"
      },
      {
        "name": "_analyze_aircraft_pair",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 240,
        "line_end": 319,
        "args": [
          "self",
          "ac1_idx",
          "ac2_idx",
          "method"
        ],
        "decorators": [],
        "docstring": "\n        Analyze specific aircraft pair for conflicts with CPA calculation\n        Implements the 300s time horizon check as requested\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_validate_llm_conflicts_with_bluesky",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1095,
        "line_end": 1149,
        "args": [
          "self",
          "llm_pairs",
          "bluesky_conflicts"
        ],
        "decorators": [],
        "docstring": "\n        Validate LLM-detected conflicts against BlueSky ground truth\n        to eliminate false positives.\n\n        Args:\n            llm_pairs: Aircraft pairs detected by LLM\n            bluesky_conflicts: Conflicts detected by BlueSky\n\n        Returns:\n            Validated aircraft pairs confirmed by BlueSky\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_assess_conflict_severity",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 436,
        "line_end": 453,
        "args": [
          "self",
          "h_sep",
          "v_sep",
          "time_to_cpa",
          "violates_icao"
        ],
        "decorators": [],
        "docstring": "Assess conflict severity based on ICAO standards and time to conflict",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_get_serializable_config",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1614,
        "line_end": 1639,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get configuration as JSON-serializable dictionary with enum conversion",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "create_enhanced_detector",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 353,
        "line_end": 355,
        "args": [],
        "decorators": [],
        "docstring": "Factory function to create enhanced hallucination detector",
        "imports_used": [],
        "intra_repo_calls": [
          "EnhancedHallucinationDetector"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_setup_enhanced_logging",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 342,
        "line_end": 375,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Setup enhanced logging with separate loggers for different components",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_calculate_average_confidence",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 392,
        "line_end": 398,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Calculate average confidence across all steps",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "to_dict",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 95,
        "line_end": 97,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Convert to dictionary for compatibility with existing code",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scenario"
      },
      {
        "name": "calc_separation_margin",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 46,
        "line_end": 47,
        "args": [
          "trajectories"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_generate_sensitivity_analysis",
        "file_path": "analysis\\visualisation.py",
        "line_start": 212,
        "line_end": 224,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Generate sensitivity and uncertainty visualizations.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "_plot_ridge_plots",
        "file_path": "analysis\\visualisation.py",
        "line_start": 496,
        "line_end": 593,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Create ridge plots (joy plots) for metric distributions.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "clear_all",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 470,
        "line_end": 487,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Clear all experiences from the store",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "VectorReplayStore"
      },
      {
        "name": "apply_distribution_shift",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 228,
        "line_end": 391,
        "args": [
          "self",
          "base_ranges",
          "shift_tier"
        ],
        "decorators": [],
        "docstring": "\n        Apply distribution shift to base ranges according to specified tier.\n\n        This function warps the YAML ranges based on the shift configuration,\n        ensuring all concrete values still come from BlueSky command sampling.\n\n        Args:\n            base_ranges: Original ranges from scenario_ranges.yaml\n            shift_tier: Tier name from distribution_shift_levels.yaml\n                       ('in_distribution', 'moderate_shift', 'extreme_shift')\n\n        Returns:\n            Modified ranges with shifts applied\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_generate_trend_analysis",
        "file_path": "analysis\\visualisation.py",
        "line_start": 197,
        "line_end": 210,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Generate trend and cumulative analysis visualizations.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "generate_action_plan",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 128,
        "line_end": 176,
        "args": [
          "self",
          "assessment"
        ],
        "decorators": [],
        "docstring": "\n        Generate detailed action plan based on conflict assessment\n\n        Args:\n            assessment: ConflictAssessment from assess_conflict\n\n        Returns:\n            ActionPlan with specific commands and expected outcomes\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ActionPlan",
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "_generate_session_summary",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 369,
        "line_end": 390,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Generate a summary of the current session",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "get_conflict_resolution_system_prompt",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 618,
        "line_end": 635,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Concise system prompt for conflict resolution",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_calculate_total_scenarios",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 455,
        "line_end": 479,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Calculate total number of scenarios to be executed",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_create_detection_performance_chart",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 1146,
        "line_end": 1189,
        "args": [
          "self",
          "detection_data",
          "save_path"
        ],
        "decorators": [],
        "docstring": "Create bar chart of false positive/negative rates.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "_parse_distilled_model_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 711,
        "line_end": 781,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Parse response from the fine-tuned BlueSky Gym distilled model",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_init_csv_file",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 377,
        "line_end": 409,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize CSV file with headers",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_verify_ollama_model",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 423,
        "line_end": 453,
        "args": [
          "self",
          "model_name"
        ],
        "decorators": [],
        "docstring": "Verify the created Ollama model.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "reset_simulation",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1314,
        "line_end": 1351,
        "args": [],
        "decorators": [],
        "docstring": "\n    Reset the BlueSky simulation to initial state.\n\n    Returns:\n        Status dictionary with reset information\n    ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "format_conflict_prompt",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 323,
        "line_end": 385,
        "args": [
          "self",
          "conflict_info"
        ],
        "decorators": [],
        "docstring": "\n        Create a descriptive natural-language prompt for conflict resolution.\n\n        Args:\n            conflict_info: Dictionary containing conflict and aircraft information\n\n        Returns:\n            Formatted prompt string for LLM query\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "__post_init__",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 86,
        "line_end": 93,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize optional fields to sensible defaults",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scenario"
      },
      {
        "name": "get_total_inference_time",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 473,
        "line_end": 475,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get total inference time across all calls.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "calc_path_extra",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 233,
        "line_end": 268,
        "args": [
          "actual_traj",
          "original_traj"
        ],
        "decorators": [],
        "docstring": "Calculate extra distance traveled due to resolution maneuvers.",
        "imports_used": [],
        "intra_repo_calls": [
          "calc_trajectory_distance"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "get_inference_count",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 477,
        "line_end": 479,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get total number of LLM calls made.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_get_mock_conflict_data",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 682,
        "line_end": 705,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Generate mock conflict data when BlueSky unavailable",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "calc_trajectory_distance",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 241,
        "line_end": 263,
        "args": [
          "traj"
        ],
        "decorators": [],
        "docstring": "Calculate total distance of a trajectory.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "detect_conflicts_comprehensive",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 80,
        "line_end": 119,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "\n        Detect conflicts using all available methods and cross-validate results\n\n        Returns:\n            List of validated conflict data\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_determine_conflict_severity",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1065,
        "line_end": 1084,
        "args": [
          "self",
          "horizontal_sep",
          "vertical_sep"
        ],
        "decorators": [],
        "docstring": "Determine conflict severity based on separation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "ask_optimized",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 183,
        "line_end": 264,
        "args": [
          "self",
          "user_prompt",
          "system_prompt",
          "expect_json",
          "context",
          "priority"
        ],
        "decorators": [],
        "docstring": "\n        High-performance LLM query with proper chat formatting.\n\n        Args:\n            user_prompt: User question/request\n            system_prompt: System instructions (ATC role, requirements)\n            expect_json: Whether to expect JSON response\n            context: Conversation context\n            priority: Request priority for timeout adjustment\n\n        Returns:\n            LLMResponse with content and performance metrics\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ChatMessage",
          "LLMResponse"
        ],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_extract_bluesky_command",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1276,
        "line_end": 1422,
        "args": [
          "self",
          "text"
        ],
        "decorators": [],
        "docstring": "\n        Extract BlueSky command using simplified two-pass approach.\n\n        First pass: Look for explicit BlueSky commands (HDG/ALT/SPD/VS)\n        Second pass: Look for natural language patterns\n        Third pass: Check for function call format\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "get_all_experiences",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 362,
        "line_end": 432,
        "args": [
          "self",
          "conflict_type",
          "num_ac",
          "limit"
        ],
        "decorators": [],
        "docstring": "\n        Get all experiences, optionally filtered by metadata\n\n        Args:\n            conflict_type: Optional conflict type filter\n            num_ac: Optional number of aircraft filter\n            limit: Optional limit on number of results\n\n        Returns:\n            List of experience documents\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "VectorReplayStore"
      },
      {
        "name": "get_aircraft_data",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 287,
        "line_end": 369,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get real aircraft data from BlueSky",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_plot_violin_comparisons",
        "file_path": "analysis\\visualisation.py",
        "line_start": 410,
        "line_end": 494,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Create violin plots for separation margins by categories.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "get_distance",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1173,
        "line_end": 1239,
        "args": [
          "aircraft_id1",
          "aircraft_id2"
        ],
        "decorators": [],
        "docstring": "\n    Compute current horizontal and vertical separation between two aircraft.\n\n    Args:\n        aircraft_id1: ID of first aircraft\n        aircraft_id2: ID of second aircraft\n\n    Returns:\n        Dictionary with separation distances:\n        - horizontal_nm: Horizontal separation in nautical miles\n        - vertical_ft: Vertical separation in feet\n        - total_3d_nm: Total 3D separation in nautical miles\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "get_all_aircraft_info",
          "haversine_distance",
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_determine_recommended_action",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 309,
        "line_end": 321,
        "args": [
          "self",
          "conflict",
          "_aircraft_data"
        ],
        "decorators": [],
        "docstring": "Determine the most appropriate action type for conflict resolution",
        "imports_used": [],
        "intra_repo_calls": [
          "PlanType"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "parse_resolution_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 490,
        "line_end": 597,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "\n        Extract BlueSky command from LLM response.\n\n        Args:\n            response_text: Raw LLM response text\n\n        Returns:\n            Parsed ResolutionResponse object or None if parsing fails\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ResolutionResponse"
        ],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "plot_cd_timeline",
        "file_path": "analysis\\visualisation.py",
        "line_start": 927,
        "line_end": 962,
        "args": [
          "df",
          "sim_id",
          "output_dir"
        ],
        "decorators": [],
        "docstring": "\n    Plot conflict detection timeline for a specific simulation.\n\n    Args:\n        df: DataFrame with simulation results\n        sim_id: Simulation ID to plot\n        output_dir: Output directory for plots\n        **kwargs: Additional arguments\n\n    Returns:\n        Path to generated plot file, or None if visualization failed\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "MonteCarloVisualizer"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "calc_separation_margin",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 567,
        "line_end": 622,
        "args": [
          "trajectories"
        ],
        "decorators": [],
        "docstring": "\n    Calculate horizontal and vertical separation margins from trajectories.\n\n    Args:\n        trajectories: List of aircraft trajectories with format:\n                     [{'aircraft_id': str, 'path': [{'lat': float, 'lon': float,\n                       'alt': float, 'time': float}]}]\n\n    Returns:\n        Dict with 'hz' (horizontal) and 'vt' (vertical) margins in nm and ft\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "SeparationStandard"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_clean_json_response",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 647,
        "line_end": 670,
        "args": [
          "self",
          "json_str"
        ],
        "decorators": [],
        "docstring": "Clean and repair common JSON formatting issues",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "get_assessment_history",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 398,
        "line_end": 400,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get history of conflict assessments",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 58,
        "line_end": 60,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize the analyzer.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "__post_init__",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 58,
        "line_end": 78,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ConflictExperience"
      },
      {
        "name": "_detect_conflicts",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 909,
        "line_end": 1035,
        "args": [
          "self",
          "scenario"
        ],
        "decorators": [],
        "docstring": "Perform conflict detection using multiple BlueSky methods for validation",
        "imports_used": [],
        "intra_repo_calls": [
          "EnhancedConflictDetector"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "quick_resolve_conflict",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1949,
        "line_end": 1981,
        "args": [
          "aircraft_1",
          "aircraft_2",
          "time_to_conflict",
          "engine"
        ],
        "decorators": [],
        "docstring": "\n    Quick conflict resolution with minimal setup.\n\n    Args:\n        aircraft_1: First aircraft data\n        aircraft_2: Second aircraft data\n        time_to_conflict: Time to conflict in seconds\n        engine: Optional engine instance\n\n    Returns:\n        ResolutionResponse or None\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "ResolutionResponse",
          "LLMPromptEngine"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_get_default_ranges",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 176,
        "line_end": 202,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Fallback ranges if YAML file is not available",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_plot_metric_distributions",
        "file_path": "analysis\\visualisation.py",
        "line_start": 226,
        "line_end": 333,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Create histograms and KDEs for key metrics.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "get_ensemble_statistics",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 605,
        "line_end": 645,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get statistics about ensemble performance",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "get_llm_client",
        "file_path": "llm_interface\\filter_sort.py",
        "line_start": 13,
        "line_end": 17,
        "args": [],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "LLMClient"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "generate_report",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 414,
        "line_end": 618,
        "args": [
          "self",
          "results_df",
          "aggregated_metrics",
          "output_file"
        ],
        "decorators": [],
        "docstring": "\n        Generate a comprehensive markdown report with all metrics and analysis.\n\n        Args:\n            results_df: DataFrame with simulation results\n            aggregated_metrics: Pre-computed metrics (if None, will compute from results_df)\n            output_file: Path to save the markdown report\n\n        Returns:\n            Path to the generated report file\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "analyze_monte_carlo_results",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 1304,
        "line_end": 1355,
        "args": [
          "results_file",
          "output_dir"
        ],
        "decorators": [],
        "docstring": "\n    Complete Monte Carlo analysis pipeline from results file to metrics and plots.\n\n    Args:\n        results_file: Path to results.json or results.csv file\n        output_dir: Directory for analysis outputs\n\n    Returns:\n        Dict with aggregated metrics and plot paths\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "MonteCarloResultsAnalyzer",
          "MonteCarloVisualizer"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "__init__",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 57,
        "line_end": 60,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "get_verification_history",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 336,
        "line_end": 338,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get history of all verification results",
        "imports_used": [],
        "intra_repo_calls": [
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "_calculate_total_uncertainty",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 352,
        "line_end": 371,
        "args": [
          "self",
          "environmental_conditions"
        ],
        "decorators": [],
        "docstring": "Calculate total uncertainty in the system",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "_plot_safety_margins",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1835,
        "line_end": 1906,
        "args": [
          "self",
          "df",
          "fig_size"
        ],
        "decorators": [],
        "docstring": "Plot safety margin distributions",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "__init__",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 42,
        "line_end": 51,
        "args": [
          "self",
          "prompt_engine"
        ],
        "decorators": [],
        "docstring": "\n        Initialize the enhanced hallucination detector.\n\n        Args:\n            prompt_engine: Optional LLMPromptEngine instance for sophisticated prompts\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "_extract_lessons",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 123,
        "line_end": 183,
        "args": [
          "self",
          "similar_experiences"
        ],
        "decorators": [],
        "docstring": "Extract actionable lessons from similar experiences",
        "imports_used": [],
        "intra_repo_calls": [
          "SimilarityResult"
        ],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "_calculate_confidence",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 455,
        "line_end": 475,
        "args": [
          "self",
          "method",
          "h_sep",
          "v_sep",
          "time_to_cpa"
        ],
        "decorators": [],
        "docstring": "Calculate confidence score for conflict detection",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "__init__",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 30,
        "line_end": 34,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "_generate_assessment",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 283,
        "line_end": 307,
        "args": [
          "self",
          "conflict",
          "aircraft_data"
        ],
        "decorators": [],
        "docstring": "Generate comprehensive conflict assessment",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "__init__",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1188,
        "line_end": 1189,
        "args": [
          "self",
          "generator"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioGenerator"
        ],
        "is_method": true,
        "class_name": "HorizontalCREnv"
      },
      {
        "name": "_test_network_connection",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 263,
        "line_end": 281,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Test network connection to BlueSky",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "store_experience",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 484,
        "line_end": 518,
        "args": [
          "self",
          "experience_data"
        ],
        "decorators": [],
        "docstring": "Simple interface to store experience data directly",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictExperience"
        ],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "store_experience",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 170,
        "line_end": 235,
        "args": [
          "self",
          "experience"
        ],
        "decorators": [],
        "docstring": "\n        Store a conflict experience in the vector store\n\n        Args:\n            experience: ConflictExperience object to store\n\n        Returns:\n            str: Experience ID if successful, empty string if failed\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictExperience"
        ],
        "is_method": true,
        "class_name": "VectorReplayStore"
      },
      {
        "name": "_get_aircraft_count_for_complexity",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 590,
        "line_end": 600,
        "args": [
          "self",
          "complexity_tier"
        ],
        "decorators": [],
        "docstring": "Get appropriate aircraft count for complexity level",
        "imports_used": [],
        "intra_repo_calls": [
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "step_simulation_real",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 521,
        "line_end": 565,
        "args": [
          "self",
          "minutes",
          "dtmult"
        ],
        "decorators": [],
        "docstring": "Step the real BlueSky simulation forward",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_initialize_knowledge_base",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 717,
        "line_end": 767,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize aviation knowledge base",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "RAGValidator"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 106,
        "line_end": 168,
        "args": [
          "self",
          "storage_dir"
        ],
        "decorators": [],
        "docstring": "\n        Initialize the replay store\n\n        Args:\n            storage_dir: Directory for Chroma persistence\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "VectorReplayStore"
      },
      {
        "name": "_resolve_conflicts",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1151,
        "line_end": 1224,
        "args": [
          "self",
          "conflicts",
          "scenario"
        ],
        "decorators": [],
        "docstring": "Generate LLM-based conflict resolutions",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_extract_lessons_learned",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 414,
        "line_end": 432,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Extract lessons learned from errors and low-confidence steps",
        "imports_used": [],
        "intra_repo_calls": [
          "StepType"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "plot_metrics_comparison",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 334,
        "line_end": 414,
        "args": [
          "llm_metrics",
          "baseline_metrics",
          "save_path"
        ],
        "decorators": [],
        "docstring": "Create comparison plots between LLM and baseline metrics.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "log_verification_step",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 197,
        "line_end": 221,
        "args": [
          "self",
          "verification"
        ],
        "decorators": [],
        "docstring": "Log a verification step",
        "imports_used": [],
        "intra_repo_calls": [
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "__init__",
        "file_path": "analysis\\visualisation.py",
        "line_start": 97,
        "line_end": 136,
        "args": [
          "self",
          "output_dir",
          "style",
          "dpi"
        ],
        "decorators": [],
        "docstring": "\n        Initialize the visualizer.\n\n        Args:\n            output_dir: Directory to save visualizations\n            style: Matplotlib style to use\n            dpi: Resolution for saved figures\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "_get_mock_aircraft_data",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 612,
        "line_end": 680,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Generate mock aircraft data when BlueSky unavailable",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_get_priority_timeout",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 608,
        "line_end": 615,
        "args": [
          "self",
          "priority"
        ],
        "decorators": [],
        "docstring": "Get timeout based on request priority",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_generate_scenario",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 558,
        "line_end": 588,
        "args": [
          "self",
          "scenario_type",
          "complexity_tier",
          "shift_level",
          "scenario_id"
        ],
        "decorators": [],
        "docstring": "Generate a scenario based on type and parameters",
        "imports_used": [],
        "intra_repo_calls": [
          "generate_vertical_scenario",
          "ScenarioType",
          "generate_sector_scenario",
          "generate_horizontal_scenario",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_is_unsafe_command",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 250,
        "line_end": 286,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Check if a command is potentially unsafe",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "search_experience_library",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 980,
        "line_end": 1052,
        "args": [
          "scenario_type",
          "similarity_threshold"
        ],
        "decorators": [],
        "docstring": "\n    Search the experience library for similar scenarios\n\n    Args:\n        scenario_type: Type of scenario to search for\n        similarity_threshold: Minimum similarity score for matches\n\n    Returns:\n        Dictionary containing matching experiences\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "detect_conflict_via_llm_with_prompts",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1150,
        "line_end": 1197,
        "args": [
          "self",
          "aircraft_states",
          "time_horizon",
          "cpa_data"
        ],
        "decorators": [],
        "docstring": "\n        Enhanced API for LLM-based conflict detection that returns prompt and response data.\n\n        Args:\n            aircraft_states: List of aircraft state dictionaries\n            time_horizon: Time horizon in minutes\n            cpa_data: Optional Closest Point of Approach data with additional context\n\n        Returns:\n            Dictionary with detection results including prompt and response\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_init_detection_patterns",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 53,
        "line_end": 88,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize detection patterns for various hallucination types",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "_calculate_bearing",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 711,
        "line_end": 727,
        "args": [
          "self",
          "lat1",
          "lon1",
          "lat2",
          "lon2"
        ],
        "decorators": [],
        "docstring": "Calculate bearing between two lat/lon points",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "get_recent_steps",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 280,
        "line_end": 286,
        "args": [
          "self",
          "count"
        ],
        "decorators": [],
        "docstring": "Get the most recent steps",
        "imports_used": [],
        "intra_repo_calls": [
          "ReasoningStep"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "__init__",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 255,
        "line_end": 303,
        "args": [
          "self",
          "config"
        ],
        "decorators": [],
        "docstring": "\n        Initialize the Monte Carlo benchmark runner.\n\n        Args:\n            config: Benchmark configuration. If None, uses defaults.\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "set_strict_mode",
          "DetectionComparison",
          "ScenarioResult",
          "ScenarioGenerator",
          "LLMPromptEngine",
          "BenchmarkConfiguration"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_avoid_vertical_conflicts_enhanced",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 616,
        "line_end": 655,
        "args": [
          "self",
          "aircraft_states",
          "climb_rates"
        ],
        "decorators": [],
        "docstring": "Enhanced vertical conflict avoidance ensuring >1000ft separation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_calculate_vertical_ground_truth",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 882,
        "line_end": 929,
        "args": [
          "self",
          "aircraft_states",
          "expect_conflicts"
        ],
        "decorators": [],
        "docstring": "Calculate ground truth conflicts for vertical scenarios",
        "imports_used": [],
        "intra_repo_calls": [
          "GroundTruthConflict"
        ],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_generate_bluesky_commands",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 578,
        "line_end": 675,
        "args": [
          "self",
          "aircraft_count",
          "aircraft_types",
          "positions",
          "speeds",
          "headings",
          "environmental_conditions",
          "force_conflicts",
          "ranges",
          "distribution_shift_tier"
        ],
        "decorators": [],
        "docstring": "Generate BlueSky commands for scenario setup (shift-aware)",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_check_heading_validity",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 235,
        "line_end": 251,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Check for invalid heading values",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "generate_comprehensive_report",
        "file_path": "analysis\\visualisation.py",
        "line_start": 138,
        "line_end": 179,
        "args": [
          "self",
          "data",
          "title"
        ],
        "decorators": [],
        "docstring": "\n        Generate a complete visualization report with all chart types.\n\n        Args:\n            data: DataFrame with Monte Carlo results\n            title: Report title\n\n        Returns:\n            Path to generated HTML report\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "validate_llm_conflicts",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 539,
        "line_end": 566,
        "args": [
          "self",
          "llm_conflicts"
        ],
        "decorators": [],
        "docstring": "\n        Validate LLM-detected conflicts against BlueSky ground truth\n        Returns list of (aircraft1, aircraft2, confidence) for validated conflicts\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_detect_with_enhanced_analysis",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 223,
        "line_end": 238,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Detect conflicts using enhanced geometric analysis",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_generate_conflict_commands",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 677,
        "line_end": 709,
        "args": [
          "self",
          "aircraft_count"
        ],
        "decorators": [],
        "docstring": "Generate commands to create conflict situations",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_determine_maneuver_type_fast",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1919,
        "line_end": 1930,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Fast maneuver type determination",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_generate_combined_summary",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1671,
        "line_end": 1706,
        "args": [
          "self",
          "df"
        ],
        "decorators": [],
        "docstring": "Generate summary across all combinations of scenario type, complexity, and shift",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_initialize_models",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 62,
        "line_end": 167,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize model ensemble configuration",
        "imports_used": [],
        "intra_repo_calls": [
          "ModelConfig",
          "ModelRole"
        ],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "_assess_severity",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 242,
        "line_end": 259,
        "args": [
          "self",
          "separation"
        ],
        "decorators": [],
        "docstring": "Assess conflict severity based on separation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "start_new_session",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 346,
        "line_end": 367,
        "args": [
          "self",
          "session_id"
        ],
        "decorators": [],
        "docstring": "\n        Start a new reasoning session\n\n        Args:\n            session_id: Optional custom session ID\n\n        Returns:\n            New session ID\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "get_all_aircraft_info",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 823,
        "line_end": 846,
        "args": [],
        "decorators": [],
        "docstring": "\n    Get information about all aircraft in the simulation\n\n    Returns:\n        Dictionary containing aircraft information\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "__init__",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 125,
        "line_end": 150,
        "args": [
          "self",
          "ranges_file",
          "distribution_shift_file",
          "ranges_dict"
        ],
        "decorators": [],
        "docstring": "Initialize generator with range configuration",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "generate_scenario",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 951,
        "line_end": 963,
        "args": [
          "complexity_tier",
          "force_conflicts",
          "distribution_shift_tier"
        ],
        "decorators": [],
        "docstring": "Generate a single scenario - convenience function",
        "imports_used": [],
        "intra_repo_calls": [
          "ComplexityTier",
          "ScenarioConfiguration",
          "BlueSkyScenarioGenerator"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_fix_common_json_issues",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 539,
        "line_end": 557,
        "args": [
          "self",
          "json_str"
        ],
        "decorators": [],
        "docstring": "Fix common JSON formatting issues",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "compute_metrics",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 37,
        "line_end": 144,
        "args": [
          "log_file"
        ],
        "decorators": [],
        "docstring": "Compute hallucination and performance metrics from simulation logs.",
        "imports_used": [],
        "intra_repo_calls": [
          "create_empty_metrics",
          "analyze_hallucinations_in_log"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_calculate_priority",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 379,
        "line_end": 396,
        "args": [
          "self",
          "assessment"
        ],
        "decorators": [],
        "docstring": "Calculate plan priority (1-10, higher is more urgent)",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "aggregate_thesis_metrics",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 271,
        "line_end": 330,
        "args": [
          "results_dir"
        ],
        "decorators": [],
        "docstring": "Aggregate metrics from multiple test result files for thesis analysis.",
        "imports_used": [],
        "intra_repo_calls": [
          "create_empty_metrics",
          "compute_metrics"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_determine_maneuver_type",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1498,
        "line_end": 1513,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Determine maneuver type from BlueSky command",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "create_chat_messages",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 60,
        "line_end": 91,
        "args": [
          "self",
          "system_prompt",
          "user_prompt",
          "context"
        ],
        "decorators": [],
        "docstring": "\n        Create properly formatted Ollama chat messages.\n\n        Args:\n            system_prompt: System instructions\n            user_prompt: User query\n            context: Optional conversation context\n\n        Returns:\n            List of message dictionaries for Ollama\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ChatMessage"
        ],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "continue_monitoring",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 875,
        "line_end": 900,
        "args": [],
        "decorators": [],
        "docstring": "\n    Continue monitoring aircraft without taking action\n\n    Returns:\n        Status information about monitoring continuation\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_check_impossible_maneuvers",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 276,
        "line_end": 307,
        "args": [
          "self",
          "response_text",
          "context"
        ],
        "decorators": [],
        "docstring": "Check for physically impossible maneuvers",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "_cross_validate_conflicts",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 477,
        "line_end": 492,
        "args": [
          "self",
          "all_conflicts"
        ],
        "decorators": [],
        "docstring": "Cross-validate conflicts detected by multiple methods",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "send_command",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 903,
        "line_end": 977,
        "args": [
          "command"
        ],
        "decorators": [],
        "docstring": "\n    Send a command to the BlueSky simulator\n\n    Args:\n        command: BlueSky command string (e.g., \"ALT AAL123 FL350\")\n\n    Returns:\n        Command execution result\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "create_visualization_summary",
        "file_path": "analysis\\visualisation.py",
        "line_start": 1017,
        "line_end": 1087,
        "args": [
          "output_dir"
        ],
        "decorators": [],
        "docstring": "\n    Create a comprehensive visualization summary.\n\n    Args:\n        output_dir: Output directory for summary\n        **kwargs: Additional arguments\n\n    Returns:\n        Path to generated summary file, or None if creation failed\n    ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "compute_false_positive_negative_rates",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 115,
        "line_end": 164,
        "args": [
          "self",
          "results_df"
        ],
        "decorators": [],
        "docstring": "\n        Compute false positive and false negative rates from results.\n\n        Args:\n            results_df: DataFrame with columns 'predicted_conflicts', 'actual_conflicts'\n\n        Returns:\n            Dict with 'false_positive_rate' and 'false_negative_rate'\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "generate_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1208,
        "line_end": 1216,
        "args": [
          "self",
          "n_aircraft",
          "conflict"
        ],
        "decorators": [],
        "docstring": "Generate vertical conflict scenario",
        "imports_used": [],
        "intra_repo_calls": [
          "Scenario"
        ],
        "is_method": true,
        "class_name": "VerticalCREnv"
      },
      {
        "name": "record_resolution_outcome",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 365,
        "line_end": 409,
        "args": [
          "self",
          "scenario_context",
          "conflict_geometry",
          "environmental_conditions",
          "llm_decision",
          "baseline_decision",
          "actual_outcome",
          "safety_metrics",
          "hallucination_result",
          "controller_override",
          "lessons_learned"
        ],
        "decorators": [],
        "docstring": "Record the outcome of a conflict resolution for future learning",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictExperience"
        ],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "_predict_position",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 224,
        "line_end": 259,
        "args": [
          "self",
          "position",
          "velocity",
          "time"
        ],
        "decorators": [],
        "docstring": "Predict future position based on current velocity",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "_get_default_config",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 128,
        "line_end": 143,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get default configuration",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyConfig"
      },
      {
        "name": "_calculate_scenario_metrics",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1419,
        "line_end": 1460,
        "args": [
          "self",
          "ground_truth",
          "detected",
          "resolutions",
          "verification"
        ],
        "decorators": [],
        "docstring": "Calculate performance metrics for scenario",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_check_command_success_rate",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 177,
        "line_end": 199,
        "args": [
          "self",
          "execution",
          "verification"
        ],
        "decorators": [],
        "docstring": "Check command success rate",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult",
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 166,
        "line_end": 175,
        "args": [
          "self",
          "strict_mode"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_calculate_uncertainty_metrics",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 556,
        "line_end": 587,
        "args": [
          "self",
          "responses"
        ],
        "decorators": [],
        "docstring": "Calculate uncertainty metrics from ensemble responses",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "_execute_chat_request",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 266,
        "line_end": 302,
        "args": [
          "self",
          "messages",
          "timeout",
          "expect_json"
        ],
        "decorators": [],
        "docstring": "Execute the actual chat request to Ollama",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_simulate_step",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 722,
        "line_end": 733,
        "args": [
          "self",
          "minutes",
          "dtmult"
        ],
        "decorators": [],
        "docstring": "Simulate stepping when BlueSky unavailable",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_generate_learning_insights",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 428,
        "line_end": 482,
        "args": [
          "self",
          "stats",
          "patterns"
        ],
        "decorators": [],
        "docstring": "Generate insights from experience data",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "_is_valid_response",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 288,
        "line_end": 295,
        "args": [
          "self",
          "response"
        ],
        "decorators": [],
        "docstring": "Check if a command response is valid",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "get_command_log",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 888,
        "line_end": 890,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get the complete command log for validation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "get_llm_stats",
        "file_path": "llm_interface\\filter_sort.py",
        "line_start": 20,
        "line_end": 27,
        "args": [],
        "decorators": [],
        "docstring": "Get LLM timing statistics.",
        "imports_used": [],
        "intra_repo_calls": [
          "get_llm_client"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_mock_execution",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 788,
        "line_end": 815,
        "args": [
          "self",
          "scenario"
        ],
        "decorators": [],
        "docstring": "Mock execution when BlueSky is not available",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioConfiguration",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_analyze_safety_flags",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 420,
        "line_end": 469,
        "args": [
          "self",
          "responses"
        ],
        "decorators": [],
        "docstring": "Analyze responses for safety flags and concerns",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "_parse_function_call_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1253,
        "line_end": 1274,
        "args": [
          "self",
          "response_dict"
        ],
        "decorators": [],
        "docstring": "Parse function call response into ResolutionResponse",
        "imports_used": [],
        "intra_repo_calls": [
          "ResolutionResponse"
        ],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "aircraft_list",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 62,
        "line_end": 86,
        "args": [
          "self"
        ],
        "decorators": [
          "property"
        ],
        "docstring": "Generate aircraft_list for backward compatibility",
        "imports_used": [],
        "intra_repo_calls": [
          "aircraft_list"
        ],
        "is_method": true,
        "class_name": "ScenarioConfiguration"
      },
      {
        "name": "_generate_distribution_analysis",
        "file_path": "analysis\\visualisation.py",
        "line_start": 181,
        "line_end": 195,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Generate distribution analysis visualizations.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "_avoid_vertical_conflicts",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 570,
        "line_end": 583,
        "args": [
          "self",
          "aircraft_states"
        ],
        "decorators": [],
        "docstring": "Ensure safe vertical separation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_get_aircraft_pair_key",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 535,
        "line_end": 537,
        "args": [
          "self",
          "ac1",
          "ac2"
        ],
        "decorators": [],
        "docstring": "Get consistent key for aircraft pair (sorted order)",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "__init__",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 31,
        "line_end": 58,
        "args": [
          "self",
          "model",
          "max_retries",
          "timeout",
          "enable_streaming",
          "enable_caching",
          "cache_size",
          "enable_optimized_prompts"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_create_empty_aggregated_metrics",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 971,
        "line_end": 992,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Create empty metrics structure for error cases.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "format_conflict_detection_prompt_optimized",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1720,
        "line_end": 1750,
        "args": [
          "self",
          "aircraft_states",
          "time_horizon"
        ],
        "decorators": [],
        "docstring": "\n        Create optimized conflict detection prompt.\n\n        Args:\n            aircraft_states: List of aircraft data\n            time_horizon: Detection time horizon in minutes\n\n        Returns:\n            Tuple of (system_prompt, user_prompt)\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_save_model_metadata",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 128,
        "line_end": 161,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Save metadata about the merged model.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "__post_init__",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 237,
        "line_end": 244,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Set defaults for mutable fields",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioResult"
      },
      {
        "name": "get_conflict_detection_system_prompt",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 637,
        "line_end": 652,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Concise system prompt for conflict detection",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "validate_response",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 481,
        "line_end": 491,
        "args": [
          "self",
          "response",
          "expected_keys"
        ],
        "decorators": [],
        "docstring": "Validate LLM response format and content.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_generate_executive_summary",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 620,
        "line_end": 687,
        "args": [
          "self",
          "metrics"
        ],
        "decorators": [],
        "docstring": "Generate executive summary section.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "generate_vertical_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1250,
        "line_end": 1255,
        "args": [
          "n_aircraft",
          "conflict"
        ],
        "decorators": [],
        "docstring": "Generate vertical scenario - convenience function",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioGenerator",
          "Scenario"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_simulate_command_execution",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 707,
        "line_end": 720,
        "args": [
          "self",
          "command"
        ],
        "decorators": [],
        "docstring": "Simulate command execution when BlueSky unavailable",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "plot_cr_flowchart",
        "file_path": "analysis\\visualisation.py",
        "line_start": 965,
        "line_end": 986,
        "args": [
          "sim_id",
          "tier",
          "output_dir"
        ],
        "decorators": [],
        "docstring": "\n    Plot conflict resolution flowchart.\n\n    Args:\n        sim_id: Simulation ID\n        tier: Distribution shift tier\n        output_dir: Output directory for plots\n        **kwargs: Additional arguments\n\n    Returns:\n        Path to generated flowchart file, or None if visualization failed\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "MonteCarloVisualizer"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "get_conflict_info",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 849,
        "line_end": 872,
        "args": [],
        "decorators": [],
        "docstring": "\n    Get information about current conflicts in the simulation\n\n    Returns:\n        Dictionary containing conflict information\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_process_function_calls",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 329,
        "line_end": 362,
        "args": [
          "self",
          "content"
        ],
        "decorators": [],
        "docstring": "Process function calls from LLM response",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "get_airspace_info",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1112,
        "line_end": 1170,
        "args": [],
        "decorators": [],
        "docstring": "\n    Get information about current airspace restrictions and constraints\n\n    Returns:\n        Airspace information dictionary\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "assess_resolution_safety",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1199,
        "line_end": 1237,
        "args": [
          "self",
          "command",
          "conflict_info"
        ],
        "decorators": [],
        "docstring": "\n        Use LLM to assess the safety of a proposed resolution.\n\n        Args:\n            command: Proposed BlueSky command\n            conflict_info: Original conflict information\n\n        Returns:\n            Safety assessment dictionary\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "generate_sector_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 417,
        "line_end": 490,
        "args": [
          "self",
          "complexity",
          "shift_level",
          "force_conflicts"
        ],
        "decorators": [],
        "docstring": "\n        Generate realistic sector scenario.\n\n        Uses full Monte Carlo generation for organic sector scenarios.\n\n        Args:\n            complexity: Scenario complexity tier\n            shift_level: Distribution shift level\n            force_conflicts: Whether to force conflicts (False for realistic scenarios)\n\n        Returns:\n            Sector scenario with ground truth\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioType",
          "ComplexityTier",
          "Scenario"
        ],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "generate_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 144,
        "line_end": 162,
        "args": [
          "self",
          "scenario_type"
        ],
        "decorators": [],
        "docstring": "\n        Dispatcher method to generate scenarios by type.\n\n        Args:\n            scenario_type: Type of scenario to generate\n            **kwargs: Type-specific arguments\n\n        Returns:\n            Generated scenario with ground truth\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioType",
          "Scenario"
        ],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_analyze_aircraft_pair",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 240,
        "line_end": 319,
        "args": [
          "self",
          "ac1_idx",
          "ac2_idx",
          "method"
        ],
        "decorators": [],
        "docstring": "\n        Analyze specific aircraft pair for conflicts with CPA calculation\n        Implements the 300s time horizon check as requested\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_format_grouped_success_table",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 771,
        "line_end": 797,
        "args": [
          "self",
          "grouped_df"
        ],
        "decorators": [],
        "docstring": "Format grouped success rates as a markdown table.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_generate_summary",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1519,
        "line_end": 1612,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Generate comprehensive summary statistics from all results",
        "imports_used": [],
        "intra_repo_calls": [
          "MonteCarloResultsAnalyzer"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_detect_with_enhanced_analysis",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 223,
        "line_end": 238,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Detect conflicts using enhanced geometric analysis",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_plot_distribution_shift_impact",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 2028,
        "line_end": 2109,
        "args": [
          "self",
          "df",
          "fig_size"
        ],
        "decorators": [],
        "docstring": "Plot impact of distribution shift on performance",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_prioritize_conflicts",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 267,
        "line_end": 281,
        "args": [
          "self",
          "conflicts"
        ],
        "decorators": [],
        "docstring": "Select the most critical conflict to address first",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "get_weather_info",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1055,
        "line_end": 1109,
        "args": [
          "lat",
          "lon"
        ],
        "decorators": [],
        "docstring": "\n    Get weather information for specified location or current area\n\n    Args:\n        lat: Latitude (optional)\n        lon: Longitude (optional)\n\n    Returns:\n        Weather information dictionary\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "export_detailed_metrics",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 516,
        "line_end": 564,
        "args": [
          "self",
          "filepath"
        ],
        "decorators": [],
        "docstring": "Export detailed metrics to JSON file",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "SafetyMetricsAggregator"
      },
      {
        "name": "get_minimum_separation",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1354,
        "line_end": 1369,
        "args": [],
        "decorators": [],
        "docstring": "\n    Get the current minimum separation standards.\n\n    Returns:\n        Dictionary with minimum separation requirements\n    ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "create_distribution_shift_plots",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 1059,
        "line_end": 1096,
        "args": [
          "self",
          "aggregated_metrics",
          "output_dir"
        ],
        "decorators": [],
        "docstring": "\n        Create scatter plots showing performance differences under distribution shifts.\n\n        Args:\n            aggregated_metrics: Output from aggregate_monte_carlo_metrics()\n            output_dir: Directory to save plots\n\n        Returns:\n            List of created plot file paths\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 1000,
        "line_end": 1007,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize the visualizer.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "_find_config_file",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 45,
        "line_end": 63,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Find the BlueSky configuration file",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyConfig"
      },
      {
        "name": "_check_response_validity",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 224,
        "line_end": 248,
        "args": [
          "self",
          "execution",
          "verification"
        ],
        "decorators": [],
        "docstring": "Check validity of command responses",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult",
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "export_session_data",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 434,
        "line_end": 448,
        "args": [
          "self",
          "format"
        ],
        "decorators": [],
        "docstring": "\n        Export session data in specified format\n\n        Args:\n            format: Export format ('json', 'dict')\n\n        Returns:\n            Session data in requested format\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "_detect_with_statebased",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 171,
        "line_end": 221,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Detect conflicts using BlueSky STATEBASED method",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "get_performance_stats",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1933,
        "line_end": 1941,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get engine performance statistics",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_detect_with_swarm",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 121,
        "line_end": 169,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Detect conflicts using BlueSky SWARM method",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_plot_detection_performance",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1761,
        "line_end": 1833,
        "args": [
          "self",
          "df",
          "fig_size"
        ],
        "decorators": [],
        "docstring": "Plot detection performance metrics",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_create_default_safety_margin",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 407,
        "line_end": 417,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Create default safety margin for error cases",
        "imports_used": [],
        "intra_repo_calls": [
          "SafetyMargin"
        ],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "get_execution_metrics",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 289,
        "line_end": 315,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get overall execution performance metrics",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionStatus"
        ],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 65,
        "line_end": 78,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictDetectionMethod"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "generate_sector_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1258,
        "line_end": 1268,
        "args": [
          "complexity",
          "shift_level",
          "force_conflicts"
        ],
        "decorators": [],
        "docstring": "Generate sector scenario - convenience function",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioGenerator",
          "ComplexityTier",
          "Scenario"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "compute_efficiency_penalties",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 369,
        "line_end": 412,
        "args": [
          "self",
          "results_df"
        ],
        "decorators": [],
        "docstring": "\n        Compute efficiency penalties from trajectory comparisons.\n\n        Args:\n            results_df: DataFrame with planned and executed trajectory data\n\n        Returns:\n            Dict with efficiency penalty statistics\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "calc_efficiency_penalty"
        ],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "chat_with_function_calling",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 388,
        "line_end": 448,
        "args": [
          "self",
          "messages",
          "max_function_calls"
        ],
        "decorators": [],
        "docstring": "\n        Extended chat interface with function calling support\n\n        Args:\n            messages: List of message dictionaries with 'role' and 'content'\n            max_function_calls: Maximum number of function calls allowed in a single chat\n\n        Returns:\n            Final response with function call history\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_plot_performance_by_type",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1974,
        "line_end": 2026,
        "args": [
          "self",
          "df",
          "fig_size"
        ],
        "decorators": [],
        "docstring": "Plot performance metrics by scenario type",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_validate_detector_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 850,
        "line_end": 886,
        "args": [
          "self",
          "json_data"
        ],
        "decorators": [],
        "docstring": "Validate detector response for completeness and correctness",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "convert_to_gguf",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 163,
        "line_end": 236,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Convert merged model to GGUF format.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "log_error_step",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 223,
        "line_end": 236,
        "args": [
          "self",
          "error_msg",
          "error_data"
        ],
        "decorators": [],
        "docstring": "Log an error step",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "_calculate_vertical_margin",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 313,
        "line_end": 319,
        "args": [
          "self",
          "geometry"
        ],
        "decorators": [],
        "docstring": "Calculate vertical separation margin",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictGeometry"
        ],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "quick_detect_conflicts",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1984,
        "line_end": 2000,
        "args": [
          "aircraft_states",
          "engine"
        ],
        "decorators": [],
        "docstring": "\n    Quick conflict detection with minimal setup.\n\n    Args:\n        aircraft_states: List of aircraft data\n        engine: Optional engine instance\n\n    Returns:\n        Detection results dictionary or None\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "LLMPromptEngine"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_plot_performance_evolution",
        "file_path": "analysis\\visualisation.py",
        "line_start": 760,
        "line_end": 819,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Plot performance metrics evolution.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "_execute_scenario_pipeline",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 681,
        "line_end": 835,
        "args": [
          "self",
          "scenario",
          "scenario_id"
        ],
        "decorators": [],
        "docstring": "\n        Execute the three-stage pipeline for a single scenario.\n\n        Args:\n            scenario: Generated scenario object\n            scenario_id: Unique scenario identifier\n\n        Returns:\n            ScenarioResult with complete execution data\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioResult",
          "ScenarioType",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "validate_llm_conflicts",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 539,
        "line_end": 566,
        "args": [
          "self",
          "llm_conflicts"
        ],
        "decorators": [],
        "docstring": "\n        Validate LLM-detected conflicts against BlueSky ground truth\n        Returns list of (aircraft1, aircraft2, confidence) for validated conflicts\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_load_scenario_commands",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 846,
        "line_end": 885,
        "args": [
          "self",
          "scenario"
        ],
        "decorators": [],
        "docstring": "Load scenario commands into BlueSky",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "reset_simulation_real",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 567,
        "line_end": 610,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Reset the real BlueSky simulation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "check_separation_violation",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1372,
        "line_end": 1432,
        "args": [
          "aircraft_id1",
          "aircraft_id2"
        ],
        "decorators": [],
        "docstring": "\n    Check if two aircraft are violating separation standards.\n\n    Args:\n        aircraft_id1: ID of first aircraft\n        aircraft_id2: ID of second aircraft\n\n    Returns:\n        Dictionary with violation status and details\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "get_minimum_separation",
          "get_distance"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_plot_shift_comparisons",
        "file_path": "analysis\\visualisation.py",
        "line_start": 335,
        "line_end": 408,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Create side-by-side density comparisons for distribution shifts.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "_setup_output_directory",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 305,
        "line_end": 319,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Create output directory structure",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "calc_efficiency_penalty",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 625,
        "line_end": 668,
        "args": [
          "planned_path",
          "executed_path"
        ],
        "decorators": [],
        "docstring": "\n    Calculate efficiency penalty as extra distance traveled due to conflict resolution.\n\n    Args:\n        planned_path: Original planned trajectory points\n                     [{'lat': float, 'lon': float, 'alt': float, 'time': float}]\n        executed_path: Actual executed trajectory points (same format)\n\n    Returns:\n        Extra distance in nautical miles\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "calculate_path_distance"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_extract_json_from_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 980,
        "line_end": 986,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Extract JSON object from response text",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_calculate_sector_ground_truth",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 931,
        "line_end": 976,
        "args": [
          "self",
          "aircraft_states",
          "base_scenario"
        ],
        "decorators": [],
        "docstring": "Calculate ground truth conflicts for sector scenarios using trajectory analysis",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioConfiguration",
          "GroundTruthConflict"
        ],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "calc_fp_fn",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 199,
        "line_end": 230,
        "args": [
          "pred_conflicts",
          "gt_conflicts"
        ],
        "decorators": [],
        "docstring": "Calculate false positive and false negative rates.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_mock_conflict_detection",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 568,
        "line_end": 586,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Mock conflict detection when BlueSky is not available",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "generate_monte_carlo_scenarios",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 966,
        "line_end": 977,
        "args": [
          "count",
          "complexity_distribution",
          "distribution_shift_distribution"
        ],
        "decorators": [],
        "docstring": "Generate multiple scenarios for Monte Carlo testing - convenience function",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioConfiguration",
          "BlueSkyScenarioGenerator"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "retrieve_experience",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 237,
        "line_end": 360,
        "args": [
          "self",
          "conflict_desc",
          "conflict_type",
          "num_ac",
          "k"
        ],
        "decorators": [],
        "docstring": "\n        Retrieve similar experiences using metadata filtering + vector search\n\n        Args:\n            conflict_desc: Description of the conflict to search for\n            conflict_type: Type of conflict to filter by\n            num_ac: Number of aircraft to filter by\n            k: Number of results to return\n\n        Returns:\n            List of experience documents in score-ascending order\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "VectorReplayStore"
      },
      {
        "name": "_assess_efficiency_performance",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 761,
        "line_end": 769,
        "args": [
          "self",
          "efficiency"
        ],
        "decorators": [],
        "docstring": "Assess efficiency performance.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_format_conflict_for_llm",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1255,
        "line_end": 1285,
        "args": [
          "self",
          "conflict",
          "scenario"
        ],
        "decorators": [],
        "docstring": "Format conflict data for LLM prompt engine",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_conflicts_to_set",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 166,
        "line_end": 181,
        "args": [
          "self",
          "conflicts"
        ],
        "decorators": [],
        "docstring": "Convert conflict list to set of aircraft pairs.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 23,
        "line_end": 33,
        "args": [
          "self",
          "replay_store"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "VectorReplayStore",
          "EnhancedHallucinationDetector",
          "SafetyMarginQuantifier"
        ],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "_add_environmental_commands",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 683,
        "line_end": 701,
        "args": [
          "self",
          "env_conditions"
        ],
        "decorators": [],
        "docstring": "Add environmental condition commands",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "detect_conflicts_comprehensive",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 80,
        "line_end": 119,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "\n        Detect conflicts using all available methods and cross-validate results\n\n        Returns:\n            List of validated conflict data\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_parse_time_values",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1543,
        "line_end": 1553,
        "args": [
          "self",
          "time_text"
        ],
        "decorators": [],
        "docstring": "Parse time values from text",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "environmental",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 89,
        "line_end": 115,
        "args": [
          "self"
        ],
        "decorators": [
          "property"
        ],
        "docstring": "Generate environmental data for backward compatibility",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioConfiguration"
      },
      {
        "name": "_create_cache_key",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 589,
        "line_end": 594,
        "args": [
          "self",
          "user_prompt",
          "system_prompt"
        ],
        "decorators": [],
        "docstring": "Create cache key from prompts",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "run",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 411,
        "line_end": 453,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "\n        Execute the complete Monte Carlo benchmark.\n\n        Returns:\n            Summary statistics and results overview\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_detect_proximity_conflicts",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 178,
        "line_end": 213,
        "args": [
          "self",
          "aircraft_data"
        ],
        "decorators": [],
        "docstring": "Detect proximity-based conflicts between aircraft",
        "imports_used": [],
        "intra_repo_calls": [
          "aircraft_list"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "_create_horizontal_conflicts",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 492,
        "line_end": 522,
        "args": [
          "self",
          "aircraft_states"
        ],
        "decorators": [],
        "docstring": "Create heading adjustments to generate horizontal conflicts",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_plot_tornado_sensitivity",
        "file_path": "analysis\\visualisation.py",
        "line_start": 821,
        "line_end": 924,
        "args": [
          "self",
          "data"
        ],
        "decorators": [],
        "docstring": "Create tornado chart for sensitivity analysis.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "log_assessment_step",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 127,
        "line_end": 146,
        "args": [
          "self",
          "assessment"
        ],
        "decorators": [],
        "docstring": "Log a conflict assessment step",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "_generate_visualizations",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1731,
        "line_end": 1759,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Generate comprehensive visualizations of results",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "__post_init__",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 138,
        "line_end": 168,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Set defaults for mutable fields",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioType",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "BenchmarkConfiguration"
      },
      {
        "name": "compute_success_rates_by_group",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 233,
        "line_end": 313,
        "args": [
          "self",
          "results_df",
          "group_cols"
        ],
        "decorators": [],
        "docstring": "\n        Compute success rates grouped by specified columns.\n\n        Args:\n            results_df: DataFrame with columns including 'success' and the grouping columns\n            group_cols: List of column names to group by (e.g. ['scenario_type', 'complexity_tier', 'distribution_shift'])\n\n        Returns:\n            Multi-index DataFrame of success rates grouped by specified columns\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "_calculate_cpa",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 321,
        "line_end": 414,
        "args": [
          "self",
          "lat1",
          "lon1",
          "alt1",
          "hdg1",
          "spd1",
          "vs1",
          "lat2",
          "lon2",
          "alt2",
          "hdg2",
          "spd2",
          "vs2"
        ],
        "decorators": [],
        "docstring": "\n        Calculate Closest Point of Approach (CPA) for two aircraft\n\n        Returns:\n            Tuple of (time_to_cpa, distance_at_cpa, min_horizontal_sep, min_vertical_sep)\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "get_safe_default_resolution",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 578,
        "line_end": 587,
        "args": [
          "self",
          "scenario_type"
        ],
        "decorators": [],
        "docstring": "Provide a safe default resolution when LLM fails",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "log_execution_step",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 172,
        "line_end": 195,
        "args": [
          "self",
          "execution"
        ],
        "decorators": [],
        "docstring": "Log a plan execution step",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "sample_from_range",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 204,
        "line_end": 218,
        "args": [
          "self",
          "range_spec"
        ],
        "decorators": [],
        "docstring": "Sample a value from a range specification",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_haversine_distance",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1242,
        "line_end": 1271,
        "args": [
          "lat1",
          "lon1",
          "lat2",
          "lon2"
        ],
        "decorators": [],
        "docstring": "\n    Calculate the great circle distance between two points on Earth in nautical miles.\n\n    Args:\n        lat1, lon1: Latitude and longitude of first point in degrees\n        lat2, lon2: Latitude and longitude of second point in degrees\n\n    Returns:\n        Distance in nautical miles\n    ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_create_safety_margins_chart",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 1191,
        "line_end": 1235,
        "args": [
          "self",
          "margins_data",
          "save_path"
        ],
        "decorators": [],
        "docstring": "Create bar chart of safety margins.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      },
      {
        "name": "count_interventions",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 671,
        "line_end": 715,
        "args": [
          "commands"
        ],
        "decorators": [],
        "docstring": "\n    Count the number of ATC interventions in a command sequence.\n\n    Args:\n        commands: List of ATC commands with format:\n                 [{'type': str, 'aircraft_id': str, 'timestamp': float, ...}]\n\n    Returns:\n        Number of intervention commands\n    ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_generate_environmental_conditions",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 549,
        "line_end": 576,
        "args": [
          "self",
          "ranges"
        ],
        "decorators": [],
        "docstring": "Generate environmental conditions from ranges (shift-aware)",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_create_valid_response_structure",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 672,
        "line_end": 682,
        "args": [
          "self",
          "raw_content"
        ],
        "decorators": [],
        "docstring": "Create a valid response structure from failed JSON parsing",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "OllamaEnsembleClient"
      },
      {
        "name": "generate_scenario",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 393,
        "line_end": 547,
        "args": [
          "self",
          "complexity_tier",
          "force_conflicts",
          "airspace_region",
          "distribution_shift_tier"
        ],
        "decorators": [],
        "docstring": "\n        Generate a complete scenario using BlueSky commands.\n\n        Args:\n            complexity_tier: Scenario complexity level\n            force_conflicts: Whether to force conflict situations\n            airspace_region: Specific airspace region to use\n            distribution_shift_tier: Distribution shift tier to apply\n                                   ('in_distribution', 'moderate_shift', 'extreme_shift')\n\n        Returns:\n            ScenarioConfiguration with all parameters and BlueSky commands\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioConfiguration",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "format_conflict_resolution_prompt_optimized",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1676,
        "line_end": 1718,
        "args": [
          "self",
          "conflict_info"
        ],
        "decorators": [],
        "docstring": "\n        Create optimized conflict resolution prompt (system + user).\n\n        Args:\n            conflict_info: Conflict scenario data\n\n        Returns:\n            Tuple of (system_prompt, user_prompt)\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_are_headings_convergent",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1124,
        "line_end": 1147,
        "args": [
          "self",
          "lat1",
          "lon1",
          "hdg1",
          "lat2",
          "lon2",
          "hdg2"
        ],
        "decorators": [],
        "docstring": "Check if two aircraft headings are convergent",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_get_fallback_conflict_prompt",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1241,
        "line_end": 1251,
        "args": [
          "self",
          "conflict_info"
        ],
        "decorators": [],
        "docstring": "Generate a simple fallback prompt when main formatting fails",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "analyze_hallucinations_in_log",
        "file_path": "llm_atc\\metrics\\__init__.py",
        "line_start": 32,
        "line_end": 34,
        "args": [
          "_log_file"
        ],
        "decorators": [],
        "docstring": "Analyze hallucinations in log file - simplified implementation",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "get_history",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 251,
        "line_end": 267,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "\n        Get complete history of the current session\n\n        Returns:\n            Dictionary containing session history and steps\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "calculate_path_distance",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 641,
        "line_end": 660,
        "args": [
          "path"
        ],
        "decorators": [],
        "docstring": "Calculate total distance of a path in nautical miles",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 41,
        "line_end": 43,
        "args": [
          "self",
          "config_path"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyConfig"
      },
      {
        "name": "calculate_safety_margins",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 88,
        "line_end": 153,
        "args": [
          "self",
          "conflict_geometry",
          "resolution_maneuver",
          "environmental_conditions"
        ],
        "decorators": [],
        "docstring": "\n        Calculate comprehensive safety margins for a conflict resolution\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "SafetyMargin",
          "ConflictGeometry"
        ],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "_load_distribution_shift_config",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 163,
        "line_end": 174,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Load distribution shift configuration from YAML",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_check_altitude_validity",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 213,
        "line_end": 233,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Check for invalid altitude values",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "log_monitoring_step",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 238,
        "line_end": 249,
        "args": [
          "self",
          "monitoring_data"
        ],
        "decorators": [],
        "docstring": "Log a monitoring step",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "set_session_metadata",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 450,
        "line_end": 452,
        "args": [
          "self",
          "metadata"
        ],
        "decorators": [],
        "docstring": "Set metadata for the current session",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "__init__",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 64,
        "line_end": 86,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "__init__",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 106,
        "line_end": 142,
        "args": [
          "self",
          "ranges_file",
          "distribution_shift_file"
        ],
        "decorators": [],
        "docstring": "\n        Initialize scenario generator.\n\n        Args:\n            ranges_file: Path to scenario ranges YAML\n            distribution_shift_file: Path to distribution shift config YAML\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyScenarioGenerator"
        ],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_calculate_bearing",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1086,
        "line_end": 1102,
        "args": [
          "self",
          "lat1",
          "lon1",
          "lat2",
          "lon2"
        ],
        "decorators": [],
        "docstring": "Calculate bearing between two points",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "ask",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 93,
        "line_end": 181,
        "args": [
          "self",
          "prompt",
          "expect_json",
          "enable_function_calls",
          "system_prompt",
          "priority"
        ],
        "decorators": [],
        "docstring": "Ask the LLM a question with retry logic and error handling - OPTIMIZED VERSION.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_initialize_bluesky",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 177,
        "line_end": 215,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize BlueSky simulator",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_get_aircraft_pair_key",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 535,
        "line_end": 537,
        "args": [
          "self",
          "ac1",
          "ac2"
        ],
        "decorators": [],
        "docstring": "Get consistent key for aircraft pair (sorted order)",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "_check_safety_compliance",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 201,
        "line_end": 222,
        "args": [
          "self",
          "execution",
          "verification"
        ],
        "decorators": [],
        "docstring": "Check safety compliance of executed commands",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult",
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "cancel_execution",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 244,
        "line_end": 259,
        "args": [
          "self",
          "execution_id"
        ],
        "decorators": [],
        "docstring": "\n        Cancel an active execution\n\n        Args:\n            execution_id: ID of execution to cancel\n\n        Returns:\n            True if cancelled successfully, False otherwise\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionStatus"
        ],
        "is_method": true,
        "class_name": "Executor"
      },
      {
        "name": "get",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 145,
        "line_end": 156,
        "args": [
          "self",
          "key_path",
          "default"
        ],
        "decorators": [],
        "docstring": "Get configuration value by dot-separated path",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyConfig"
      },
      {
        "name": "_basic_conflict_detection_fallback",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 1037,
        "line_end": 1058,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Basic conflict detection fallback when enhanced detection fails",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "get_available_tools",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1488,
        "line_end": 1490,
        "args": [],
        "decorators": [],
        "docstring": "Get list of available tools",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_format_conversation_for_prompt",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 450,
        "line_end": 465,
        "args": [
          "self",
          "messages"
        ],
        "decorators": [],
        "docstring": "Format conversation history into a single prompt",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "process_conflict_resolution",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 35,
        "line_end": 84,
        "args": [
          "self",
          "scenario_context",
          "conflict_geometry",
          "environmental_conditions",
          "llm_decision",
          "baseline_decision"
        ],
        "decorators": [],
        "docstring": "\n        Process a conflict resolution with experience replay integration\n\n        Returns:\n            Tuple of (enhanced_decision, lessons_learned)\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "_setup_simulation",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 217,
        "line_end": 261,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Setup simulation parameters",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "get_conflict_resolution",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 988,
        "line_end": 1043,
        "args": [
          "self",
          "conflict_info",
          "use_function_calls"
        ],
        "decorators": [],
        "docstring": "\n        High-level API for getting conflict resolution from LLM.\n\n        Args:\n            conflict_info: Conflict scenario information\n            use_function_calls: Override function calling setting\n\n        Returns:\n            BlueSky command string or None if resolution fails\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "compute_average_separation_margins",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 315,
        "line_end": 367,
        "args": [
          "self",
          "results_df"
        ],
        "decorators": [],
        "docstring": "\n        Compute average separation margins from results.\n\n        Args:\n            results_df: DataFrame with trajectory or margin data\n\n        Returns:\n            Dict with horizontal and vertical margin averages\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "calc_separation_margin"
        ],
        "is_method": true,
        "class_name": "MonteCarloResultsAnalyzer"
      },
      {
        "name": "is_available",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 283,
        "line_end": 285,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Check if BlueSky is available and initialized",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_analyze_trajectory_conflict",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 978,
        "line_end": 1063,
        "args": [
          "self",
          "ac1",
          "ac2"
        ],
        "decorators": [],
        "docstring": "Analyze if two aircraft trajectories will conflict",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ScenarioGenerator"
      },
      {
        "name": "_check_execution_timing",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 155,
        "line_end": 175,
        "args": [
          "self",
          "execution",
          "verification"
        ],
        "decorators": [],
        "docstring": "Check execution timing constraints",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult",
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "_determine_severity",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 328,
        "line_end": 350,
        "args": [
          "self",
          "detected_types"
        ],
        "decorators": [],
        "docstring": "Determine severity based on detected hallucination types",
        "imports_used": [],
        "intra_repo_calls": [
          "HallucinationType"
        ],
        "is_method": true,
        "class_name": "EnhancedHallucinationDetector"
      },
      {
        "name": "quick_conflict_resolution",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 683,
        "line_end": 717,
        "args": [
          "aircraft_1",
          "aircraft_2",
          "time_to_conflict",
          "client"
        ],
        "decorators": [],
        "docstring": "\n    Quick conflict resolution with minimal prompt overhead.\n\n    Args:\n        aircraft_1: First aircraft data\n        aircraft_2: Second aircraft data\n        time_to_conflict: Time to conflict in seconds\n        client: Optional client instance\n\n    Returns:\n        LLMResponse with resolution command\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "LLMResponse",
          "LLMClient"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_is_distilled_model_response",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 696,
        "line_end": 709,
        "args": [
          "self",
          "response_text"
        ],
        "decorators": [],
        "docstring": "Check if this is a response from the distilled BlueSky Gym model",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "get_experience_summary",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 411,
        "line_end": 426,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get summary of stored experiences",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "_calculate_baseline_margin",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 373,
        "line_end": 395,
        "args": [
          "self",
          "geometry"
        ],
        "decorators": [],
        "docstring": "Calculate baseline margin without any resolution maneuver",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictGeometry"
        ],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "_calculate_horizontal_separation",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 440,
        "line_end": 457,
        "args": [
          "self",
          "ac1_idx",
          "ac2_idx"
        ],
        "decorators": [],
        "docstring": "Calculate horizontal separation between two aircraft",
        "imports_used": [],
        "intra_repo_calls": [
          "haversine_distance"
        ],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "get_conflict_detection_optimized",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1794,
        "line_end": 1832,
        "args": [
          "self",
          "aircraft_states",
          "time_horizon",
          "priority"
        ],
        "decorators": [],
        "docstring": "\n        High-performance conflict detection API.\n\n        Args:\n            aircraft_states: List of aircraft data\n            time_horizon: Detection time horizon in minutes\n            priority: Request priority\n\n        Returns:\n            Detection results dictionary or None if failed\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "get_session_metrics",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 454,
        "line_end": 473,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get performance metrics for the current session",
        "imports_used": [],
        "intra_repo_calls": [
          "StepType"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "_simulate_reset",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 735,
        "line_end": 745,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Simulate reset when BlueSky unavailable",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "BlueSkyInterface"
      },
      {
        "name": "_check_execution_status",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 135,
        "line_end": 153,
        "args": [
          "self",
          "execution",
          "verification"
        ],
        "decorators": [],
        "docstring": "Check if execution completed successfully",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult",
          "VerificationResult",
          "ExecutionStatus"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "quick_conflict_detection",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 720,
        "line_end": 752,
        "args": [
          "aircraft_states",
          "client"
        ],
        "decorators": [],
        "docstring": "\n    Quick conflict detection with minimal overhead.\n\n    Args:\n        aircraft_states: List of aircraft data\n        client: Optional client instance\n\n    Returns:\n        LLMResponse with detection results (JSON)\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "LLMResponse",
          "LLMClient"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "add_conflict_resolution",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 427,
        "line_end": 468,
        "args": [
          "self",
          "conflict_id",
          "geometry",
          "llm_resolution",
          "baseline_resolution",
          "environmental_conditions"
        ],
        "decorators": [],
        "docstring": "Add a conflict resolution case and compute comparative metrics",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictGeometry"
        ],
        "is_method": true,
        "class_name": "SafetyMetricsAggregator"
      },
      {
        "name": "_find_relevant_experiences",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 86,
        "line_end": 121,
        "args": [
          "self",
          "scenario_context",
          "conflict_geometry",
          "environmental_conditions"
        ],
        "decorators": [],
        "docstring": "Find experiences relevant to current scenario",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictExperience",
          "SimilarityResult"
        ],
        "is_method": true,
        "class_name": "ExperienceIntegrator"
      },
      {
        "name": "check_prerequisites",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 36,
        "line_end": 62,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Check if all required files and directories exist.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "generate_scenario_batch",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 817,
        "line_end": 886,
        "args": [
          "self",
          "count",
          "complexity_distribution",
          "distribution_shift_distribution"
        ],
        "decorators": [],
        "docstring": "\n        Generate multiple scenarios for Monte Carlo testing.\n\n        Args:\n            count: Number of scenarios to generate\n            complexity_distribution: Distribution of complexity levels\n            distribution_shift_distribution: Distribution of shift tiers\n\n        Returns:\n            List of generated scenarios\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioConfiguration",
          "ComplexityTier"
        ],
        "is_method": true,
        "class_name": "BlueSkyScenarioGenerator"
      },
      {
        "name": "_save_detection_analysis",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 2289,
        "line_end": 2333,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Save detection analysis summary",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "__init__",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1205,
        "line_end": 1206,
        "args": [
          "self",
          "generator"
        ],
        "decorators": [],
        "docstring": null,
        "imports_used": [],
        "intra_repo_calls": [
          "ScenarioGenerator"
        ],
        "is_method": true,
        "class_name": "VerticalCREnv"
      },
      {
        "name": "complete_session",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 288,
        "line_end": 344,
        "args": [
          "self",
          "success",
          "final_status"
        ],
        "decorators": [],
        "docstring": "\n        Complete the current session and generate summary\n\n        Args:\n            success: Whether the session completed successfully\n            final_status: Final status description\n\n        Returns:\n            SessionSummary of the completed session\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "SessionSummary",
          "StepType"
        ],
        "is_method": true,
        "class_name": "Scratchpad"
      },
      {
        "name": "_generate_commands",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 335,
        "line_end": 364,
        "args": [
          "self",
          "assessment"
        ],
        "decorators": [],
        "docstring": "Generate specific BlueSky commands for conflict resolution",
        "imports_used": [],
        "intra_repo_calls": [
          "PlanType",
          "ConflictAssessment"
        ],
        "is_method": true,
        "class_name": "Planner"
      },
      {
        "name": "step_simulation",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1274,
        "line_end": 1311,
        "args": [
          "minutes",
          "dtmult"
        ],
        "decorators": [],
        "docstring": "\n    Advance the BlueSky simulation by a number of minutes.\n\n    Args:\n        minutes: Number of minutes to advance the simulation\n        dtmult: Time multiplier (simulation speed factor)\n\n    Returns:\n        Status dictionary with simulation step information\n    ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "create_ollama_model",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 238,
        "line_end": 313,
        "args": [
          "self",
          "use_gguf"
        ],
        "decorators": [],
        "docstring": "Create Ollama model from merged weights.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "_write_csv_row",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 2255,
        "line_end": 2287,
        "args": [
          "self",
          "comparison"
        ],
        "decorators": [],
        "docstring": "Write detection comparison to CSV",
        "imports_used": [],
        "intra_repo_calls": [
          "DetectionComparison"
        ],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "_calculate_safety_score",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 297,
        "line_end": 315,
        "args": [
          "self",
          "verification"
        ],
        "decorators": [],
        "docstring": "Calculate overall safety score based on verification results",
        "imports_used": [],
        "intra_repo_calls": [
          "VerificationResult"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "get_average_inference_time",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 467,
        "line_end": 471,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get average inference time per LLM call.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMClient"
      },
      {
        "name": "_init_prompt_templates",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 104,
        "line_end": 321,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Initialize standardized prompt templates",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "_apply_resolution_maneuver",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 155,
        "line_end": 222,
        "args": [
          "self",
          "geometry",
          "maneuver"
        ],
        "decorators": [],
        "docstring": "Apply resolution maneuver and predict future conflict geometry",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictGeometry"
        ],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "get_verification_metrics",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 340,
        "line_end": 370,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Get overall verification performance metrics",
        "imports_used": [],
        "intra_repo_calls": [
          "VerificationStatus"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "_detect_with_statebased",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 171,
        "line_end": 221,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Detect conflicts using BlueSky STATEBASED method",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "generate_scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1191,
        "line_end": 1199,
        "args": [
          "self",
          "n_aircraft",
          "conflict"
        ],
        "decorators": [],
        "docstring": "Generate horizontal conflict scenario",
        "imports_used": [],
        "intra_repo_calls": [
          "Scenario"
        ],
        "is_method": true,
        "class_name": "HorizontalCREnv"
      },
      {
        "name": "_calculate_closest_approach",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 261,
        "line_end": 302,
        "args": [
          "self",
          "pos1",
          "pos2",
          "vel1",
          "vel2"
        ],
        "decorators": [],
        "docstring": "Calculate time and distance of closest approach",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "SafetyMarginQuantifier"
      },
      {
        "name": "_mock_conflict_detection",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 568,
        "line_end": 586,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Mock conflict detection when BlueSky is not available",
        "imports_used": [],
        "intra_repo_calls": [
          "ConflictData"
        ],
        "is_method": true,
        "class_name": "EnhancedConflictDetector"
      },
      {
        "name": "delete_experience",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 460,
        "line_end": 468,
        "args": [
          "self",
          "experience_id"
        ],
        "decorators": [],
        "docstring": "Delete an experience by ID",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "VectorReplayStore"
      },
      {
        "name": "check",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 58,
        "line_end": 133,
        "args": [
          "self",
          "execution_result",
          "timeout_seconds"
        ],
        "decorators": [],
        "docstring": "\n        Perform verification check on execution result\n\n        Args:\n            execution_result: ExecutionResult to verify\n            timeout_seconds: Maximum time to wait for verification\n\n        Returns:\n            True if verification passes, False otherwise\n        ",
        "imports_used": [],
        "intra_repo_calls": [
          "ExecutionResult",
          "VerificationResult",
          "VerificationStatus"
        ],
        "is_method": true,
        "class_name": "Verifier"
      },
      {
        "name": "_parse_aircraft_pairs",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 1515,
        "line_end": 1541,
        "args": [
          "self",
          "pairs_text"
        ],
        "decorators": [],
        "docstring": "Parse aircraft pairs from text using configurable aircraft ID pattern",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LLMPromptEngine"
      },
      {
        "name": "execute_tool",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 1452,
        "line_end": 1485,
        "args": [
          "tool_name"
        ],
        "decorators": [],
        "docstring": "\n    Execute a tool by name with provided arguments\n\n    Args:\n        tool_name: Name of the tool to execute\n        **kwargs: Arguments to pass to the tool\n\n    Returns:\n        Tool execution result\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "BlueSkyToolsError"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "_extract_ground_truth_conflicts",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 887,
        "line_end": 907,
        "args": [
          "self",
          "scenario"
        ],
        "decorators": [],
        "docstring": "Extract ground truth conflicts from scenario",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloBenchmark"
      },
      {
        "name": "plot_tier_comparison",
        "file_path": "analysis\\visualisation.py",
        "line_start": 989,
        "line_end": 1014,
        "args": [
          "df",
          "output_dir"
        ],
        "decorators": [],
        "docstring": "\n    Plot tier comparison analysis.\n\n    Args:\n        df: DataFrame with results across tiers\n        output_dir: Output directory for plots\n        **kwargs: Additional arguments\n\n    Returns:\n        Path to generated comparison plot, or None if visualization failed\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "MonteCarloVisualizer"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "run_benchmark_with_config",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 2336,
        "line_end": 2368,
        "args": [
          "config_path"
        ],
        "decorators": [],
        "docstring": "\n    Run Monte Carlo benchmark with configuration file.\n\n    Args:\n        config_path: Path to JSON configuration file\n\n    Returns:\n        Benchmark summary results\n    ",
        "imports_used": [],
        "intra_repo_calls": [
          "MonteCarloBenchmark",
          "BenchmarkConfiguration",
          "ScenarioType",
          "ComplexityTier"
        ],
        "is_method": false,
        "class_name": null
      },
      {
        "name": "run_complete_pipeline",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 455,
        "line_end": 508,
        "args": [
          "self"
        ],
        "decorators": [],
        "docstring": "Run the complete LoRA merge and conversion pipeline.",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "LoRAMerger"
      },
      {
        "name": "create_performance_summary_charts",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 1009,
        "line_end": 1057,
        "args": [
          "self",
          "aggregated_metrics",
          "output_dir"
        ],
        "decorators": [],
        "docstring": "\n        Create bar charts summarizing performance across scenario types.\n\n        Args:\n            aggregated_metrics: Output from aggregate_monte_carlo_metrics()\n            output_dir: Directory to save plots\n\n        Returns:\n            List of created plot file paths\n        ",
        "imports_used": [],
        "intra_repo_calls": [],
        "is_method": true,
        "class_name": "MonteCarloVisualizer"
      }
    ],
    "removed_imports": [],
    "added_imports": [
      {
        "module": "sys",
        "names": [
          "sys"
        ],
        "alias": null,
        "line_number": 6,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "llm_atc.tools.llm_prompt_engine",
        "names": [
          "LLMPromptEngine"
        ],
        "alias": null,
        "line_number": 40,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional",
          "Union"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": true
      },
      {
        "module": "pathlib",
        "names": [
          "Path"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": true
      },
      {
        "module": "planner",
        "names": [
          "ActionPlan"
        ],
        "alias": null,
        "line_number": 12,
        "file_path": "llm_atc\\agents\\executor.py",
        "is_from_import": true
      },
      {
        "module": "bluesky",
        "names": [
          "bluesky"
        ],
        "alias": "bs",
        "line_number": 24,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 16,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "is_from_import": false
      },
      {
        "module": "transformers",
        "names": [
          "AutoTokenizer",
          "AutoModelForCausalLM"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional",
          "Dict",
          "List"
        ],
        "alias": null,
        "line_number": 4,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": true
      },
      {
        "module": "re",
        "names": [
          "re"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "is_from_import": false
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 720,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": false
      },
      {
        "module": "matplotlib.pyplot",
        "names": [
          "matplotlib.pyplot"
        ],
        "alias": "plt",
        "line_number": 16,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "re",
        "names": [
          "re"
        ],
        "alias": null,
        "line_number": 541,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 3,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 5,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 3,
        "file_path": "llm_interface\\filter_sort.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 24,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": true
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 15,
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "is_from_import": false
      },
      {
        "module": "subprocess",
        "names": [
          "subprocess"
        ],
        "alias": null,
        "line_number": 15,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      },
      {
        "module": "os",
        "names": [
          "os"
        ],
        "alias": null,
        "line_number": 5,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "chromadb",
        "names": [
          "chromadb"
        ],
        "alias": null,
        "line_number": 14,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": false
      },
      {
        "module": "contextlib",
        "names": [
          "contextlib"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": true
      },
      {
        "module": "numpy",
        "names": [
          "numpy"
        ],
        "alias": "np",
        "line_number": 15,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": false
      },
      {
        "module": "traceback",
        "names": [
          "traceback"
        ],
        "alias": null,
        "line_number": 311,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      },
      {
        "module": "re",
        "names": [
          "re"
        ],
        "alias": null,
        "line_number": 497,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": false
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 19,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": false
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "is_from_import": true
      },
      {
        "module": "sklearn.preprocessing",
        "names": [
          "StandardScaler"
        ],
        "alias": null,
        "line_number": 72,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "numpy",
        "names": [
          "numpy"
        ],
        "alias": "np",
        "line_number": 16,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "is_from_import": false
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 11,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": true
      },
      {
        "module": "torch",
        "names": [
          "torch"
        ],
        "alias": null,
        "line_number": 14,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 19,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "is_from_import": true
      },
      {
        "module": "plotly.express",
        "names": [
          "plotly.express"
        ],
        "alias": "px",
        "line_number": 47,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 16,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional",
          "List",
          "Dict"
        ],
        "alias": null,
        "line_number": 34,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 16,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": false
      },
      {
        "module": "llm_prompt_engine",
        "names": [
          "ConflictPromptData",
          "LLMPromptEngine",
          "ResolutionResponse"
        ],
        "alias": null,
        "line_number": 26,
        "file_path": "llm_atc\\tools\\__init__.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 12,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": true
      },
      {
        "module": "yaml",
        "names": [
          "yaml"
        ],
        "alias": null,
        "line_number": 13,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "bluesky",
        "names": [
          "sim",
          "stack",
          "traf"
        ],
        "alias": null,
        "line_number": 27,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": true
      },
      {
        "module": "seaborn",
        "names": [
          "seaborn"
        ],
        "alias": "sns",
        "line_number": 33,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "bluesky",
        "names": [
          "sim",
          "stack",
          "traf"
        ],
        "alias": null,
        "line_number": 25,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "is_from_import": true
      },
      {
        "module": "numpy",
        "names": [
          "numpy"
        ],
        "alias": "np",
        "line_number": 20,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": false
      },
      {
        "module": "traceback",
        "names": [
          "traceback"
        ],
        "alias": null,
        "line_number": 124,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      },
      {
        "module": "pathlib",
        "names": [
          "Path"
        ],
        "alias": null,
        "line_number": 33,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 21,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\agents\\executor.py",
        "is_from_import": true
      },
      {
        "module": "peft",
        "names": [
          "PeftModel"
        ],
        "alias": null,
        "line_number": 19,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": true
      },
      {
        "module": "warnings",
        "names": [
          "warnings"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "os",
        "names": [
          "os"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "matplotlib.pyplot",
        "names": [
          "matplotlib.pyplot"
        ],
        "alias": "plt",
        "line_number": 30,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": true
      },
      {
        "module": "llm_atc.memory.replay_store",
        "names": [
          "ConflictExperience"
        ],
        "alias": null,
        "line_number": 488,
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "is_from_import": true
      },
      {
        "module": "llm_interface.llm_client",
        "names": [
          "LLMClient"
        ],
        "alias": null,
        "line_number": 21,
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 6,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 20,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "is_from_import": false
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": false
      },
      {
        "module": "os",
        "names": [
          "os"
        ],
        "alias": null,
        "line_number": 26,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "asdict",
          "dataclass"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": true
      },
      {
        "module": "os",
        "names": [
          "os"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": false
      },
      {
        "module": "plotly.figure_factory",
        "names": [
          "plotly.figure_factory"
        ],
        "alias": "ff",
        "line_number": 49,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "executor",
        "names": [
          "ExecutionResult",
          "ExecutionStatus"
        ],
        "alias": null,
        "line_number": 12,
        "file_path": "llm_atc\\agents\\verifier.py",
        "is_from_import": true
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 761,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": false
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "is_from_import": false
      },
      {
        "module": "planner",
        "names": [
          "ActionPlan",
          "ConflictAssessment"
        ],
        "alias": null,
        "line_number": 14,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": true
      },
      {
        "module": "csv",
        "names": [
          "csv"
        ],
        "alias": null,
        "line_number": 30,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_atc\\agents\\verifier.py",
        "is_from_import": false
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 12,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": false
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 20,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "is_from_import": true
      },
      {
        "module": "uuid",
        "names": [
          "uuid"
        ],
        "alias": null,
        "line_number": 181,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 6,
        "file_path": "llm_atc\\agents\\executor.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "llm_atc\\agents\\verifier.py",
        "is_from_import": true
      },
      {
        "module": "pathlib",
        "names": [
          "Path"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 13,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": false
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 20,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": true
      },
      {
        "module": "datetime",
        "names": [
          "datetime"
        ],
        "alias": null,
        "line_number": 32,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "matplotlib.animation",
        "names": [
          "FuncAnimation"
        ],
        "alias": null,
        "line_number": 32,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\agents\\executor.py",
        "is_from_import": true
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 1,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": false
      },
      {
        "module": "llm_atc.memory.replay_store",
        "names": [
          "ConflictExperience",
          "SimilarityResult",
          "VectorReplayStore"
        ],
        "alias": null,
        "line_number": 12,
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": false
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\agents\\verifier.py",
        "is_from_import": true
      },
      {
        "module": "tempfile",
        "names": [
          "tempfile"
        ],
        "alias": null,
        "line_number": 1360,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": false
      },
      {
        "module": "scenarios.monte_carlo_framework",
        "names": [
          "ComplexityTier"
        ],
        "alias": null,
        "line_number": 41,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "is_from_import": false
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "llm_atc.metrics.safety_margin_quantifier",
        "names": [
          "SafetyMarginQuantifier"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "is_from_import": false
      },
      {
        "module": "pathlib",
        "names": [
          "Path"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": true
      },
      {
        "module": "pathlib",
        "names": [
          "Path"
        ],
        "alias": null,
        "line_number": 12,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": true
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 2,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "re",
        "names": [
          "re"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 11,
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "is_from_import": true
      },
      {
        "module": "shapely.geometry",
        "names": [
          "Point",
          "LineString"
        ],
        "alias": null,
        "line_number": 61,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "numpy",
        "names": [
          "numpy"
        ],
        "alias": "np",
        "line_number": 15,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": false
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 2,
        "file_path": "llm_interface\\filter_sort.py",
        "is_from_import": false
      },
      {
        "module": "pathlib",
        "names": [
          "Path"
        ],
        "alias": null,
        "line_number": 19,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_atc\\agents\\executor.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": false
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 16,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": false
      },
      {
        "module": "scipy",
        "names": [
          "stats"
        ],
        "alias": null,
        "line_number": 70,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\agents\\planner.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": false
      },
      {
        "module": "llm_atc.tools.enhanced_conflict_detector",
        "names": [
          "EnhancedConflictDetector"
        ],
        "alias": null,
        "line_number": 915,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": false
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_atc\\agents\\planner.py",
        "is_from_import": false
      },
      {
        "module": "numpy",
        "names": [
          "numpy"
        ],
        "alias": "np",
        "line_number": 11,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "chromadb.config",
        "names": [
          "Settings"
        ],
        "alias": null,
        "line_number": 16,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "List",
          "Optional",
          "Tuple"
        ],
        "alias": null,
        "line_number": 21,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": false
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_atc\\agents\\verifier.py",
        "is_from_import": true
      },
      {
        "module": "plotly.subplots",
        "names": [
          "make_subplots"
        ],
        "alias": null,
        "line_number": 48,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "llm_client",
        "names": [
          "LLMClient"
        ],
        "alias": null,
        "line_number": 5,
        "file_path": "llm_interface\\filter_sort.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any"
        ],
        "alias": null,
        "line_number": 14,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": true
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 121,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 15,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": false
      },
      {
        "module": "analysis.enhanced_hallucination_detection",
        "names": [
          "EnhancedHallucinationDetector"
        ],
        "alias": null,
        "line_number": 11,
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 24,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "sentence_transformers",
        "names": [
          "SentenceTransformer"
        ],
        "alias": null,
        "line_number": 20,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": true
      },
      {
        "module": "seaborn",
        "names": [
          "seaborn"
        ],
        "alias": "sns",
        "line_number": 17,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 111,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "pandas",
        "names": [
          "pandas"
        ],
        "alias": "pd",
        "line_number": 12,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "yaml",
        "names": [
          "yaml"
        ],
        "alias": null,
        "line_number": 23,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional",
          "List"
        ],
        "alias": null,
        "line_number": 19,
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 6,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": false
      },
      {
        "module": "traceback",
        "names": [
          "traceback"
        ],
        "alias": null,
        "line_number": 28,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "is_from_import": true
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 15,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": false
      },
      {
        "module": "argparse",
        "names": [
          "argparse"
        ],
        "alias": null,
        "line_number": 2373,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Callable",
          "Optional"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "llm_atc\\agents\\executor.py",
        "is_from_import": true
      },
      {
        "module": "random",
        "names": [
          "random"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": false
      },
      {
        "module": "ollama",
        "names": [
          "ollama"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 18,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "is_from_import": false
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 21,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "dataclasses",
        "names": [
          "asdict",
          "dataclass"
        ],
        "alias": null,
        "line_number": 31,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 13,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": true
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 11,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "geopandas",
        "names": [
          "geopandas"
        ],
        "alias": "gpd",
        "line_number": 60,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "re",
        "names": [
          "re"
        ],
        "alias": null,
        "line_number": 400,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "shutil",
        "names": [
          "shutil"
        ],
        "alias": null,
        "line_number": 16,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 4,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": false
      },
      {
        "module": "socket",
        "names": [
          "socket"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 21,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "llm_atc\\agents\\planner.py",
        "is_from_import": true
      },
      {
        "module": "math",
        "names": [
          "math"
        ],
        "alias": null,
        "line_number": 25,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "llm_atc.tools.bluesky_tools",
        "names": [
          "set_strict_mode"
        ],
        "alias": null,
        "line_number": 267,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "asdict",
          "dataclass"
        ],
        "alias": null,
        "line_number": 22,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 12,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": true
      },
      {
        "module": "llm_atc.metrics.monte_carlo_analysis",
        "names": [
          "MonteCarloResultsAnalyzer"
        ],
        "alias": null,
        "line_number": 56,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 23,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": true
      },
      {
        "module": "matplotlib.pyplot",
        "names": [
          "matplotlib.pyplot"
        ],
        "alias": "plt",
        "line_number": 36,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "matplotlib.patches",
        "names": [
          "matplotlib.patches"
        ],
        "alias": "patches",
        "line_number": 31,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "plotly.graph_objects",
        "names": [
          "plotly.graph_objects"
        ],
        "alias": "go",
        "line_number": 46,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "scenarios.scenario_generator",
        "names": [
          "ScenarioGenerator",
          "ScenarioType",
          "generate_horizontal_scenario",
          "generate_sector_scenario",
          "generate_vertical_scenario"
        ],
        "alias": null,
        "line_number": 46,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "bluesky",
        "names": [
          "bluesky"
        ],
        "alias": "bs",
        "line_number": 22,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": false
      },
      {
        "module": "concurrent.futures",
        "names": [
          "ThreadPoolExecutor",
          "as_completed"
        ],
        "alias": null,
        "line_number": 11,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": true
      },
      {
        "module": "scenarios.monte_carlo_framework",
        "names": [
          "BlueSkyScenarioGenerator",
          "ComplexityTier",
          "ScenarioConfiguration"
        ],
        "alias": null,
        "line_number": 27,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": true
      },
      {
        "module": "executor",
        "names": [
          "ExecutionResult"
        ],
        "alias": null,
        "line_number": 13,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": true
      },
      {
        "module": "folium",
        "names": [
          "folium"
        ],
        "alias": null,
        "line_number": 59,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "ollama",
        "names": [
          "ollama"
        ],
        "alias": null,
        "line_number": 8,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": false
      },
      {
        "module": "uuid",
        "names": [
          "uuid"
        ],
        "alias": null,
        "line_number": 29,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "bluesky",
        "names": [
          "sim",
          "stack",
          "traf"
        ],
        "alias": null,
        "line_number": 25,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "is_from_import": true
      },
      {
        "module": "collections",
        "names": [
          "defaultdict"
        ],
        "alias": null,
        "line_number": 7,
        "file_path": "llm_atc\\metrics\\__init__.py",
        "is_from_import": true
      },
      {
        "module": "bluesky",
        "names": [
          "sim",
          "traf"
        ],
        "alias": null,
        "line_number": 188,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 2,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": false
      },
      {
        "module": "scipy.spatial.distance",
        "names": [
          "pdist",
          "squareform"
        ],
        "alias": null,
        "line_number": 71,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "functools",
        "names": [
          "lru_cache"
        ],
        "alias": null,
        "line_number": 6,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Optional",
          "Union"
        ],
        "alias": null,
        "line_number": 11,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "llm_atc\\memory\\replay_store.py",
        "is_from_import": false
      },
      {
        "module": "safety_margin_quantifier",
        "names": [
          "calc_efficiency_penalty",
          "calc_separation_margin"
        ],
        "alias": null,
        "line_number": 40,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": true
      },
      {
        "module": "os",
        "names": [
          "os"
        ],
        "alias": null,
        "line_number": 11,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      },
      {
        "module": "pandas",
        "names": [
          "pandas"
        ],
        "alias": "pd",
        "line_number": 26,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "List",
          "Optional",
          "Tuple"
        ],
        "alias": null,
        "line_number": 21,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "is_from_import": true
      },
      {
        "module": "tempfile",
        "names": [
          "tempfile"
        ],
        "alias": null,
        "line_number": 20,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      },
      {
        "module": "re",
        "names": [
          "re"
        ],
        "alias": null,
        "line_number": 686,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "requests",
        "names": [
          "requests"
        ],
        "alias": null,
        "line_number": 212,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "numpy",
        "names": [
          "numpy"
        ],
        "alias": "np",
        "line_number": 25,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": false
      },
      {
        "module": "dataclasses",
        "names": [
          "asdict"
        ],
        "alias": null,
        "line_number": 22,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 17,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": true
      },
      {
        "module": "bluesky",
        "names": [
          "bluesky"
        ],
        "alias": "bs",
        "line_number": 24,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "is_from_import": false
      },
      {
        "module": "random",
        "names": [
          "random"
        ],
        "alias": null,
        "line_number": 20,
        "file_path": "scenarios\\scenario_generator.py",
        "is_from_import": false
      },
      {
        "module": "matplotlib.pyplot",
        "names": [
          "matplotlib.pyplot"
        ],
        "alias": "plt",
        "line_number": 25,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": false
      },
      {
        "module": "verifier",
        "names": [
          "VerificationResult"
        ],
        "alias": null,
        "line_number": 15,
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "is_from_import": true
      },
      {
        "module": "pandas",
        "names": [
          "pandas"
        ],
        "alias": "pd",
        "line_number": 21,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": false
      },
      {
        "module": "safety_margin_quantifier",
        "names": [
          "calc_efficiency_penalty",
          "calc_separation_margin"
        ],
        "alias": null,
        "line_number": 33,
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 19,
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "is_from_import": true
      },
      {
        "module": "typing",
        "names": [
          "Any",
          "Dict",
          "List",
          "Optional",
          "Tuple",
          "Union"
        ],
        "alias": null,
        "line_number": 20,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 16,
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "is_from_import": false
      },
      {
        "module": "collections",
        "names": [
          "defaultdict"
        ],
        "alias": null,
        "line_number": 23,
        "file_path": "analysis\\visualisation.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 10,
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 27,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "enum",
        "names": [
          "Enum"
        ],
        "alias": null,
        "line_number": 9,
        "file_path": "llm_atc\\agents\\planner.py",
        "is_from_import": true
      },
      {
        "module": "llm_atc.tools",
        "names": [
          "bluesky_tools"
        ],
        "alias": null,
        "line_number": 39,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": true
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 6,
        "file_path": "llm_atc\\agents\\verifier.py",
        "is_from_import": false
      },
      {
        "module": "bluesky",
        "names": [
          "sim",
          "stack",
          "traf"
        ],
        "alias": null,
        "line_number": 23,
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "is_from_import": true
      },
      {
        "module": "bluesky_tools",
        "names": [
          "TOOL_REGISTRY",
          "AircraftInfo",
          "BlueSkyToolsError",
          "ConflictInfo",
          "check_separation_violation",
          "continue_monitoring",
          "execute_tool",
          "get_airspace_info",
          "get_all_aircraft_info",
          "get_available_tools",
          "get_conflict_info",
          "get_distance",
          "get_minimum_separation",
          "get_weather_info",
          "reset_simulation",
          "search_experience_library",
          "send_command",
          "step_simulation"
        ],
        "alias": null,
        "line_number": 6,
        "file_path": "llm_atc\\tools\\__init__.py",
        "is_from_import": true
      },
      {
        "module": "dataclasses",
        "names": [
          "dataclass"
        ],
        "alias": null,
        "line_number": 19,
        "file_path": "scenarios\\monte_carlo_framework.py",
        "is_from_import": true
      },
      {
        "module": "time",
        "names": [
          "time"
        ],
        "alias": null,
        "line_number": 3,
        "file_path": "llm_interface\\llm_client.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 6,
        "file_path": "llm_atc\\agents\\planner.py",
        "is_from_import": false
      },
      {
        "module": "pandas",
        "names": [
          "pandas"
        ],
        "alias": "pd",
        "line_number": 37,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "re",
        "names": [
          "re"
        ],
        "alias": null,
        "line_number": 649,
        "file_path": "llm_interface\\ensemble.py",
        "is_from_import": false
      },
      {
        "module": "json",
        "names": [
          "json"
        ],
        "alias": null,
        "line_number": 23,
        "file_path": "scenarios\\monte_carlo_runner.py",
        "is_from_import": false
      },
      {
        "module": "logging",
        "names": [
          "logging"
        ],
        "alias": null,
        "line_number": 13,
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "is_from_import": false
      }
    ],
    "removed_classes": [],
    "added_classes": [
      {
        "name": "SimilarityResult",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 82,
        "line_end": 87,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "SessionSummary",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 44,
        "line_end": 57,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ExperienceIntegrator",
        "file_path": "llm_atc\\memory\\experience_integrator.py",
        "line_start": 20,
        "line_end": 518,
        "methods": [
          "__init__",
          "process_conflict_resolution",
          "_find_relevant_experiences",
          "_extract_lessons",
          "_check_hallucination_patterns",
          "_enhance_decision_with_experience",
          "record_resolution_outcome",
          "get_experience_summary",
          "_generate_learning_insights",
          "store_experience"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ConflictDetectionMethod",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 33,
        "line_end": 38,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "VerificationResult",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 23,
        "line_end": 36,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ConflictAssessment",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 41,
        "line_end": 51,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "PlanType",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 31,
        "line_end": 37,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "BlueSkyInterface",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 163,
        "line_end": 745,
        "methods": [
          "__init__",
          "_initialize_bluesky",
          "_setup_simulation",
          "_test_network_connection",
          "is_available",
          "get_aircraft_data",
          "get_conflict_data",
          "_calculate_horizontal_separation",
          "_assess_conflict_severity",
          "send_bluesky_command",
          "step_simulation_real",
          "reset_simulation_real",
          "_get_mock_aircraft_data",
          "_get_mock_conflict_data",
          "_simulate_command_execution",
          "_simulate_step",
          "_simulate_reset"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "SafetyMetricsAggregator",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 420,
        "line_end": 564,
        "methods": [
          "__init__",
          "add_conflict_resolution",
          "generate_safety_summary",
          "export_detailed_metrics"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "SafetyMarginQuantifier",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 59,
        "line_end": 417,
        "methods": [
          "__init__",
          "calculate_safety_margins",
          "_apply_resolution_maneuver",
          "_predict_position",
          "_calculate_closest_approach",
          "_calculate_horizontal_margin",
          "_calculate_vertical_margin",
          "_calculate_temporal_margin",
          "_calculate_effective_margin",
          "_calculate_total_uncertainty",
          "_calculate_baseline_margin",
          "_determine_safety_level",
          "_create_default_safety_margin"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ExecutionResult",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 27,
        "line_end": 38,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "OllamaEnsembleClient",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 54,
        "line_end": 708,
        "methods": [
          "__init__",
          "_initialize_models",
          "_get_available_models",
          "query_ensemble",
          "_create_role_specific_prompts",
          "_query_single_model",
          "_analyze_safety_flags",
          "_calculate_consensus",
          "_calculate_uncertainty_metrics",
          "_create_error_response",
          "get_ensemble_statistics",
          "_clean_json_response",
          "_create_valid_response_structure",
          "_extract_partial_response_data"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ConflictExperience",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 30,
        "line_end": 78,
        "methods": [
          "__post_init__"
        ],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "RAGValidator",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 711,
        "line_end": 842,
        "methods": [
          "__init__",
          "_initialize_knowledge_base",
          "validate_response"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ConflictDetectionMethod",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 33,
        "line_end": 38,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "HorizontalCREnv",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1185,
        "line_end": 1199,
        "methods": [
          "__init__",
          "generate_scenario"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "VectorReplayStore",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 100,
        "line_end": 487,
        "methods": [
          "__init__",
          "store_experience",
          "retrieve_experience",
          "get_all_experiences",
          "get_stats",
          "delete_experience",
          "clear_all"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ConflictPromptData",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 25,
        "line_end": 35,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ReasoningStep",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 29,
        "line_end": 40,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "BlueSkyConfig",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 38,
        "line_end": 156,
        "methods": [
          "__init__",
          "_find_config_file",
          "_create_default_config",
          "_load_config",
          "_get_default_config",
          "get"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "LLMResponse",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 20,
        "line_end": 27,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ConflictInfo",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 775,
        "line_end": 784,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "VerificationStatus",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 15,
        "line_end": 19,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "DetectionComparison",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 67,
        "line_end": 101,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ConflictGeometry",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 45,
        "line_end": 56,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ActionPlan",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 55,
        "line_end": 67,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "Planner",
        "file_path": "llm_atc\\agents\\planner.py",
        "line_start": 70,
        "line_end": 404,
        "methods": [
          "__init__",
          "assess_conflict",
          "generate_action_plan",
          "_detect_proximity_conflicts",
          "_calculate_separation",
          "_assess_severity",
          "_estimate_time_to_conflict",
          "_prioritize_conflicts",
          "_generate_assessment",
          "_determine_recommended_action",
          "_generate_reasoning",
          "_generate_commands",
          "_calculate_expected_outcome",
          "_calculate_priority",
          "get_assessment_history",
          "get_plan_history"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "MonteCarloResultsAnalyzer",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 53,
        "line_end": 992,
        "methods": [
          "__init__",
          "read_results_file",
          "_read_json_results",
          "compute_false_positive_negative_rates",
          "_conflicts_to_set",
          "compute_success_rates_by_scenario",
          "compute_success_rates_by_group",
          "compute_average_separation_margins",
          "compute_efficiency_penalties",
          "generate_report",
          "_generate_executive_summary",
          "_assess_detection_performance",
          "_assess_safety_margins",
          "_assess_efficiency_performance",
          "_format_grouped_success_table",
          "_format_distribution_shift_analysis",
          "_generate_recommendations",
          "aggregate_monte_carlo_metrics",
          "_analyze_distribution_shift_performance",
          "_create_empty_aggregated_metrics"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "StepType",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 18,
        "line_end": 25,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "EnsembleResponse",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 41,
        "line_end": 51,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ScenarioResult",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 172,
        "line_end": 244,
        "methods": [
          "__post_init__"
        ],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ChatMessage",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 12,
        "line_end": 16,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "Executor",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 41,
        "line_end": 325,
        "methods": [
          "__init__",
          "send_plan",
          "_send_command",
          "_simulate_command_execution",
          "cancel_execution",
          "get_execution_status",
          "get_active_executions",
          "get_execution_history",
          "get_execution_metrics",
          "set_command_sender"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ResolutionResponse",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 39,
        "line_end": 47,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "EnhancedConflictDetector",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 60,
        "line_end": 586,
        "methods": [
          "__init__",
          "detect_conflicts_comprehensive",
          "_detect_with_swarm",
          "_detect_with_statebased",
          "_detect_with_enhanced_analysis",
          "_analyze_aircraft_pair",
          "_calculate_cpa",
          "_calculate_horizontal_distance",
          "_assess_conflict_severity",
          "_calculate_confidence",
          "_cross_validate_conflicts",
          "_merge_conflict_detections",
          "_get_aircraft_pair_key",
          "validate_llm_conflicts",
          "_mock_conflict_detection"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "MonteCarloBenchmark",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 247,
        "line_end": 2333,
        "methods": [
          "__init__",
          "_setup_output_directory",
          "_setup_logging",
          "_setup_enhanced_logging",
          "_init_csv_file",
          "run",
          "_calculate_total_scenarios",
          "_run_scenario_batch",
          "_generate_scenario",
          "_get_aircraft_count_for_complexity",
          "_run_single_scenario",
          "_execute_scenario_pipeline",
          "_reset_bluesky_simulation",
          "_load_scenario_commands",
          "_extract_ground_truth_conflicts",
          "_detect_conflicts",
          "_basic_conflict_detection_fallback",
          "_get_aircraft_states_for_llm",
          "_validate_llm_conflicts_with_bluesky",
          "_resolve_conflicts",
          "_is_valid_bluesky_command",
          "_format_conflict_for_llm",
          "_verify_resolutions",
          "_calculate_all_separations",
          "_calculate_scenario_metrics",
          "_create_error_result",
          "_generate_summary",
          "_get_serializable_config",
          "_generate_summary_by_group",
          "_generate_combined_summary",
          "_print_detailed_analysis",
          "_generate_visualizations",
          "_plot_detection_performance",
          "_plot_safety_margins",
          "_plot_efficiency_metrics",
          "_plot_performance_by_type",
          "_plot_distribution_shift_impact",
          "_save_results",
          "_run_enhanced_scenario",
          "_create_detection_comparison",
          "_write_csv_row",
          "_save_detection_analysis"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "SafetyMargin",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 32,
        "line_end": 41,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ComplexityTier",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 35,
        "line_end": 41,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "SeparationStandard",
        "file_path": "llm_atc\\metrics\\safety_margin_quantifier.py",
        "line_start": 23,
        "line_end": 28,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "SectorCREnv",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1219,
        "line_end": 1238,
        "methods": [
          "__init__",
          "generate_scenario"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ScenarioGenerator",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 100,
        "line_end": 1181,
        "methods": [
          "__init__",
          "generate_scenario",
          "generate_horizontal_scenario",
          "generate_vertical_scenario",
          "generate_sector_scenario",
          "_create_horizontal_conflicts",
          "_avoid_horizontal_conflicts",
          "_create_vertical_conflicts",
          "_avoid_vertical_conflicts",
          "_create_vertical_conflicts_enhanced",
          "_avoid_vertical_conflicts_enhanced",
          "_optimize_conflict_timing",
          "_add_environmental_commands",
          "_calculate_horizontal_ground_truth",
          "_analyze_aircraft_pair_trajectory",
          "_calculate_vertical_ground_truth",
          "_calculate_sector_ground_truth",
          "_analyze_trajectory_conflict",
          "_determine_conflict_severity",
          "_calculate_bearing",
          "_calculate_distance_nm",
          "_are_headings_convergent",
          "_project_position"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "Scratchpad",
        "file_path": "llm_atc\\agents\\scratchpad.py",
        "line_start": 60,
        "line_end": 473,
        "methods": [
          "__init__",
          "log_step",
          "log_assessment_step",
          "log_planning_step",
          "log_execution_step",
          "log_verification_step",
          "log_error_step",
          "log_monitoring_step",
          "get_history",
          "get_step_by_id",
          "get_steps_by_type",
          "get_recent_steps",
          "complete_session",
          "start_new_session",
          "_generate_session_summary",
          "_calculate_average_confidence",
          "_extract_key_decisions",
          "_extract_lessons_learned",
          "export_session_data",
          "set_session_metadata",
          "get_session_metrics"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ConflictData",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector_clean.py",
        "line_start": 42,
        "line_end": 57,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "ScenarioType",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 34,
        "line_end": 39,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "HallucinationType",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 14,
        "line_end": 23,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "ConflictData",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 42,
        "line_end": 57,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "GroundTruthConflict",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 43,
        "line_end": 51,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "AircraftInfo",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 760,
        "line_end": 771,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "BlueSkyToolsError",
        "file_path": "llm_atc\\tools\\bluesky_tools.py",
        "line_start": 787,
        "line_end": 788,
        "methods": [],
        "base_classes": [
          "Exception"
        ],
        "decorators": []
      },
      {
        "name": "LLMPromptEngine",
        "file_path": "llm_atc\\tools\\llm_prompt_engine.py",
        "line_start": 50,
        "line_end": 1945,
        "methods": [
          "__init__",
          "_init_prompt_templates",
          "format_conflict_prompt",
          "format_detector_prompt",
          "parse_resolution_response",
          "parse_detector_response",
          "_is_distilled_model_response",
          "_parse_distilled_model_response",
          "_parse_detector_response_legacy",
          "_validate_detector_response",
          "_validate_aircraft_pairs",
          "_validate_confidence",
          "_validate_priority",
          "_validate_sector_response",
          "_validate_calculation_details",
          "_extract_json_from_response",
          "get_conflict_resolution",
          "get_conflict_resolution_with_prompts",
          "detect_conflict_via_llm",
          "detect_conflict_via_llm_with_prompts",
          "assess_resolution_safety",
          "_get_fallback_conflict_prompt",
          "_parse_function_call_response",
          "_extract_bluesky_command",
          "_normalize_bluesky_command",
          "_extract_aircraft_id",
          "_determine_maneuver_type",
          "_parse_aircraft_pairs",
          "_parse_time_values",
          "_parse_safety_response",
          "format_conflict_resolution_prompt_optimized",
          "format_conflict_detection_prompt_optimized",
          "get_conflict_resolution_optimized",
          "get_conflict_detection_optimized",
          "_parse_resolution_response_fast",
          "_parse_detection_response_fast",
          "_extract_aircraft_id_fast",
          "_determine_maneuver_type_fast",
          "get_performance_stats",
          "reset_performance_stats"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "BlueSkyScenarioGenerator",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 118,
        "line_end": 947,
        "methods": [
          "__init__",
          "_load_ranges",
          "_load_distribution_shift_config",
          "_get_default_ranges",
          "sample_from_range",
          "weighted_choice",
          "apply_distribution_shift",
          "generate_scenario",
          "_generate_environmental_conditions",
          "_generate_bluesky_commands",
          "_generate_conflict_commands",
          "_calculate_bearing",
          "execute_scenario",
          "_mock_execution",
          "generate_scenario_batch",
          "get_command_log",
          "validate_ranges"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "VerticalCREnv",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 1202,
        "line_end": 1216,
        "methods": [
          "__init__",
          "generate_scenario"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "MonteCarloVisualizer",
        "file_path": "llm_atc\\metrics\\monte_carlo_analysis.py",
        "line_start": 995,
        "line_end": 1300,
        "methods": [
          "__init__",
          "create_performance_summary_charts",
          "create_distribution_shift_plots",
          "_create_success_rate_chart",
          "_create_detection_performance_chart",
          "_create_safety_margins_chart",
          "_create_shift_performance_scatter"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "RetrievedExperience",
        "file_path": "llm_atc\\memory\\replay_store.py",
        "line_start": 91,
        "line_end": 97,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "LLMClient",
        "file_path": "llm_interface\\llm_client.py",
        "line_start": 30,
        "line_end": 679,
        "methods": [
          "__init__",
          "create_chat_messages",
          "ask",
          "ask_optimized",
          "_execute_chat_request",
          "_enhance_prompt_for_function_calling",
          "_process_function_calls",
          "_execute_function_call",
          "chat_with_function_calling",
          "_format_conversation_for_prompt",
          "get_average_inference_time",
          "get_total_inference_time",
          "get_inference_count",
          "validate_response",
          "_parse_json_response_fast",
          "_fix_common_json_issues",
          "_validate_atc_json_structure",
          "get_safe_default_resolution",
          "_create_cache_key",
          "_cache_response",
          "_get_priority_timeout",
          "get_conflict_resolution_system_prompt",
          "get_conflict_detection_system_prompt",
          "get_performance_stats",
          "reset_stats"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ExecutionStatus",
        "file_path": "llm_atc\\agents\\executor.py",
        "line_start": 18,
        "line_end": 23,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      },
      {
        "name": "BenchmarkConfiguration",
        "file_path": "scenarios\\monte_carlo_runner.py",
        "line_start": 105,
        "line_end": 168,
        "methods": [
          "__post_init__"
        ],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "Verifier",
        "file_path": "llm_atc\\agents\\verifier.py",
        "line_start": 39,
        "line_end": 375,
        "methods": [
          "__init__",
          "check",
          "_check_execution_status",
          "_check_execution_timing",
          "_check_command_success_rate",
          "_check_safety_compliance",
          "_check_response_validity",
          "_is_unsafe_command",
          "_is_valid_response",
          "_calculate_safety_score",
          "_calculate_confidence",
          "get_verification_history",
          "get_verification_metrics",
          "update_safety_thresholds"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "MonteCarloVisualizer",
        "file_path": "analysis\\visualisation.py",
        "line_start": 85,
        "line_end": 924,
        "methods": [
          "__init__",
          "generate_comprehensive_report",
          "_generate_distribution_analysis",
          "_generate_trend_analysis",
          "_generate_sensitivity_analysis",
          "_plot_metric_distributions",
          "_plot_shift_comparisons",
          "_plot_violin_comparisons",
          "_plot_ridge_plots",
          "_plot_cumulative_error_curves",
          "_plot_time_series_analysis",
          "_plot_performance_evolution",
          "_plot_tornado_sensitivity"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ScenarioConfiguration",
        "file_path": "scenarios\\monte_carlo_framework.py",
        "line_start": 45,
        "line_end": 115,
        "methods": [
          "aircraft_list",
          "environmental"
        ],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "EnhancedConflictDetector",
        "file_path": "llm_atc\\tools\\enhanced_conflict_detector.py",
        "line_start": 60,
        "line_end": 586,
        "methods": [
          "__init__",
          "detect_conflicts_comprehensive",
          "_detect_with_swarm",
          "_detect_with_statebased",
          "_detect_with_enhanced_analysis",
          "_analyze_aircraft_pair",
          "_calculate_cpa",
          "_calculate_horizontal_distance",
          "_assess_conflict_severity",
          "_calculate_confidence",
          "_cross_validate_conflicts",
          "_merge_conflict_detections",
          "_get_aircraft_pair_key",
          "validate_llm_conflicts",
          "_mock_conflict_detection"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "EnhancedHallucinationDetector",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 37,
        "line_end": 350,
        "methods": [
          "__init__",
          "_init_detection_patterns",
          "detect_hallucinations",
          "_check_aircraft_existence",
          "_check_altitude_validity",
          "_check_heading_validity",
          "_check_protocol_violations",
          "_check_impossible_maneuvers",
          "_check_nonsensical_response",
          "_determine_severity"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ModelConfig",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 28,
        "line_end": 37,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "HallucinationResult",
        "file_path": "analysis\\enhanced_hallucination_detection.py",
        "line_start": 27,
        "line_end": 34,
        "methods": [],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "Scenario",
        "file_path": "scenarios\\scenario_generator.py",
        "line_start": 55,
        "line_end": 97,
        "methods": [
          "__post_init__",
          "to_dict"
        ],
        "base_classes": [],
        "decorators": [
          "dataclass"
        ]
      },
      {
        "name": "LoRAMerger",
        "file_path": "BSKY_GYM_LLM\\merge_lora_and_convert.py",
        "line_start": 29,
        "line_end": 508,
        "methods": [
          "__init__",
          "check_prerequisites",
          "merge_lora_adapter",
          "_save_model_metadata",
          "convert_to_gguf",
          "create_ollama_model",
          "_create_enhanced_modelfile",
          "_verify_ollama_model",
          "run_complete_pipeline"
        ],
        "base_classes": [],
        "decorators": []
      },
      {
        "name": "ModelRole",
        "file_path": "llm_interface\\ensemble.py",
        "line_start": 20,
        "line_end": 24,
        "methods": [],
        "base_classes": [
          "Enum"
        ],
        "decorators": []
      }
    ],
    "modified_files": [
      "solver/__init__.py",
      "solver/conflict_solver.py"
    ]
  }
}